{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"BAN 501: Predictive Modeling","text":""},{"location":"#course-companion","title":"Course Companion","text":"<p>Welcome to the BAN 501 Course Companion. This resource provides comprehensive coverage of predictive modeling concepts, from foundational machine learning principles to advanced deep learning applications.</p>"},{"location":"#how-to-use-this-companion","title":"How to Use This Companion","text":"<ul> <li>Sequential reading: Work through modules 1-10 in order for a complete learning path</li> <li>Reference: Jump to specific topics using the navigation sidebar</li> <li>Search: Use the search bar to find specific concepts or terms</li> <li>Deep dives: Explore appendices for extended technical discussions</li> </ul>"},{"location":"#module-overview","title":"Module Overview","text":"Module Topic Key Concepts 1 Foundations Train/test splits, bias-variance tradeoff, evaluation metrics 2 Regression Linear regression, gradient descent, regularization 3 Classification Logistic regression, decision boundaries, ROC/AUC 4 Ensemble Methods Bagging, boosting, random forests, XGBoost 5 Unsupervised Learning Clustering, dimensionality reduction, PCA 6 Neural Networks Perceptrons, backpropagation, deep learning 7 Computer Vision CNNs, image classification, transfer learning 8 NLP Text processing, embeddings, transformers 9 Interpretability SHAP, LIME, feature importance 10 Ethics &amp; Deployment Fairness, model deployment, monitoring"},{"location":"#prerequisites","title":"Prerequisites","text":"<p>This companion assumes familiarity with:</p> <ul> <li>Basic Python programming</li> <li>Introductory statistics (mean, variance, distributions)</li> <li>Linear algebra fundamentals (matrices, vectors)</li> </ul>"},{"location":"#appendices","title":"Appendices","text":"<p>The appendices provide deeper technical explorations:</p> <ul> <li>Universal Approximators: Why neural networks can learn any function</li> <li>CNN Architecture: Detailed breakdown of convolutional networks</li> <li>Transformer Architecture: The architecture behind modern NLP</li> </ul>"},{"location":"appendices/cnn-architecture/","title":"Deep Dive: CNN Architecture","text":"<p>Extends Module 7: Computer Vision &amp; CNNs</p>"},{"location":"appendices/cnn-architecture/#introduction","title":"Introduction","text":"<p>In Module 7, we learned that CNNs are dramatically more parameter-efficient than fully connected networks for images. But why? What makes convolution so special?</p> <p>This deep dive explores the three principles that make CNNs work, walks through the convolution operation with concrete numbers, and shows exactly where parameters live in PyTorch's <code>nn.Conv2d</code>.</p> <p>By the end, you'll understand not just how CNNs work, but why they're designed the way they are.</p> <p>Why weren't CNNs always used? CNNs were discovered early (LeNet, 1989) but required computational resources that didn't exist. The 2012 AlexNet breakthrough combined sufficient data (ImageNet), compute (GPUs), and architecture improvements (ReLU, dropout). CNNs existed but weren't practical; FC networks and hand-crafted features (SIFT, HOG) were what people could actually train.</p>"},{"location":"appendices/cnn-architecture/#the-parameter-explosion-problem","title":"The Parameter Explosion Problem","text":""},{"location":"appendices/cnn-architecture/#why-fully-connected-networks-fail-for-images","title":"Why Fully Connected Networks Fail for Images","text":"<p>Consider a standard image classification task: - Input: 224 \u00d7 224 RGB image - Goal: Classify into 1000 categories</p> <p>If we flatten the image and connect to a hidden layer:</p> <pre><code>Input pixels: 224 \u00d7 224 \u00d7 3 = 150,528 features\nHidden layer: 1,000 neurons\nParameters: 150,528 \u00d7 1,000 = 150,528,000 (just the first layer!)\n</code></pre> <p>That's 150 million parameters for ONE layer!</p> <p>A typical network might have several hidden layers:</p> Layer Input Size Output Size Parameters FC1 150,528 4,096 616,562,688 FC2 4,096 4,096 16,781,312 FC3 4,096 1,000 4,097,000 Total 637,441,000 <p>637 million parameters just to process a 224\u00d7224 image!</p>"},{"location":"appendices/cnn-architecture/#the-problems-this-creates","title":"The Problems This Creates","text":"<ol> <li>Memory: Each parameter is 4 bytes (float32)</li> <li>637M params \u00d7 4 bytes = 2.5 GB just for weights</li> <li> <p>Plus gradients, optimizer states: 10+ GB</p> </li> <li> <p>Overfitting: More parameters than training examples</p> </li> <li>ImageNet has 1.2M training images</li> <li> <p>637M parameters would massively overfit</p> </li> <li> <p>Computation: Each forward pass multiplies huge matrices</p> </li> <li> <p>Billions of multiply-add operations per image</p> </li> <li> <p>No spatial understanding:</p> </li> <li>Pixel (0,0) and pixel (223,223) are equally \"distant\"</li> <li>A cat in the corner looks completely different than a cat in the center</li> </ol>"},{"location":"appendices/cnn-architecture/#the-three-principles-that-save-parameters","title":"The Three Principles That Save Parameters","text":"<p>CNNs achieve 99.9%+ parameter reduction through three key principles:</p>"},{"location":"appendices/cnn-architecture/#principle-1-local-connectivity-sparse-connections","title":"Principle 1: Local Connectivity (Sparse Connections)","text":"<p>The insight: Nearby pixels are related; distant pixels are less so.</p> <p>Instead of connecting every input pixel to every hidden neuron, each neuron only \"sees\" a small local region called the receptive field.</p> <p>Connectivity Comparison:</p> <p>Fully Connected: <pre><code>Every input pixel \u2192 Every hidden neuron\nConnections: 150,528 \u00d7 1,000 = 150,528,000\n</code></pre></p> <p>Locally Connected (3\u00d73 receptive field): <pre><code>Each hidden neuron sees only 3\u00d73\u00d73 = 27 input values\nConnections per neuron: 27\n</code></pre></p> <p></p> <p>This mirrors the visual cortex: neurons in V1 respond to small regions of the visual field. Hubel &amp; Wiesel won the Nobel Prize for discovering this.</p> <p>Brain inspiration: CNNs were directly inspired by neuroscience\u2014Hubel &amp; Wiesel showed visual cortex neurons respond to simple patterns in small regions. Other brain-inspired ideas: the neuron model (McCulloch-Pitts), ReLU (firing thresholds), dropout (neural noise), attention (selective focus). Caution warranted: backpropagation has no clear biological analog; transformers aren't obviously brain-like.</p>"},{"location":"appendices/cnn-architecture/#principle-2-weight-sharing-translation-equivariance","title":"Principle 2: Weight Sharing (Translation Equivariance)","text":"<p>The insight: The same pattern can appear anywhere in an image.</p> <p>A \"vertical edge detector\" should work whether the edge is in the top-left or bottom-right. Why learn separate detectors for each position?</p> <p>Without Weight Sharing (Locally Connected):</p> <p>If we had local connectivity but different weights at each position:</p> <pre><code>For 224\u00d7224 output with 3\u00d73 filters:\nPositions: 224 \u00d7 224 = 50,176\nParameters per position: 3 \u00d7 3 \u00d7 3 = 27\nTotal: 50,176 \u00d7 27 = 1,354,752 parameters (per filter)\n</code></pre> <p>Still a lot!</p> <p>With Weight Sharing (Convolutional):</p> <p>The same 3\u00d73\u00d73 filter is applied at every position:</p> <pre><code>Parameters: 3 \u00d7 3 \u00d7 3 + 1 (bias) = 28 parameters (per filter)\n</code></pre> <p>That's a 50,000\u00d7 reduction!</p> <p>What This Enables: Translation Equivariance</p> <p>If you shift the input, the output shifts by the same amount: $\\(f(\\text{shift}(x)) = \\text{shift}(f(x))\\)$</p> <p>A cat detector fires whether the cat is at (10, 10) or (200, 200).</p>"},{"location":"appendices/cnn-architecture/#principle-3-hierarchical-feature-learning","title":"Principle 3: Hierarchical Feature Learning","text":"<p>The insight: Complex features are built from simpler ones.</p> <p>Through multiple layers, CNNs build a hierarchy:</p> Layer Depth Receptive Field What's Learned Example Layer 1 3\u00d73 Edges, colors Horizontal line, blue patch Layer 2 7\u00d77 Textures, corners Fur texture, eye corner Layer 3 15\u00d715 Parts Eye, ear, nose Layer 4 31\u00d731 Objects Cat face, dog face Layer 5 63\u00d763 Scenes Cat on couch <p>Effective Receptive Field Calculation:</p> <p>For a network with L layers of K\u00d7K convolutions:</p> \\[\\text{Receptive Field} = 1 + L \\times (K - 1)\\] <p>Example with 3\u00d73 convolutions: - After 1 layer: 1 + 1\u00d72 = 3\u00d73 - After 2 layers: 1 + 2\u00d72 = 5\u00d75 - After 5 layers: 1 + 5\u00d72 = 11\u00d711 - After 10 layers: 1 + 10\u00d72 = 21\u00d721</p> <p>With pooling (stride 2), receptive field grows much faster\u2014each pooling layer doubles the effective receptive field.</p> <p>Seeing the whole object: The calculation above is without pooling\u2014practical networks reach 200+ pixel receptive fields via pooling. More importantly, the network doesn't need every neuron to see the whole object. Hierarchical structure means different neurons specialize at different scales. Global average pooling aggregates information across all positions, giving the classifier access to features detected anywhere.</p>"},{"location":"appendices/cnn-architecture/#the-savings-breakdown","title":"The Savings Breakdown","text":"<p>Let's compare a CNN to an equivalent fully connected network:</p>"},{"location":"appendices/cnn-architecture/#cnn-architecture-vgg-16-style-simplified","title":"CNN Architecture (VGG-16 style, simplified)","text":"Layer Output Shape Parameters Calculation Input 224\u00d7224\u00d73 0 - Conv1 (3\u00d73, 64) 224\u00d7224\u00d764 1,792 3\u00d73\u00d73\u00d764 + 64 Conv2 (3\u00d73, 64) 224\u00d7224\u00d764 36,928 3\u00d73\u00d764\u00d764 + 64 MaxPool 112\u00d7112\u00d764 0 - ... ... ... ... Flatten 25,088 0 7\u00d77\u00d7512 FC1 4,096 102,764,544 25,088\u00d74,096 + 4,096 Conv layers total ~9.5M FC layers total ~123M <p>The convolution layers have ~9.5 million parameters vs potential trillions for equivalent FC layers.</p> Principle Savings Factor Local Connectivity ~150,000\u00d7 (150,528 \u2192 ~27 connections) Weight Sharing ~50,000\u00d7 (one filter for all positions) Combined ~7,500,000,000\u00d7 (billions)"},{"location":"appendices/cnn-architecture/#why-convolution-works-for-images","title":"Why Convolution Works for Images","text":""},{"location":"appendices/cnn-architecture/#property-1-spatial-locality","title":"Property 1: Spatial Locality","text":"<p>Nearby pixels are statistically related</p> <ul> <li>Adjacent pixels often have similar values (smooth regions)</li> <li>Edges are local phenomena (sharp transitions between neighbors)</li> <li>Texture is defined by local patterns</li> </ul> <p>This justifies local connectivity: Most useful information is local.</p>"},{"location":"appendices/cnn-architecture/#property-2-stationarity","title":"Property 2: Stationarity","text":"<p>The same patterns appear throughout the image</p> <ul> <li>Edges can occur anywhere</li> <li>Textures repeat across regions</li> <li>Objects can appear at any location</li> </ul> <p>This justifies weight sharing: One detector works everywhere.</p>"},{"location":"appendices/cnn-architecture/#property-3-compositionality","title":"Property 3: Compositionality","text":"<p>Complex patterns are built from simpler ones</p> <ul> <li>Eyes are made of edges, curves, and colors</li> <li>Faces are made of eyes, nose, mouth</li> <li>Scenes are made of objects</li> </ul> <p>This justifies hierarchical layers: Build complexity gradually.</p>"},{"location":"appendices/cnn-architecture/#when-convolution-doesnt-work-well","title":"When Convolution Doesn't Work Well","text":"<p>CNNs struggle when these properties don't hold:</p> <ol> <li>Absolute position matters:</li> <li>Satellite imagery where \"top\" means north</li> <li>Document layouts where header is always at top</li> <li> <p>Solution: Add positional encoding or coordinates</p> </li> <li> <p>Global context needed immediately:</p> </li> <li>Recognizing objects from single distinctive features</li> <li>Tasks requiring reasoning about entire image at once</li> <li> <p>Solution: Attention mechanisms, Vision Transformers</p> </li> <li> <p>Non-grid data:</p> </li> <li>Graphs, point clouds, irregular meshes</li> <li>Solution: Graph neural networks, PointNet</li> </ol>"},{"location":"appendices/cnn-architecture/#the-convolution-operation-in-depth","title":"The Convolution Operation In-Depth","text":"<p>Let's trace through a convolution with concrete numbers.</p>"},{"location":"appendices/cnn-architecture/#setup","title":"Setup","text":"<ul> <li>Input: 1 image, 3 channels (RGB), 8\u00d78 pixels</li> <li> <p>Shape: <code>(1, 3, 8, 8)</code> - (batch, channels, height, width)</p> </li> <li> <p>Filter: 1 filter, 3\u00d73 kernel</p> </li> <li> <p>Shape: <code>(1, 3, 3, 3)</code> - (out_channels, in_channels, kernel_h, kernel_w)</p> </li> <li> <p>Output: 1 feature map, 6\u00d76</p> </li> <li>Shape: <code>(1, 1, 6, 6)</code> - (batch, out_channels, height, width)</li> </ul>"},{"location":"appendices/cnn-architecture/#the-sliding-window-process","title":"The Sliding Window Process","text":"<p>For each position (i, j) in the output:</p> \\[\\text{output}[0, 0, i, j] = \\sum_{c=0}^{2} \\sum_{m=0}^{2} \\sum_{n=0}^{2} \\text{filter}[0, c, m, n] \\times \\text{input}[0, c, i+m, j+n] + \\text{bias}[0]\\]"},{"location":"appendices/cnn-architecture/#concrete-example","title":"Concrete Example","text":"<p>Let's compute one output value at position (0, 0):</p> <p>Input patch (top-left 3\u00d73 of each channel): <pre><code>Red channel:        Green channel:      Blue channel:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 0.1 0.2 0.3 \u2502     \u2502 0.4 0.5 0.6 \u2502     \u2502 0.7 0.8 0.9 \u2502\n\u2502 0.2 0.3 0.4 \u2502     \u2502 0.5 0.6 0.7 \u2502     \u2502 0.8 0.9 1.0 \u2502\n\u2502 0.3 0.4 0.5 \u2502     \u2502 0.6 0.7 0.8 \u2502     \u2502 0.9 1.0 1.1 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <p>Filter weights (Sobel-like vertical edge detector): <pre><code>Red weights:        Green weights:      Blue weights:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 1   0  -1   \u2502     \u2502 1   0  -1   \u2502     \u2502 1   0  -1   \u2502\n\u2502 2   0  -2   \u2502     \u2502 2   0  -2   \u2502     \u2502 2   0  -2   \u2502\n\u2502 1   0  -1   \u2502     \u2502 1   0  -1   \u2502     \u2502 1   0  -1   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <p>Computation: <pre><code># Red channel contribution\nred = (0.1\u00d71 + 0.2\u00d70 + 0.3\u00d7(-1) +\n       0.2\u00d72 + 0.3\u00d70 + 0.4\u00d7(-2) +\n       0.3\u00d71 + 0.4\u00d70 + 0.5\u00d7(-1))\n    = 0.1 - 0.3 + 0.4 - 0.8 + 0.3 - 0.5\n    = -0.8\n\n# Similarly for green and blue...\n# Total (sum across channels) + bias\noutput[0,0,0,0] = red + green + blue + bias = -2.4\n</code></pre></p> <p>The filter slides across all valid positions to fill the 6\u00d76 output.</p>"},{"location":"appendices/cnn-architecture/#output-size-formula","title":"Output Size Formula","text":"\\[\\text{output\\_size} = \\left\\lfloor \\frac{\\text{input\\_size} - \\text{kernel\\_size} + 2 \\times \\text{padding}}{\\text{stride}} \\right\\rfloor + 1\\] <p>For our example: $\\(\\frac{8 - 3 + 2 \\times 0}{1} + 1 = 6\\)$</p>"},{"location":"appendices/cnn-architecture/#annotated-nnconv2d-walkthrough","title":"Annotated nn.Conv2d Walkthrough","text":"<pre><code>import torch\nimport torch.nn as nn\n\n# Create a convolutional layer\nconv = nn.Conv2d(\n    in_channels=3,      # RGB input\n    out_channels=64,    # Number of filters (feature maps)\n    kernel_size=3,      # 3\u00d73 filter\n    stride=1,           # Move 1 pixel at a time\n    padding=1,          # Add 1 pixel border of zeros\n    bias=True           # Include bias term\n)\n\n# WHERE THE PARAMETERS LIVE\nprint(f\"Weight shape: {conv.weight.shape}\")\n# Output: torch.Size([64, 3, 3, 3])\n# Interpretation: [out_channels, in_channels, kernel_height, kernel_width]\n# - 64 different filters\n# - Each filter has 3 channels (one for R, G, B)\n# - Each channel is 3\u00d73\n\nprint(f\"Weight parameters: {conv.weight.numel()}\")\n# Output: 1728\n# Calculation: 64 \u00d7 3 \u00d7 3 \u00d7 3 = 1,728\n\nprint(f\"Bias shape: {conv.bias.shape}\")\n# Output: torch.Size([64])\n# One bias per output channel (filter)\n\nprint(f\"Total parameters: {sum(p.numel() for p in conv.parameters())}\")\n# Output: 1792\n# = 1,728 (weights) + 64 (biases)\n</code></pre>"},{"location":"appendices/cnn-architecture/#understanding-the-weight-tensor","title":"Understanding the Weight Tensor","text":"<pre><code># Access filter 0 (first of 64 filters)\nfilter_0 = conv.weight[0]  # Shape: [3, 3, 3]\n\n# Filter 0's red channel weights\nfilter_0_red = conv.weight[0, 0]  # Shape: [3, 3]\n\n# Filter 0's green channel weights\nfilter_0_green = conv.weight[0, 1]  # Shape: [3, 3]\n\n# Filter 0's blue channel weights\nfilter_0_blue = conv.weight[0, 2]  # Shape: [3, 3]\n</code></pre>"},{"location":"appendices/cnn-architecture/#visualizing-parameter-sharing","title":"Visualizing Parameter Sharing","text":"<pre><code># The SAME weights are used for EVERY spatial position\n# This is what makes CNNs parameter-efficient\n\n# These two output positions use the SAME filter weights:\n# y[0, 0, 0, 0] uses conv.weight[0] applied at position (0,0)\n# y[0, 0, 5, 5] uses conv.weight[0] applied at position (5,5)\n\n# Without weight sharing, we'd need:\n# 64 filters \u00d7 8\u00d78 positions \u00d7 3\u00d73\u00d73 weights = 884,736 parameters\n# With weight sharing:\n# 64 filters \u00d7 3\u00d73\u00d73 weights = 1,728 parameters\n# That's a 512\u00d7 reduction!\n</code></pre>"},{"location":"appendices/cnn-architecture/#pooling-trading-resolution-for-invariance","title":"Pooling: Trading Resolution for Invariance","text":""},{"location":"appendices/cnn-architecture/#what-max-pooling-does","title":"What Max Pooling Does","text":"<p>Max pooling takes the maximum value in each patch:</p> <p></p>"},{"location":"appendices/cnn-architecture/#why-max-pooling-works","title":"Why Max Pooling Works","text":"<ol> <li>Translation invariance: The feature's exact position within the pooling region doesn't matter</li> <li>Dimensionality reduction: Halves spatial dimensions, reducing computation</li> <li>Increased receptive field: Each pooled output sees a larger input region</li> <li>Slight noise robustness: Small perturbations don't change the max</li> </ol>"},{"location":"appendices/cnn-architecture/#pooling-vs-stride-trade-off","title":"Pooling vs Stride Trade-off","text":"<p>Modern networks often use strided convolutions instead of pooling:</p> Approach Operation Parameters Learned Max Pool Take max 0 No Avg Pool Take mean 0 No Strided Conv Conv with stride 2 kernel\u00b2 \u00d7 channels\u00b2 Yes <p>Strided convolution can learn what information to preserve, but requires more parameters.</p>"},{"location":"appendices/cnn-architecture/#common-misconceptions","title":"Common Misconceptions","text":"Misconception Reality \"CNNs learn the filters by hand\" Filters are learned automatically through backpropagation, not designed by humans \"More filters is always better\" Diminishing returns; excess filters learn redundant or noise patterns \"CNNs work on any image task\" CNNs assume spatial locality and translation equivariance; they struggle when these don't hold \"The first layer must have 3 input channels\" Input channels match your data: 1 for grayscale, 3 for RGB, any number for multi-spectral \"Pooling is necessary\" Modern architectures often use strided convolutions instead \"Deeper is always better\" Without residual connections, very deep CNNs fail to train"},{"location":"appendices/cnn-architecture/#reflection-questions","title":"Reflection Questions","text":"<ol> <li> <p>If we didn't share weights across positions, how many parameters would a 3\u00d73 conv layer have for a 224\u00d7224 image with 64 filters?</p> </li> <li> <p>Why might a CNN struggle with satellite imagery where \"up\" always means north?</p> </li> <li> <p>How does the receptive field grow through layers? Why does this matter?</p> </li> <li> <p>A 7\u00d77 conv has 49 weights per channel. Two stacked 3\u00d73 convs have only 18 weights but achieve the same receptive field. What's the trade-off?</p> </li> <li> <p>What happens if you set padding=0 with a 3\u00d73 kernel on a 224\u00d7224 input?</p> </li> <li> <p>Why do we typically increase the number of filters as we go deeper while decreasing spatial size?</p> </li> </ol>"},{"location":"appendices/cnn-architecture/#practice-problems","title":"Practice Problems","text":"<ol> <li> <p>Calculate parameters for VGG-16 from scratch</p> </li> <li> <p>Compute output size for: input 64\u00d764, kernel 5\u00d75, stride 2, padding 2</p> </li> <li> <p>Design a CNN for 64\u00d764 grayscale images with target receptive field of 32\u00d732</p> </li> </ol>"},{"location":"appendices/cnn-architecture/#summary","title":"Summary","text":"<p>Key takeaways:</p> <ol> <li> <p>Local connectivity reduces connections from millions to dozens per neuron</p> </li> <li> <p>Weight sharing reduces parameters by 50,000\u00d7 for a single filter</p> </li> <li> <p>Hierarchical learning builds complex features from simple ones</p> </li> <li> <p>These principles work because images have spatial locality, stationarity, and compositionality</p> </li> <li> <p>The total savings is on the order of billions of parameters</p> </li> <li> <p>Understanding where parameters live helps you reason about model capacity and design</p> </li> </ol>"},{"location":"appendices/transformer-architecture/","title":"Deep Dive: Transformer Architecture","text":"<p>Extends Module 8: Natural Language Processing</p>"},{"location":"appendices/transformer-architecture/#introduction","title":"Introduction","text":"<p>In Module 8, we learned that transformers use self-attention to process sequences. We covered the high-level concepts of Query, Key, Value and the attention formula.</p> <p>This deep dive goes deeper. We'll trace through the exact matrix dimensions at each step, see exactly where the learnable parameters live, and build a complete transformer from scratch in PyTorch.</p> <p>By the end, you'll understand not just what transformers do, but how they do it\u2014down to the individual matrix operations.</p> <p>What parameters learn: Token embeddings learn dense representations where similar words cluster together. Attention projections (W_Q, W_K, W_V) learn relevance between tokens\u2014W_Q learns what tokens \"look for,\" W_K what they \"offer,\" W_V what information to pass. FFNs appear to store factual knowledge as distributed key-value memories. Layer norms stabilize training.</p>"},{"location":"appendices/transformer-architecture/#where-parameters-live-in-a-transformer","title":"Where Parameters Live in a Transformer","text":"<p>Understanding where the learnable parameters actually reside is crucial for understanding what the model \"learns\" during training.</p>"},{"location":"appendices/transformer-architecture/#parameter-overview","title":"Parameter Overview","text":"<p>For a transformer with: - <code>vocab_size</code> = 30,000 (vocabulary) - <code>d_model</code> = 512 (model dimension) - <code>n_heads</code> = 8 (attention heads) - <code>d_k = d_v</code> = 64 (dimension per head = d_model / n_heads) - <code>d_ff</code> = 2048 (feedforward hidden dimension, typically 4\u00d7 d_model) - <code>n_layers</code> = 6 (number of transformer blocks) - <code>max_seq_len</code> = 512 (maximum sequence length)</p> <p>Why 4\u00d7 expansion in FFN? Largely empirical\u2014it worked well in the original paper. The expansion provides \"intermediate reasoning space.\" Too small (2\u00d7) limits expressivity; too large (8\u00d7) adds parameters with diminishing returns. Some efficient transformers use 2-2.67\u00d7; the ratio isn't sacred if you have specific constraints.</p>"},{"location":"appendices/transformer-architecture/#token-embedding-matrix","title":"Token Embedding Matrix","text":"<p>Shape: <code>(vocab_size, d_model)</code> = <code>(30000, 512)</code></p> <p>Parameters: 15,360,000</p> <p>What it learns: A dense vector representation for each token in the vocabulary. This is the \"lookup table\" that converts token IDs to continuous vectors.</p> <p>How similar embeddings emerge: Word relationships emerge from the training objective (distributional hypothesis). Tokens in similar contexts (\"cat\" and \"dog\" both appear in \"the ___ ran across the yard\") get similar embeddings. The model learns they're interchangeable in many contexts. Subtle relationships emerge too: \"king\" \u2013 \"man\" + \"woman\" \u2248 \"queen.\" None is programmed\u2014it falls out of optimizing prediction accuracy.</p> <pre><code>self.token_embedding = nn.Embedding(\n    num_embeddings=vocab_size,  # 30,000\n    embedding_dim=d_model        # 512\n)\n# Weight shape: (30000, 512)\n</code></pre>"},{"location":"appendices/transformer-architecture/#positional-encoding-embedding","title":"Positional Encoding / Embedding","text":"<p>Sinusoidal (Original Transformer): No learnable parameters\u2014computed deterministically</p> <p>Learned Positional Embedding: - Shape: <code>(max_seq_len, d_model)</code> = <code>(512, 512)</code> - Parameters: 262,144</p> <pre><code># Learned positional embeddings\nself.pos_embedding = nn.Embedding(\n    num_embeddings=max_seq_len,  # 512\n    embedding_dim=d_model         # 512\n)\n</code></pre>"},{"location":"appendices/transformer-architecture/#attention-layer-parameters-per-layer","title":"Attention Layer Parameters (Per Layer)","text":"<p>Each attention layer has four weight matrices:</p> Matrix Shape Parameters Purpose W_Q <code>(d_model, d_model)</code> 512 \u00d7 512 = 262,144 Projects input to queries W_K <code>(d_model, d_model)</code> 512 \u00d7 512 = 262,144 Projects input to keys W_V <code>(d_model, d_model)</code> 512 \u00d7 512 = 262,144 Projects input to values W_O <code>(d_model, d_model)</code> 512 \u00d7 512 = 262,144 Projects concatenated heads to output Biases 4 \u00d7 <code>(d_model)</code> 4 \u00d7 512 = 2,048 One bias per projection <p>Total per attention layer: ~1,050,624 parameters</p> <p>Key insight: Even though we have 8 heads, the total parameter count is the same as if we had one big head. The \"heads\" are created by reshaping, not by adding parameters.</p> <p>Why multiple heads help: They enable attending to different relationships simultaneously\u2014syntactic, semantic, positional. Research finds heads that specialize: one attends to grammatical antecedents, another to adjacent tokens. This specialization emerges during training. Trade-off: each head has smaller d_k, but specialization benefits outweigh capacity reduction.</p>"},{"location":"appendices/transformer-architecture/#feed-forward-network-per-layer","title":"Feed-Forward Network (Per Layer)","text":"<p>The FFN applies two linear transformations with a non-linearity:</p> \\[\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2\\] Matrix Shape Parameters Purpose W_1 <code>(d_model, d_ff)</code> 512 \u00d7 2048 = 1,048,576 Expand to higher dimension b_1 <code>(d_ff)</code> 2,048 Bias for expansion W_2 <code>(d_ff, d_model)</code> 2048 \u00d7 512 = 1,048,576 Contract back to model dimension b_2 <code>(d_model)</code> 512 Bias for contraction <p>Total per FFN: ~2,099,712 parameters</p> <p>FFN as knowledge storage: Research supports this hypothesis. Specific FFN neurons activate for specific concepts (\"The capital of France is ___\" triggers neurons contributing \"Paris\"). Researchers have edited factual knowledge by modifying FFN weights. Attention handles \"routing\" (context-dependent); FFN handles fixed transformations. Fixed parameters suit stable facts; dynamic computation suits context-dependent processing.</p>"},{"location":"appendices/transformer-architecture/#layer-normalization-per-layer","title":"Layer Normalization (Per Layer)","text":"<p>Each transformer block typically has 2 layer norms:</p> Component Shape Parameters Scale (\u03b3) <code>(d_model)</code> 512 Shift (\u03b2) <code>(d_model)</code> 512 <p>Total per layer norm: 1,024 parameters Total per transformer block: 2,048 parameters (2 layer norms)</p> <p>Why layer norm matters: Without it, activations grow/shrink exponentially through layers\u2014causing gradient explosion/vanishing. Layer norm keeps activations stable regardless of depth. The learned \u03b3 and \u03b2 parameters let the model recover useful mean/variance. Appears twice per block (before attention, before FFN) because both operations can distort statistics.</p>"},{"location":"appendices/transformer-architecture/#parameter-count-summary","title":"Parameter Count Summary","text":"Component Count Parameters Each Total Token Embedding 1 15,360,000 15,360,000 Positional Embedding 1 262,144 262,144 Attention Layers 6 ~1,050,624 ~6,303,744 FFN Layers 6 ~2,099,712 ~12,598,272 Layer Norms 12 1,024 12,288 Output Head 1 15,360,000 15,360,000* <p>Total: ~50 million parameters (with tied embeddings: ~35 million)</p> <p>*Often tied with token embedding</p>"},{"location":"appendices/transformer-architecture/#what-each-component-does-the-why","title":"What Each Component Does (The \"Why\")","text":""},{"location":"appendices/transformer-architecture/#why-positional-encoding","title":"Why Positional Encoding?","text":"<p>The Problem: Self-attention is permutation-invariant</p> <p>Without position information, these sentences would be identical to the model: - \"Dog bites man\" - \"Man bites dog\" - \"Bites man dog\"</p> <p>The attention mechanism only cares about what tokens are present and their relationships, not where they appear.</p> <p>Sinusoidal Positional Encoding</p> \\[PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$ $$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\\] <p>Why sine and cosine?</p> <ol> <li>Bounded range: Values stay in [-1, 1], preventing position from dominating</li> <li>Unique per position: Each position gets a unique encoding</li> <li>Relative positions via linear transformation: For any fixed offset k, \\(PE_{pos+k}\\) can be represented as a linear function of \\(PE_{pos}\\)</li> <li>Generalizes to longer sequences: Works for sequences longer than seen during training</li> </ol> <p>Why learned embeddings over sinusoidal? Sinusoidal generalizes to arbitrary lengths, but fixed context windows make this rarely matter in practice. Learned embeddings perform slightly better and can capture task-specific patterns (code structure, conversation turns). Neither extrapolates well beyond training lengths. Modern architectures use relative positional encodings (RoPE, ALiBi) that generalize better via relative distances.</p>"},{"location":"appendices/transformer-architecture/#why-self-attention","title":"Why Self-Attention?","text":"<p>Advantage 1: O(1) Path Length</p> <p>To connect position 1 to position 100: - RNN: Information must pass through 99 sequential steps - Attention: Direct connection in one step</p> <p>This solves the long-range dependency problem.</p> <p>Advantage 2: Parallelization</p> <ul> <li>RNN: Must process sequentially (h\u2081 \u2192 h\u2082 \u2192 h\u2083 \u2192 ...)</li> <li>Attention: All positions computed simultaneously</li> </ul> <p>This enables massive speedups on GPUs.</p> <p>Advantage 3: Dynamic, Content-Dependent Connections</p> <p>RNNs have fixed connections (previous \u2192 current). Attention weights are computed based on the content of the sequence:</p> <pre><code>\"The cat sat on the mat because it was soft\"\n                                    \u2191\n                              \"it\" attends strongly to \"mat\"\n\n\"The cat sat on the mat because it was tired\"\n                                    \u2191\n                              \"it\" attends strongly to \"cat\"\n</code></pre> <p>Same architecture, different attention patterns based on meaning.</p> <p>Learning contextual attention: Entirely learned during training via backpropagation. When the model predicts incorrectly (attended to \"mat\" instead of \"cat\"), gradients adjust W_Q, W_K, W_V so \"it\" generates queries with higher similarity to \"cat\" when context suggests animacy. Different heads learn different aspects (proximity, syntax, semantics), enabling sophisticated disambiguation.</p>"},{"location":"appendices/transformer-architecture/#why-the-ffn-mlp","title":"Why the FFN (MLP)?","text":"<p>The attention mechanism is powerful but has limitations:</p> <ol> <li>Attention is linear (after softmax): Just weighted sums of values</li> <li>Attention is the same for all positions: The W_Q, W_K, W_V matrices don't change per position</li> </ol> <p>What FFN provides:</p> <p>Non-linearity: The ReLU (or GELU) activation adds non-linear transformations that attention alone cannot provide.</p> <p>Position-wise processing: Each position gets the same transformation, but independently.</p> <p>Memory/Knowledge storage: Research suggests FFN layers store factual knowledge. When you ask \"The capital of France is ___\", the FFN layers help retrieve \"Paris.\"</p> <p>Why the expansion to 4\u00d7?</p> <p>The expansion (512 \u2192 2048 \u2192 512) creates a \"bottleneck\" architecture: - Expansion: Project to higher dimension, allowing richer intermediate representations - Non-linearity: Apply ReLU/GELU - Contraction: Compress back to model dimension</p> <p>FFN as key-value memory: W_1 rows are \"keys\"\u2014patterns to match. Input compared via matrix multiplication; GELU sparsifies activations. W_2 stores \"values\" retrieved when keys match. Computing FFN(x) = GELU(x W_1) W_2 is: find matching keys, weight by match strength, retrieve values. Ablating specific W_2 rows removes specific facts\u2014strong evidence for this interpretation.</p>"},{"location":"appendices/transformer-architecture/#self-attention-step-by-step-with-matrix-dimensions","title":"Self-Attention Step-by-Step with Matrix Dimensions","text":"<p>Let's trace through self-attention with concrete numbers:</p> <p>Setup: - <code>batch_size (B)</code> = 2 - <code>seq_len (T)</code> = 4 - <code>d_model</code> = 8 - <code>n_heads</code> = 2 - <code>d_k = d_v</code> = 4 (= d_model / n_heads)</p>"},{"location":"appendices/transformer-architecture/#step-1-input","title":"Step 1: Input","text":"<p>X shape: <code>(B, T, d_model)</code> = <code>(2, 4, 8)</code></p> <p>This is 2 sequences, each with 4 tokens, each token represented by 8 dimensions.</p>"},{"location":"appendices/transformer-architecture/#step-2-linear-projections","title":"Step 2: Linear Projections","text":"<p>Weight matrices (learnable parameters): - W_Q: <code>(8, 8)</code> - W_K: <code>(8, 8)</code> - W_V: <code>(8, 8)</code></p> <p>Compute Q, K, V: <pre><code>Q = X @ W_Q: (2, 4, 8) @ (8, 8) \u2192 (2, 4, 8)\nK = X @ W_K: (2, 4, 8) @ (8, 8) \u2192 (2, 4, 8)\nV = X @ W_V: (2, 4, 8) @ (8, 8) \u2192 (2, 4, 8)\n</code></pre></p>"},{"location":"appendices/transformer-architecture/#step-3-reshape-for-multi-head-attention","title":"Step 3: Reshape for Multi-Head Attention","text":"<p>We split d_model=8 into n_heads=2 heads, each with d_k=4 dimensions.</p> <p>Reshape: <code>(B, T, d_model)</code> \u2192 <code>(B, T, n_heads, d_k)</code> \u2192 <code>(B, n_heads, T, d_k)</code></p> <pre><code>Q: (2, 4, 8) \u2192 (2, 4, 2, 4) \u2192 (2, 2, 4, 4)\nK: (2, 4, 8) \u2192 (2, 4, 2, 4) \u2192 (2, 2, 4, 4)\nV: (2, 4, 8) \u2192 (2, 4, 2, 4) \u2192 (2, 2, 4, 4)\n</code></pre> <p>Now we have: - 2 batches - 2 heads per batch - 4 tokens per head - 4 dimensions per token</p>"},{"location":"appendices/transformer-architecture/#step-4-compute-attention-scores","title":"Step 4: Compute Attention Scores","text":"<p>Formula: scores = Q @ K^T / \u221ad_k</p> <pre><code>Q @ K^T: (2, 2, 4, 4) @ (2, 2, 4, 4)^T \u2192 (2, 2, 4, 4)\n</code></pre> <p>The result <code>(2, 2, 4, 4)</code> means: for each batch, for each head, we have a 4\u00d74 matrix where entry (i,j) is how much token i attends to token j.</p> <p>Scale by \u221ad_k = \u221a4 = 2: <pre><code>scores = scores / 2\n</code></pre></p> <p>This prevents the dot products from growing too large (which would make softmax saturate).</p> <p>Without \u221ad_k scaling: Dot products grow with d_k, pushing softmax into saturated regions (nearly one-hot). Problems: gradients vanish (softmax derivative approaches zero), attention becomes too \"sharp\" (loses weighted combination), model becomes brittle (small changes flip attention). Scaling maintains consistent softmax behavior regardless of dimensionality.</p>"},{"location":"appendices/transformer-architecture/#step-5-apply-softmax","title":"Step 5: Apply Softmax","text":"<pre><code>attn_weights = softmax(scores, dim=-1)\nShape: (2, 2, 4, 4) \u2192 (2, 2, 4, 4)\n</code></pre> <p>Each row now sums to 1:</p> <pre><code>Example attention matrix for one head:\n        Token0  Token1  Token2  Token3\nToken0:  0.4     0.3     0.2     0.1    = 1.0\nToken1:  0.1     0.5     0.3     0.1    = 1.0\nToken2:  0.2     0.2     0.4     0.2    = 1.0\nToken3:  0.1     0.1     0.2     0.6    = 1.0\n</code></pre>"},{"location":"appendices/transformer-architecture/#step-6-apply-attention-to-values","title":"Step 6: Apply Attention to Values","text":"<pre><code>attn_weights @ V: (2, 2, 4, 4) @ (2, 2, 4, 4) \u2192 (2, 2, 4, 4)\n</code></pre> <p>Each token's output is now a weighted sum of all value vectors.</p>"},{"location":"appendices/transformer-architecture/#step-7-concatenate-heads","title":"Step 7: Concatenate Heads","text":"<p>Reshape: <code>(B, n_heads, T, d_k)</code> \u2192 <code>(B, T, n_heads, d_k)</code> \u2192 <code>(B, T, d_model)</code></p> <pre><code>context: (2, 2, 4, 4) \u2192 (2, 4, 2, 4) \u2192 (2, 4, 8)\n</code></pre>"},{"location":"appendices/transformer-architecture/#step-8-output-projection","title":"Step 8: Output Projection","text":"<pre><code>output = context @ W_O: (2, 4, 8) @ (8, 8) \u2192 (2, 4, 8)\n</code></pre> <p>Final output has the same shape as input: <code>(2, 4, 8)</code>.</p> <p>Stacking limits: Shape preservation enables many layers (GPT-3 has 96), but practical limits exist: compute/memory scale linearly, training stability degrades, performance shows diminishing returns (12\u219224 helps more than 96\u2192192), and latency matters for real-time applications. For fixed compute budgets, there's an optimal balance between depth, width, and data.</p>"},{"location":"appendices/transformer-architecture/#dimension-flow-diagram","title":"Dimension Flow Diagram","text":""},{"location":"appendices/transformer-architecture/#from-scratch-pytorch-implementation","title":"From-Scratch PyTorch Implementation","text":""},{"location":"appendices/transformer-architecture/#self-attention-module","title":"Self-Attention Module","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\n\nclass MultiHeadSelfAttention(nn.Module):\n    \"\"\"Multi-head self-attention from scratch.\"\"\"\n\n    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):\n        super().__init__()\n\n        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.d_k = d_model // n_heads\n\n        # Learnable projection matrices\n        self.W_Q = nn.Linear(d_model, d_model)\n        self.W_K = nn.Linear(d_model, d_model)\n        self.W_V = nn.Linear(d_model, d_model)\n        self.W_O = nn.Linear(d_model, d_model)\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -&gt; torch.Tensor:\n        batch_size, seq_len, _ = x.shape\n\n        # Step 1: Linear projections\n        Q = self.W_Q(x)\n        K = self.W_K(x)\n        V = self.W_V(x)\n\n        # Step 2: Reshape for multi-head attention\n        Q = Q.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n        K = K.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n        V = V.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n\n        # Step 3: Compute attention scores\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n\n        # Step 4: Apply mask (optional)\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, float('-inf'))\n\n        # Step 5: Softmax\n        attn_weights = F.softmax(scores, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n\n        # Step 6: Apply attention to values\n        context = torch.matmul(attn_weights, V)\n\n        # Step 7: Concatenate heads\n        context = context.transpose(1, 2).contiguous()\n        context = context.view(batch_size, seq_len, self.d_model)\n\n        # Step 8: Output projection\n        output = self.W_O(context)\n\n        return output\n</code></pre>"},{"location":"appendices/transformer-architecture/#feed-forward-network","title":"Feed-Forward Network","text":"<pre><code>class FeedForward(nn.Module):\n    \"\"\"Position-wise feed-forward network.\"\"\"\n\n    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n        super().__init__()\n        self.fc1 = nn.Linear(d_model, d_ff)\n        self.fc2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = self.fc1(x)           # Expand\n        x = F.gelu(x)             # Non-linearity\n        x = self.dropout(x)\n        x = self.fc2(x)           # Contract\n        return x\n</code></pre>"},{"location":"appendices/transformer-architecture/#transformer-block","title":"Transformer Block","text":"<pre><code>class TransformerBlock(nn.Module):\n    \"\"\"A single transformer block.\"\"\"\n\n    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):\n        super().__init__()\n\n        self.attention = MultiHeadSelfAttention(d_model, n_heads, dropout)\n        self.ffn = FeedForward(d_model, d_ff, dropout)\n        self.ln1 = nn.LayerNorm(d_model)\n        self.ln2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -&gt; torch.Tensor:\n        # Attention with residual connection\n        normed = self.ln1(x)\n        attn_out = self.attention(normed, mask=mask)\n        x = x + self.dropout(attn_out)\n\n        # FFN with residual connection\n        normed = self.ln2(x)\n        ffn_out = self.ffn(normed)\n        x = x + self.dropout(ffn_out)\n\n        return x\n</code></pre>"},{"location":"appendices/transformer-architecture/#complete-decoder-only-transformer-gpt-style","title":"Complete Decoder-Only Transformer (GPT-style)","text":"<pre><code>class TransformerDecoder(nn.Module):\n    \"\"\"A complete decoder-only transformer (GPT-style).\"\"\"\n\n    def __init__(\n        self,\n        vocab_size: int,\n        d_model: int,\n        n_heads: int,\n        n_layers: int,\n        d_ff: int,\n        max_seq_len: int,\n        dropout: float = 0.1,\n        tie_weights: bool = True\n    ):\n        super().__init__()\n\n        self.token_embedding = nn.Embedding(vocab_size, d_model)\n        self.pos_embedding = nn.Embedding(max_seq_len, d_model)\n\n        self.layers = nn.ModuleList([\n            TransformerBlock(d_model, n_heads, d_ff, dropout)\n            for _ in range(n_layers)\n        ])\n\n        self.ln_final = nn.LayerNorm(d_model)\n        self.output_head = nn.Linear(d_model, vocab_size, bias=False)\n\n        if tie_weights:\n            self.output_head.weight = self.token_embedding.weight\n\n    def forward(self, input_ids: torch.Tensor) -&gt; torch.Tensor:\n        batch_size, seq_len = input_ids.shape\n\n        # Create causal mask\n        mask = torch.tril(torch.ones(seq_len, seq_len, device=input_ids.device))\n\n        # Embeddings\n        positions = torch.arange(seq_len, device=input_ids.device)\n        x = self.token_embedding(input_ids) + self.pos_embedding(positions)\n\n        # Transformer blocks\n        for layer in self.layers:\n            x = layer(x, mask=mask)\n\n        # Output\n        x = self.ln_final(x)\n        logits = self.output_head(x)\n\n        return logits\n</code></pre> <p>Weight tying: Input embedding maps tokens to semantic space; output projection maps back. Sharing weights (saves ~15M parameters) makes sense since they're conceptually inverse operations. Empirically, tying usually helps or is neutral\u2014the shared weights get more training signal, providing regularization. Some very large models untie for more expressivity, but tying is a sensible default.</p>"},{"location":"appendices/transformer-architecture/#common-misconceptions","title":"Common Misconceptions","text":"Misconception Reality \"The attention weights are the main learnable parameters\" W_Q, W_K, W_V, W_O are learned. Attention weights are computed dynamically. \"More attention heads is always better\" Each head gets smaller d_k. Diminishing returns exist. \"Self-attention is expensive because of parameters\" It's expensive because of O(n\u00b2) computation in the attention matrix. \"Transformers understand language\" Transformers learn statistical patterns, not \"understanding.\" \"Attention visualizations show what the model 'thinks'\" Attention weights don't always correlate with importance."},{"location":"appendices/transformer-architecture/#reflection-questions","title":"Reflection Questions","text":"<ol> <li> <p>If you increase the number of attention heads but keep d_model fixed, what happens to d_k? What's the trade-off?</p> </li> <li> <p>Why does the FFN typically expand to 4\u00d7 the model dimension?</p> </li> <li> <p>Where would you expect most of the \"knowledge\" to be stored\u2014attention weights or FFN weights?</p> </li> <li> <p>Why do we scale by \u221ad_k in the attention formula?</p> </li> <li> <p>Why do we need the output projection W_O after concatenating heads?</p> </li> <li> <p>What happens if we remove the residual connections?</p> </li> </ol>"},{"location":"appendices/transformer-architecture/#practice-problems","title":"Practice Problems","text":"<ol> <li> <p>Calculate the total parameter count for GPT-2 (d_model=768, n_heads=12, n_layers=12, vocab=50257)</p> </li> <li> <p>Trace the dimensions through cross-attention (encoder-decoder attention)</p> </li> <li> <p>Implement masked self-attention for causal language modeling</p> </li> </ol>"},{"location":"appendices/transformer-architecture/#summary","title":"Summary","text":"<p>Key takeaways:</p> <ol> <li> <p>Token embeddings contain the most parameters in small transformers (~15M for 30K vocab)</p> </li> <li> <p>Attention parameters: W_Q, W_K, W_V, W_O project inputs to queries, keys, values, and combine head outputs</p> </li> <li> <p>FFN parameters often dominate in larger transformers (2\u00d7 the attention parameters per layer)</p> </li> <li> <p>Multi-head attention doesn't add parameters\u2014it reshapes existing projections</p> </li> <li> <p>The dimension flow: (B, T, d_model) \u2192 project \u2192 reshape for heads \u2192 attention \u2192 concatenate \u2192 project \u2192 (B, T, d_model)</p> </li> <li> <p>Why it works: O(1) path length, parallelization, content-dependent connections, and non-linearity from FFN</p> </li> <li> <p>Understanding the architecture helps you reason about capacity, compute requirements, and what the model might be learning</p> </li> </ol>"},{"location":"appendices/universal-approximators/","title":"Deep Dive: Neural Networks as Universal Approximators","text":"<p>Extends Module 6: Neural Networks</p>"},{"location":"appendices/universal-approximators/#introduction","title":"Introduction","text":"<p>In Module 6, we learned how to build and train neural networks. But there's a deeper question: why can neural networks learn such a wide variety of functions?</p> <p>The answer lies in the Universal Approximation Theorem\u2014one of the most important theoretical results in deep learning. This theorem tells us that neural networks can, in principle, learn any reasonable function.</p> <p>But here's the surprising part: many classical ML models you already know are actually special cases of neural networks. Linear regression, logistic regression, even aspects of decision trees\u2014they're all points on a spectrum of neural network complexity.</p> <p>This deep dive explores these connections and helps you understand when simple models suffice and when you need the full power of neural networks.</p> <p>Why research continues: The Universal Approximation Theorem is an existence result\u2014it guarantees a solution exists but says nothing about finding it. \"Enough neurons\" might mean exponentially many. The theorem provides no guarantees about training difficulty, data requirements, or generalization. Depth enables compositional learning (f(g(h(x))) is more parameter-efficient than one massive layer). The gap between \"theoretically possible\" and \"practically achievable\" drives ongoing research.</p>"},{"location":"appendices/universal-approximators/#the-universal-approximation-theorem","title":"The Universal Approximation Theorem","text":""},{"location":"appendices/universal-approximators/#statement","title":"Statement","text":"<p>Theorem (Cybenko, 1989; Hornik, 1991):</p> <p>A feedforward neural network with a single hidden layer containing a finite number of neurons can approximate any continuous function on a compact subset of \\(\\mathbb{R}^n\\) to arbitrary accuracy.</p> <p>In simpler terms: Given enough hidden neurons, a one-hidden-layer neural network can learn any reasonable function.</p>"},{"location":"appendices/universal-approximators/#intuition-neurons-as-bump-functions","title":"Intuition: Neurons as \"Bump\" Functions","text":"<p>Each hidden neuron with a sigmoid activation creates an S-shaped curve:</p> <p></p> <p>By combining multiple neurons: - Two sigmoids can create a \"bump\" (rise then fall) - Many bumps can approximate any shape - The more neurons, the smoother the approximation</p>"},{"location":"appendices/universal-approximators/#visual-example-approximating-a-square-wave","title":"Visual Example: Approximating a Square Wave","text":""},{"location":"appendices/universal-approximators/#important-caveats","title":"Important Caveats","text":"<p>The theorem tells us approximation is possible, but NOT: 1. How many neurons are needed (could be astronomically large) 2. How to find the right weights (training might fail) 3. How much data is required (might need infinite samples) 4. Whether it will generalize (might just memorize)</p>"},{"location":"appendices/universal-approximators/#example-1-linear-regression","title":"Example 1: Linear Regression","text":"<p>The simplest case: Neural networks can perform linear regression exactly.</p>"},{"location":"appendices/universal-approximators/#linear-regression-model","title":"Linear Regression Model","text":"\\[\\hat{y} = w_1 x_1 + w_2 x_2 + b\\] <p>For example: \\(\\hat{y} = 2x_1 + 3x_2 + 1\\)</p>"},{"location":"appendices/universal-approximators/#neural-network-equivalent","title":"Neural Network Equivalent","text":"<p>A neural network with: - No hidden layers - Linear activation (or no activation function) - Single output neuron</p> <pre><code>Architecture:\n    x\u2081 \u2500\u2500(w\u2081)\u2500\u2500\u2510\n               \u251c\u2500\u2500 \u03a3 + b \u2500\u2500 \u0177\n    x\u2082 \u2500\u2500(w\u2082)\u2500\u2500\u2518\n</code></pre>"},{"location":"appendices/universal-approximators/#code-demonstration","title":"Code Demonstration","text":"<pre><code>import torch\nimport torch.nn as nn\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\n# Generate synthetic data: y = 2*x1 + 3*x2 + 1 + noise\nnp.random.seed(42)\nn_samples = 1000\nX = np.random.randn(n_samples, 2).astype(np.float32)\ny_true = 2 * X[:, 0] + 3 * X[:, 1] + 1\ny = y_true + 0.1 * np.random.randn(n_samples).astype(np.float32)\n\n# SKLEARN LINEAR REGRESSION\nsklearn_model = LinearRegression()\nsklearn_model.fit(X, y)\nprint(f\"Sklearn: Coefficients={sklearn_model.coef_}, Intercept={sklearn_model.intercept_}\")\n# Output: Coefficients=[1.997, 2.998], Intercept=1.003\n\n# PYTORCH NEURAL NETWORK (equivalent)\nclass LinearRegressionNN(nn.Module):\n    def __init__(self, input_dim):\n        super().__init__()\n        self.linear = nn.Linear(input_dim, 1)  # Single linear layer\n\n    def forward(self, x):\n        return self.linear(x)\n\nnn_model = LinearRegressionNN(input_dim=2)\n# ... training code ...\nprint(f\"PyTorch: Weights={nn_model.linear.weight.data}, Bias={nn_model.linear.bias.data}\")\n# Output: Weights=[1.998, 2.999], Bias=1.002\n</code></pre>"},{"location":"appendices/universal-approximators/#key-insight","title":"Key Insight","text":"<p>Linear regression IS a neural network with: - 0 hidden layers - Linear (identity) activation - MSE loss</p> <p>The sklearn <code>LinearRegression</code> uses a closed-form solution (matrix inverse), while the neural network uses gradient descent, but they converge to the same answer.</p> <p>Why use gradient descent for linear regression? The closed-form solution requires O(n\u00b3) matrix inversion\u2014prohibitive for millions of samples or thousands of features. Stochastic gradient descent scales via mini-batches. Also, closed-form only works for squared error with linear models. Gradient descent handles L1 regularization, non-standard losses, and extends to non-linear models. Use closed-form when small enough; gradient descent otherwise.</p>"},{"location":"appendices/universal-approximators/#example-2-logistic-regression","title":"Example 2: Logistic Regression","text":"<p>Binary classification: Neural networks can implement logistic regression exactly.</p>"},{"location":"appendices/universal-approximators/#logistic-regression-model","title":"Logistic Regression Model","text":"\\[P(y=1|x) = \\sigma(w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}\\]"},{"location":"appendices/universal-approximators/#neural-network-equivalent_1","title":"Neural Network Equivalent","text":"<p>A neural network with: - No hidden layers - Sigmoid activation on the output - Binary cross-entropy loss</p> <pre><code>Architecture:\n    x\u2081 \u2500\u2500(w\u2081)\u2500\u2500\u2510\n               \u251c\u2500\u2500 \u03a3 + b \u2500\u2500 \u03c3 \u2500\u2500 P(y=1)\n    x\u2082 \u2500\u2500(w\u2082)\u2500\u2500\u2518\n</code></pre>"},{"location":"appendices/universal-approximators/#code-demonstration_1","title":"Code Demonstration","text":"<pre><code>from sklearn.linear_model import LogisticRegression\nimport torch.nn as nn\n\n# SKLEARN LOGISTIC REGRESSION\nsklearn_model = LogisticRegression(penalty=None)\nsklearn_model.fit(X, y)\n\n# PYTORCH NEURAL NETWORK (equivalent)\nclass LogisticRegressionNN(nn.Module):\n    def __init__(self, input_dim):\n        super().__init__()\n        self.linear = nn.Linear(input_dim, 1)\n\n    def forward(self, x):\n        return torch.sigmoid(self.linear(x))\n\n# Both produce the same decision boundary!\n</code></pre>"},{"location":"appendices/universal-approximators/#key-insight_1","title":"Key Insight","text":"<p>Logistic regression IS a neural network with: - 0 hidden layers - Sigmoid activation - Binary cross-entropy loss</p> <p>The decision boundary is identical: a linear hyperplane.</p>"},{"location":"appendices/universal-approximators/#example-3-approximating-step-functions","title":"Example 3: Approximating Step Functions","text":"<p>The challenge: Decision trees create axis-aligned step functions. Can neural networks do this?</p>"},{"location":"appendices/universal-approximators/#decision-tree-behavior","title":"Decision Tree Behavior","text":"<pre><code>Decision tree output:\n     1 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502           \u2502\n       \u2502           \u2502\n     0 \u2518           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n              x = 0.5\n       (hard step at threshold)\n</code></pre>"},{"location":"appendices/universal-approximators/#neural-network-approximation","title":"Neural Network Approximation","text":"<p>A ReLU neuron creates a \"bent line\": $\\(\\text{ReLU}(x) = \\max(0, x)\\)$</p> <p></p> <p>Combining ReLUs can approximate steps:</p> <pre><code>def train_step_approximator(n_hidden):\n    \"\"\"Train a network to approximate a step function.\"\"\"\n    model = nn.Sequential(\n        nn.Linear(1, n_hidden),\n        nn.ReLU(),\n        nn.Linear(n_hidden, 1),\n        nn.Sigmoid()\n    )\n    # ... training code ...\n    return model\n\n# Results:\n# 2 neurons: rough approximation\n# 5 neurons: better\n# 50 neurons: nearly exact step\n</code></pre> <p>Observation: More neurons \u2192 sharper approximation of the step.</p>"},{"location":"appendices/universal-approximators/#why-relu-combinations-work","title":"Why ReLU Combinations Work","text":"<p>A single ReLU is a ramp: \\(\\max(0, x)\\)</p> <p>Two ReLUs can make a bump: $\\(f(x) = \\text{ReLU}(x) - \\text{ReLU}(x - 1)\\)$</p> <p></p> <p>Many bumps at different positions \u2192 approximate any shape.</p>"},{"location":"appendices/universal-approximators/#key-insight_2","title":"Key Insight","text":"<p>Neural networks can approximate decision tree boundaries, but: - Trees create exact axis-aligned steps - NNs create smooth approximations that approach steps - With enough neurons, the approximation becomes arbitrarily close</p>"},{"location":"appendices/universal-approximators/#example-4-polynomial-regression","title":"Example 4: Polynomial Regression","text":"<p>Challenge: Learn \\(y = x^2\\) using only linear layers and ReLU.</p>"},{"location":"appendices/universal-approximators/#the-problem","title":"The Problem","text":"<p>A single linear layer can only learn: \\(y = wx + b\\)</p> <p>How can we learn \\(y = x^2\\) without explicitly computing \\(x^2\\)?</p>"},{"location":"appendices/universal-approximators/#the-solution-hidden-layer-creates-basis-functions","title":"The Solution: Hidden Layer Creates Basis Functions","text":"<p>With ReLU activations, the network learns piecewise linear approximations:</p> <pre><code># Train networks with different widths to learn y = x\u00b2\n# Results:\n# 2 neurons: Very rough (2 line segments)\n# 5 neurons: Better (5 segments)\n# 50 neurons: Nearly perfect curve\n# 100 neurons: Indistinguishable from x\u00b2\n</code></pre>"},{"location":"appendices/universal-approximators/#whats-happening-inside","title":"What's Happening Inside","text":"<p>Each ReLU neuron contributes a \"kink\" in the function:</p> <pre><code>Hidden neuron 1: ReLU(w\u2081x + b\u2081) - kink at x = -b\u2081/w\u2081\nHidden neuron 2: ReLU(w\u2082x + b\u2082) - kink at x = -b\u2082/w\u2082\n...\n\nOutput = sum of scaled, shifted kinks = piecewise linear approximation\n</code></pre> <p>The network learns WHERE to put kinks and HOW MUCH each contributes.</p>"},{"location":"appendices/universal-approximators/#key-insight_3","title":"Key Insight","text":"<p>Neural networks don't explicitly compute polynomials\u2014they approximate them with piecewise linear functions. More neurons = more pieces = smoother approximation.</p>"},{"location":"appendices/universal-approximators/#the-unifying-framework","title":"The Unifying Framework","text":"<p>All these models are points on a spectrum:</p> <p></p>"},{"location":"appendices/universal-approximators/#flexibility-trade-offs","title":"Flexibility Trade-offs","text":"Model Flexibility Data Needed Interpretability Overfitting Risk Linear Regression Low Low High Low Logistic Regression Low Low High Low Decision Tree Medium Medium High Medium Shallow NN High Medium Low Medium Deep NN Very High High Very Low High"},{"location":"appendices/universal-approximators/#when-to-use-simpler-models","title":"When to Use Simpler Models","text":"<p>Even though NNs can do everything, simpler models are often better:</p> <ol> <li>Interpretability required: Linear/logistic regression coefficients are meaningful</li> <li>Small data: Simple models generalize better with few samples</li> <li>Fast inference: Linear prediction is O(d), deep NN is O(millions)</li> <li>Debugging: Easier to understand what went wrong</li> <li>Baseline: Always try simple models first</li> </ol>"},{"location":"appendices/universal-approximators/#example-decision","title":"Example Decision","text":"<p>You have 100 samples and want to predict a continuous outcome. Neural network or linear regression?</p> <p>Start with linear regression. With 100 samples, a neural network will likely overfit unless heavily regularized. Linear regression provides a strong baseline and is interpretable.</p> <p>Samples per parameter: Traditional rules (10-20 samples per parameter) don't apply cleanly to neural networks. Modern NNs often work in the overparameterized regime (more parameters than samples) due to implicit regularization from SGD, early stopping, and batch normalization. The honest answer: monitor validation loss. If it diverges from training loss, you're overfitting\u2014apply more regularization, get more data, or use a smaller model.</p>"},{"location":"appendices/universal-approximators/#when-neural-networks-shine","title":"When Neural Networks Shine","text":"<ol> <li>Large data: More samples \u2192 can fit more complex patterns</li> <li>Raw inputs: Images, audio, text need feature learning</li> <li>Complex relationships: Highly non-linear, interacting features</li> <li>Transfer learning: Pre-trained models for your task</li> </ol>"},{"location":"appendices/universal-approximators/#example-decision_1","title":"Example Decision","text":"<p>You have 1 million images and want to classify them. Neural network or logistic regression?</p> <p>Neural network, specifically a CNN. Logistic regression would require hand-engineered features and couldn't capture the spatial patterns that CNNs learn automatically.</p>"},{"location":"appendices/universal-approximators/#common-misconceptions","title":"Common Misconceptions","text":"Misconception Reality \"Neural networks understand data better than simpler models\" NNs fit patterns statistically; simpler models may capture the true underlying structure better \"Universal approximation means NNs are always best\" The theorem says nothing about training difficulty, data requirements, or generalization \"More neurons is always better\" More neurons = more capacity to overfit; regularization and data size matter \"Deep networks are always better than shallow\" For some functions, shallow networks are more efficient; depth helps for compositional structure \"If sklearn and PyTorch give same results, there's no benefit to NNs\" True for simple models, but NNs allow extending to more complex architectures"},{"location":"appendices/universal-approximators/#reflection-questions","title":"Reflection Questions","text":"<ol> <li> <p>If neural networks can approximate any function, why use simpler models?</p> </li> <li> <p>What does a neural network learn that logistic regression doesn't?</p> </li> <li> <p>How many hidden neurons are needed to approximate a degree-n polynomial?</p> </li> <li> <p>The Universal Approximation Theorem says one hidden layer is enough. Why do we use deep networks?</p> </li> <li> <p>Your neural network achieves 100% training accuracy but 60% test accuracy. What happened?</p> </li> <li> <p>A colleague says \"just use a deep neural network for everything.\" What's your response?</p> </li> </ol>"},{"location":"appendices/universal-approximators/#practice-problems","title":"Practice Problems","text":"<ol> <li> <p>Train a neural network to learn \\(\\sin(x)\\) - how many neurons needed for MSE &lt; 0.01?</p> </li> <li> <p>Implement logistic regression as a neural network and verify it matches sklearn</p> </li> <li> <p>Find the minimum network (fewest neurons) that achieves 95% accuracy on XOR</p> </li> <li> <p>Compare training time: sklearn vs PyTorch for logistic regression on a large dataset</p> </li> </ol>"},{"location":"appendices/universal-approximators/#summary","title":"Summary","text":"<p>Key takeaways:</p> <ol> <li> <p>The Universal Approximation Theorem guarantees that neural networks can learn any reasonable function (given enough neurons)</p> </li> <li> <p>Linear regression is a neural network with 0 hidden layers and no activation</p> </li> <li> <p>Logistic regression is a neural network with 0 hidden layers and sigmoid activation</p> </li> <li> <p>Neural networks approximate step functions and polynomials through piecewise linear combinations of ReLU units</p> </li> <li> <p>The theorem guarantees existence but says nothing about training difficulty, data requirements, or generalization</p> </li> <li> <p>Simpler models are often better: interpretability, fewer data requirements, faster training, less overfitting</p> </li> <li> <p>Match model complexity to problem complexity and data size</p> </li> </ol>"},{"location":"modules/01-foundations/","title":"Module 1: Foundations of Machine Learning","text":""},{"location":"modules/01-foundations/#introduction","title":"Introduction","text":"<p>This foundational module establishes everything you'll use throughout the course. The vocabulary, the concepts, the data preparation skills, the evaluation methodology\u2014all of it forms the bedrock of your machine learning practice.</p> <p>By the end of this module, you'll have a conceptual framework for understanding what machine learning actually is, how to prepare data for it, and critically, how to know whether your models are actually working. That last part\u2014evaluation\u2014is where most business ML projects go wrong, so we'll spend significant time there.</p> <p>Evaluation failures typically stem from three categories: data leakage (test set information inadvertently influences training), metric mismatch (optimizing for the wrong measure), and insufficient validation rigor (relying on a single train/test split). Teams focus on building models without equal rigor on proving they work\u2014evaluation methodology should receive as much scrutiny as model architecture.</p>"},{"location":"modules/01-foundations/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, you should be able to:</p> <ol> <li>Differentiate between AI, Machine Learning, Data Science, and related fields</li> <li>Classify business problems into appropriate ML task categories (supervised, unsupervised, reinforcement)</li> <li>Construct data preparation pipelines that prevent data leakage</li> <li>Select appropriate evaluation metrics based on business context</li> <li>Diagnose overfitting and underfitting using the bias-variance framework</li> </ol>"},{"location":"modules/01-foundations/#11-introduction-historical-context","title":"1.1 Introduction &amp; Historical Context","text":""},{"location":"modules/01-foundations/#the-aimldata-science-landscape","title":"The AI/ML/Data Science Landscape","text":"<p>Understanding how these concepts relate to each other matters because these terms are thrown around loosely in industry, and you need to cut through the hype.</p> <p></p> <p>At the outermost level, we have Computer Science\u2014the study of computation, information, and automation. More precisely, computer science is concerned with the theory, design, and application of algorithms: step-by-step procedures for solving problems and processing information.</p> <p>Within that sits Artificial Intelligence\u2014machines that exhibit intelligent behavior. The term was coined in 1956, but the foundations go back further\u2014Alan Turing's 1950 paper \"Computing Machinery and Intelligence\" proposed the Turing test: can a machine's responses be indistinguishable from a human's? AI is a broad umbrella that includes rule-based systems, expert systems, and machine learning.</p> <p>Inside AI, we have Machine Learning\u2014systems that learn from data without being explicitly programmed. This is our focus for the course. The key distinction is learning from data. Instead of a human writing rules, the system discovers patterns from examples.</p> <p>And inside ML, we have Deep Learning\u2014machine learning using neural networks with many layers. Deep learning has driven most of the recent AI breakthroughs, but it's just one approach within ML.</p> <p>Data Science sits alongside and overlaps with all of these. Data Science is about extracting insights from data\u2014it combines statistics, domain expertise, and programming. A data scientist might use ML, or might use traditional statistical methods, or might just create visualizations. It's about the goal (insights from data), not the method.</p> <p>In practice, these boundaries are fuzzy and a single project often spans multiple domains. A customer churn project might involve data science (exploratory analysis), machine learning (predictive model), and software engineering (deployment). The skills transfer across domains: wrangling data, building models, evaluating rigorously, and communicating effectively.</p>"},{"location":"modules/01-foundations/#key-definitions","title":"Key Definitions","text":"Term Definition Artificial Intelligence Machines that exhibit intelligence (broad umbrella) Machine Learning Systems that learn from data without being explicitly programmed Deep Learning ML using neural networks with many layers Data Science Extracting insights from data (may or may not use ML) <p>Important nuance: These terms are used loosely in industry. Job postings for \"AI Engineer\" and \"ML Engineer\" and \"Data Scientist\" often describe the same role. Help yourself by understanding what people actually mean, not just what they say. When evaluating roles, look at specific tools (SQL, Python, TensorFlow), deliverables (reports, dashboards, deployed models), and team structure rather than job titles.</p>"},{"location":"modules/01-foundations/#historical-timeline","title":"Historical Timeline","text":"<p>Understanding where ML came from helps you understand why it works the way it does\u2014and why we've seen cycles of hype and disappointment.</p>"},{"location":"modules/01-foundations/#early-years","title":"Early Years","text":"Year Milestone Significance 1847 Gradient descent published (Cauchy) The optimization algorithm that powers nearly all modern ML 1950 Turing Test proposed Defined the question \"Can machines think?\" 1957 Perceptron invented First neural network, sparked initial optimism 1969 Minsky &amp; Papert's \"Perceptrons\" Showed limitations, contributed to AI Winter 1980s Expert Systems boom Rule-based AI, eventually hit scalability limits 1984 CART published Foundation for all tree-based methods 1986 Backpropagation popularized Enabled training multi-layer networks 1997 Deep Blue beats Kasparov Specialized AI success, but not learning"},{"location":"modules/01-foundations/#modern-era","title":"Modern Era","text":"Year Milestone Significance 2001 Random Forests published Go-to algorithm for tabular data 2006 Deep learning revival; CUDA released New training techniques and GPU computing 2012 AlexNet wins ImageNet Deep learning breakthrough, GPU training 2014 XGBoost released Dominates Kaggle, still state-of-the-art for tabular data 2016 AlphaGo beats Lee Sedol Reinforcement learning milestone 2017 \"Attention Is All You Need\" Transformer architecture, foundation for GPT/BERT 2018 BERT released Transfer learning comes to NLP 2020 GPT-3 Scaling produces emergent capabilities 2022 ChatGPT released Large language models go mainstream <p>AI Winters were caused by overpromising followed by underdelivering\u2014researchers making bold claims to secure funding, then hitting fundamental limitations. The lesson is to be realistic about what current technology can and cannot do.</p> <p>The key insight for business: Most AI projects fail due to poor problem definition, not technical limitations. Getting the problem right matters more than getting the algorithm right. Poor problem definition manifests as vague objectives (\"use AI to improve customer experience\"), wrong target variables (predicting email responses when the business needs conversions), or misaligned success metrics (optimizing call duration when customer satisfaction is the goal). Before writing any code, get crystal clear on: What exactly are we predicting? How will predictions be used? What decisions will change?</p>"},{"location":"modules/01-foundations/#ml-task-categories","title":"ML Task Categories","text":"<p>Machine Learning branches into three main categories:</p> <p>Supervised Learning: You have labeled data, and you want to predict labels for new data. The \"supervision\" comes from the labels\u2014they tell the algorithm what the right answer is. - Regression: Predicting continuous values (numbers)   - Examples: Sales forecasting, price prediction, demand estimation - Classification: Predicting categories (discrete labels)   - Examples: Spam detection, customer churn, fraud detection</p> <p>Unsupervised Learning: No labels. You're trying to find hidden structure in the data. - Clustering: Grouping similar items together   - Examples: Customer segmentation, document grouping, anomaly detection - Dimensionality Reduction: Compressing many features into fewer features   - Examples: Visualization, noise reduction, feature extraction</p> <p>Reinforcement Learning: The algorithm learns optimal actions through trial and error, receiving rewards or penalties for its choices. - Examples: Game playing, robotics, recommendation systems - Brief overview only\u2014not a focus of this course</p> <p>The choice between regression and classification depends on what decision the prediction enables. If the business needs a specific number (\"How many units will we sell?\"), that's regression. If it needs a category (\"Will this customer churn?\"), that's classification. Many problems could be framed either way\u2014a common pattern is to build a regression model (predict probability) and threshold it for classification decisions, giving you both continuous scores for prioritization and discrete classes for action.</p>"},{"location":"modules/01-foundations/#business-applications-by-industry","title":"Business Applications by Industry","text":"Industry Application ML Type Retail Demand forecasting Regression Finance Fraud detection Classification Marketing Customer segmentation Clustering Healthcare Disease diagnosis Classification Manufacturing Predictive maintenance Classification E-commerce Product recommendations Various <p>Classification appears frequently in business because we often need to make decisions: approve or deny, flag or pass, target or ignore. Pedagogically, we learn regression first because it's simpler\u2014you can visualize a line through points, and the loss function (MSE) is intuitive. Many core concepts (features, coefficients, overfitting, regularization) work identically in both settings, so learning them in the simpler regression context means you can focus on concepts rather than classification-specific complications.</p>"},{"location":"modules/01-foundations/#common-misconceptions","title":"Common Misconceptions","text":"Misconception Reality \"AI and ML are the same thing\" ML is a subset of AI. AI includes rule-based systems that don't learn from data. \"ML will replace all human decision-making\" ML augments human decisions. Many problems require human judgment, ethics, and contextual understanding. \"Deep Learning is always better than traditional ML\" Deep learning requires lots of data and compute. For tabular business data, traditional ML (XGBoost, Random Forest) often wins. \"More data always leads to better models\" Data quality matters more than quantity. Biased or noisy data leads to biased or noisy models. <p>How much data is \"enough\"? For classical ML, a common rule of thumb is 10-30 samples per feature for linear models. For deep learning, you typically need thousands to millions of samples, though transfer learning reduces this. The practical test: plot learning curves. If validation performance is still improving as you add data, you need more. If it's plateaued, more data won't help\u2014you need better features or a different model.</p>"},{"location":"modules/01-foundations/#12-data-preparation-feature-engineering","title":"1.2 Data Preparation &amp; Feature Engineering","text":""},{"location":"modules/01-foundations/#the-golden-rule-garbage-in-garbage-out","title":"The Golden Rule: Garbage In, Garbage Out","text":"<p>Data preparation often takes 80% of project time but determines success or failure. This isn't glamorous work, but it's where projects succeed or fail.</p> <p>\"The algorithm is not the hard part. Getting the data right is the hard part.\"</p> <p>Your job as a business analytics professional is often more about data preparation than algorithm selection. The algorithm is almost a commodity at this point\u2014scikit-learn gives you excellent implementations of everything. What matters is what you feed it.</p> <p>Since algorithms are implemented for us, focus on higher-leverage skills: problem framing (translating vague business requests into well-defined ML problems), data intuition (recognizing quality issues and predictive features), evaluation rigor (proper validation setup), communication (explaining results to stakeholders), and debugging (diagnosing whether issues are data quality, feature engineering, or methodology). Libraries implement algorithms; they don't tell you which algorithm to use or whether your features make sense.</p>"},{"location":"modules/01-foundations/#common-data-quality-issues","title":"Common Data Quality Issues","text":"<ol> <li>Missing values: NaN, null, empty strings, placeholder values (-999, \"N/A\")</li> <li>Duplicates: Exact duplicates or near-duplicates</li> <li>Outliers: Extreme values (errors vs. legitimate rare events)</li> <li>Inconsistent formatting: \"USA\" vs \"United States\" vs \"US\"</li> <li>Data entry errors: Typos, wrong units, swapped fields</li> </ol>"},{"location":"modules/01-foundations/#initial-data-exploration","title":"Initial Data Exploration","text":"<p>Before you do anything else, explore your data:</p> <pre><code>import polars as pl\n\ndf.shape              # How big is this? (rows, columns)\ndf.schema             # What are the data types?\ndf.null_count()       # Where are the missing values?\ndf.describe()         # Basic statistics\ndf.is_duplicated().sum()  # Any duplicate rows?\n</code></pre> <p>Always explore before modeling. Don't jump straight into building models without checking basic things like \"are there missing values?\"</p> <p>A reasonable heuristic is to spend 10-20% of your total project time on EDA before modeling. You've explored \"enough\" when you can answer: What are the data types and ranges? Where are missing values and what causes them? Are there obvious outliers? What is the target distribution? Which features correlate with the target? The goal is to catch major issues\u2014I've seen projects waste weeks on sophisticated modeling only to discover the target was incorrectly defined. A few hours of EDA would have saved that time.</p>"},{"location":"modules/01-foundations/#handling-missing-data","title":"Handling Missing Data","text":"<p>Strategy 1: Deletion - Drop rows with any missing values - Simple, but you lose information and might introduce bias - When to use: Missing completely at random, small percentage missing</p> <p>Strategy 2: Imputation - Mean/Median/Mode: Simple, but ignores relationships between variables - Forward/Backward fill: For time series\u2014use the previous or next value - Model-based (k-NN): Predict the missing value from other features</p> <pre><code>from sklearn.impute import SimpleImputer, KNNImputer\n\n# Simple imputation\nimputer = SimpleImputer(strategy='median')\nX_imputed = imputer.fit_transform(X)\n\n# K-NN imputation (considers relationships)\nknn_imputer = KNNImputer(n_neighbors=5)\nX_imputed = knn_imputer.fit_transform(X)\n</code></pre>"},{"location":"modules/01-foundations/#why-data-is-missing-matters","title":"Why Data is Missing Matters","text":"<p>The reason data is missing determines what you should do about it:</p> Type Description Implication MCAR (Missing Completely at Random) Missingness has nothing to do with any values Safe to delete MAR (Missing at Random) Missingness is related to other observed variables Imputation can work MNAR (Missing Not at Random) Missingness is related to the missing value itself Problematic\u2014missingness is informative <p>Example: If high-income people don't report their income, that's MNAR\u2014you can't fully recover the missing information because it's biased.</p> <p>Diagnosing missingness type requires evidence and domain knowledge. For MCAR, compare distributions of other variables between rows with and without missing values\u2014they should be similar if missingness is random. For MAR vs MNAR, build a model predicting whether a value is missing using other features; if it has predictive power, missingness is at least partially MAR. Domain knowledge is essential: ask why data might be missing and whether that reason relates to the missing value itself.</p>"},{"location":"modules/01-foundations/#feature-scaling","title":"Feature Scaling","text":"<p>Many algorithms are sensitive to the scale of features. If one feature ranges from 0-1 and another from 0-1,000,000, the larger feature will dominate.</p> <p>Algorithms that need scaling: - Linear regression, SVM, k-NN, neural networks - Regularized methods (regularization only penalizes fairly when features are scaled)</p> <p>Standardization (Z-score): $\\(x_{scaled} = \\frac{x - \\mu}{\\sigma}\\)$ - Centers at 0, standard deviation of 1 - Preserves outliers - Use when: Algorithm assumes normally distributed data</p> <p>Min-Max Normalization: $\\(x_{scaled} = \\frac{x - x_{min}}{x_{max} - x_{min}}\\)$ - Scales to [0, 1] range - Sensitive to outliers - Use when: Need bounded range (neural networks)</p> <pre><code>from sklearn.preprocessing import StandardScaler, MinMaxScaler\n\n# Standardization\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)  # Use training parameters!\n\n# Min-Max\nminmax = MinMaxScaler()\nX_train_scaled = minmax.fit_transform(X_train)\n</code></pre> <p>When NOT to scale: Tree-based methods (Decision Trees, Random Forest, XGBoost) are scale-invariant! They make splits based on thresholds, not distances. Don't waste time scaling for tree-based models.</p> <p>If you're unsure which algorithm you'll use, wait until you've chosen one, or incorporate scaling into your pipeline so it's applied conditionally. Different algorithms prefer different scaling (neural networks work better with MinMax [0,1], while SVMs use standardization). The practical solution is sklearn Pipelines\u2014create a pipeline where scaling is a step before the model, which also prevents data leakage during cross-validation.</p>"},{"location":"modules/01-foundations/#outlier-detection","title":"Outlier Detection","text":"<p>Methods for different data distributions:</p> <p>For normally distributed data: - Z-score method: Points more than 3 standard deviations from the mean are outliers</p> <p>For non-normal or unknown distributions: - IQR method: Outliers are below Q1 - 1.5\u00d7IQR or above Q3 + 1.5\u00d7IQR (what box plots use) - Modified Z-score: Uses median and MAD instead of mean and std\u2014very robust - Isolation Forest: Tree-based method that works well in high dimensions</p> <pre><code>from sklearn.ensemble import IsolationForest\nclf = IsolationForest(contamination=0.05)  # Expect ~5% outliers\noutliers = clf.fit_predict(X)  # Returns -1 for outliers\n</code></pre> <p>What to do with outliers: Investigate first! Are they data errors? Legitimate extreme values? Different populations? Options include: remove, cap/winsorize, transform (log), or use robust methods.</p> <p>Critical distinction: Outliers in features vs. outliers as targets are completely different problems. If you're detecting fraud, fraudulent transactions ARE what you're trying to predict\u2014they're your positive class, not outliers to remove. Never remove outliers blindly based on statistics alone. Investigate: are they errors, rare-but-legitimate events, or the signal you're looking for? For anomaly detection tasks, use algorithms designed to find outliers (Isolation Forest, One-Class SVM), don't remove them.</p>"},{"location":"modules/01-foundations/#encoding-categorical-variables","title":"Encoding Categorical Variables","text":"<p>ML algorithms need numbers, not strings. When you have categorical variables, you need to convert them.</p> <p>One-Hot Encoding: - Creates binary column for each category - Use for: Nominal categories (no order), small number of categories - Watch out: High cardinality (many categories) creates many columns</p> <p>Label Encoding: - Assigns integer to each category - Use for: Ordinal categories (low/medium/high) - Watch out: Implies ordering where none exists</p> <p>Target Encoding: - Replace category with mean of target variable for that category - Use for: High cardinality categories (ZIP codes, product SKUs) - Watch out: Data leakage! Must use cross-validation</p> <p>Target encoding causes leakage because when you calculate the mean target value for a category, you're using information from rows you'll later predict\u2014each row's outcome influences its own encoded feature value. The severity scales with category size (worse for rare categories). The solution is cross-validation-style encoding: for each row, calculate the category mean using only OTHER rows. Libraries like <code>category_encoders</code> implement this properly.</p> <pre><code>from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n\n# One-hot encoding\nencoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\nX_encoded = encoder.fit_transform(X[['category_column']])\n\n# With polars (simpler for exploration)\nX_encoded = df.to_dummies(columns=['category_column'])\n</code></pre>"},{"location":"modules/01-foundations/#the-cardinal-rule-preventing-data-leakage","title":"The Cardinal Rule: Preventing Data Leakage","text":"<p>Never let information from the test set influence training!</p> <p>This is called data leakage, and it will give you overly optimistic results that don't hold up in production.</p> <p>Common leakage examples: 1. Scaling before splitting: Scaler sees test data statistics 2. Feature selection on all data: Test data influences feature choice 3. Target encoding without proper CV: Test data target values leak 4. Time series: future predicts past: Future information used for past predictions</p> <p>The correct workflow:</p> <pre><code>1. Split data FIRST\n   \u2514\u2500\u2192 Training set | Test set\n\n2. Fit preprocessing on TRAINING only\n   \u2514\u2500\u2192 scaler.fit_transform(X_train)\n\n3. Transform test using training parameters\n   \u2514\u2500\u2192 scaler.transform(X_test)\n</code></pre> <p>Handling extrapolation (test data with values outside training range): For standardization, values get z-scores beyond the training range\u2014most models handle this gracefully. For MinMax scaling, values might exceed [0,1], consider clipping. For one-hot encoding, new categories are problematic\u2014set <code>handle_unknown='ignore'</code> to assign zeros. Check whether training data covers the expected production range and monitor for out-of-distribution inputs.</p> <pre><code>from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Step 1: Split FIRST\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Step 2: Fit on training ONLY\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\n\n# Step 3: Transform test using training parameters\nX_test_scaled = scaler.transform(X_test)  # NOT fit_transform!\n</code></pre> <p>Understanding fit, transform, and fit_transform:</p> Method What it does When to use <code>fit()</code> Learns parameters from data (e.g., mean, std) When you only need to learn, not transform <code>transform()</code> Applies learned parameters to transform data On test data (using parameters learned from train) <code>fit_transform()</code> Does both in one step On training data (learn + transform together)"},{"location":"modules/01-foundations/#common-misconceptions_1","title":"Common Misconceptions","text":"Misconception Reality \"More features always improve models\" Too many features can cause overfitting. Feature selection is often necessary. \"Just drop all rows with missing values\" This can introduce bias and waste data. Imputation is often better. \"Always standardize your features\" Tree-based models don't need scaling. Know your algorithm! \"One-hot encoding is always best\" High-cardinality features may need target encoding or embeddings. <p>Embeddings (covered in detail in Module 6) are learned dense vector representations for high-cardinality categories. Instead of one-hot encoding (millions of columns for product IDs) or target encoding (loses information), each category gets a small vector of continuous values learned during training. The model figures out which categories are \"similar\" based on their relationship to the target.</p>"},{"location":"modules/01-foundations/#13-model-evaluation-validation","title":"1.3 Model Evaluation &amp; Validation","text":""},{"location":"modules/01-foundations/#why-evaluation-matters","title":"Why Evaluation Matters","text":"<p>How do we know if a model is actually good? This seems straightforward, but it's actually subtle: - Good on what metric? Different metrics capture different aspects of performance. - Good compared to what baseline? 85% accuracy might be great or terrible depending on context. - Will it work on new, unseen data? Performance on training data is meaningless if it doesn't generalize.</p> <p>Always compare to a meaningful baseline: for regression, predict the mean (any positive R\u00b2 beats this); for classification, predict the majority class (90% accuracy from always predicting the majority in a 90/10 dataset). In time series, \"predict yesterday's value\" is a common baseline. If your model doesn't substantially beat these simple baselines, either the problem is harder than expected or something is wrong with your setup.</p>"},{"location":"modules/01-foundations/#regression-metrics","title":"Regression Metrics","text":"<p>Mean Squared Error (MSE): $\\(MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2\\)$ - Penalizes large errors heavily (squared) - Units are squared (hard to interpret)</p> <p>Root Mean Squared Error (RMSE): $\\(RMSE = \\sqrt{MSE}\\)$ - Same units as target variable (interpretable) - Most common regression metric - \"On average, our predictions are off by $X\"</p> <p>Mean Absolute Error (MAE): $\\(MAE = \\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y}_i|\\)$ - Less sensitive to outliers than RMSE - Linear penalty</p> <p>R\u00b2 (Coefficient of Determination): $\\(R^2 = 1 - \\frac{\\sum(y_i - \\hat{y}_i)^2}{\\sum(y_i - \\bar{y})^2}\\)$ - Proportion of variance explained - 1 = perfect, 0 = no better than mean prediction - Can be negative if model is worse than mean!</p> <p>Mean Absolute Percentage Error (MAPE): $\\(MAPE = \\frac{100\\%}{n}\\sum_{i=1}^{n}\\left|\\frac{y_i - \\hat{y}_i}{y_i}\\right|\\)$ - Scale-independent (percentage) - Easy for stakeholders: \"predictions are off by 5% on average\" - Undefined when y = 0</p> <p>Which to use?</p> Metric Use When RMSE Default choice, care about large errors MAE Outliers in target, want robustness R\u00b2 Comparing models, explaining to stakeholders MAPE Need percentage interpretation, values not near zero <p>For stakeholders, translate metrics to business language: RMSE becomes \"predictions are off by $15,000 on average\"; MAPE becomes \"predictions are typically within 5%\"; R\u00b2 becomes \"our model captures 75% of the predictive signal.\" Best practice: connect to business outcomes\u2014\"we'll avoid $200K in overstock costs annually.\" Stakeholders care about business impact, not statistical properties.</p>"},{"location":"modules/01-foundations/#classification-metrics","title":"Classification Metrics","text":"<p>Everything starts with the confusion matrix:</p> <p></p> <ul> <li>TP (True Positive): Predicted positive, actually positive. Correct.</li> <li>TN (True Negative): Predicted negative, actually negative. Correct.</li> <li>FP (False Positive): Predicted positive, actually negative. False alarm. Type I error.</li> <li>FN (False Negative): Predicted negative, actually positive. Missed it. Type II error.</li> </ul> <p>Accuracy: $\\(Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}\\)$ - Proportion of correct predictions - Misleading with imbalanced classes! A model predicting \"not fraud\" for everything gets 99% accuracy if 99% of transactions are legitimate\u2014but it catches zero fraud.</p> <p>Precision: $\\(Precision = \\frac{TP}{TP + FP}\\)$ - \"Of those we predicted positive, how many were actually positive?\" - High precision = few false alarms</p> <p>Recall (Sensitivity): $\\(Recall = \\frac{TP}{TP + FN}\\)$ - \"Of actual positives, how many did we catch?\" - High recall = few missed positives</p> <p>F1 Score: $\\(F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}\\)$ - Harmonic mean of precision and recall - Use when you need to balance both</p> <p>The harmonic mean penalizes extreme imbalances more than the arithmetic mean. With precision=0.99 and recall=0.01, the arithmetic mean is 0.50 (suggesting \"medium\" performance), but F1 is just 0.02\u2014correctly reflecting the model is nearly useless. A model can't compensate for terrible recall with great precision; F1 remains low. If you care more about one metric (e.g., recall for medical diagnosis), optimize that directly.</p> <p>AUC-ROC: - Area Under the ROC Curve - Measures ranking ability across all thresholds - 0.5 = random, 1.0 = perfect</p> <p>The ROC curve plots True Positive Rate vs. False Positive Rate at different classification thresholds. The diagonal line represents random guessing; a perfect model hugs the top-left corner. AUC=0.8 means if you randomly pick one positive and one negative example, there's an 80% chance the model scores the positive higher\u2014independent of the threshold you'll use in production.</p> <pre><code>from sklearn.metrics import (\n    accuracy_score, precision_score, recall_score,\n    f1_score, roc_auc_score, confusion_matrix,\n    classification_report\n)\n\n# All-in-one report\nprint(classification_report(y_test, y_pred))\n</code></pre>"},{"location":"modules/01-foundations/#cross-validation","title":"Cross-Validation","text":"<p>A single train/test split can be lucky or unlucky. Cross-validation gives us: - More reliable performance estimate - Confidence interval (mean \u00b1 standard deviation) - Uses all data for both training and validation</p> <p>K-Fold Cross-Validation:</p> <p></p> <ol> <li>Split data into K folds</li> <li>Train on K-1 folds, validate on remaining fold</li> <li>Repeat K times, each fold as validation once</li> <li>Average the results</li> </ol> <p>Stratified K-Fold: Essential for imbalanced data\u2014maintains class distribution in each fold. Without stratification, regular K-Fold can create folds with very different class distributions, causing unreliable estimates (high variance CV scores) and training on unrealistic distributions. Use <code>StratifiedKFold</code> by default for classification\u2014it never hurts.</p> <p>Time Series Split: Respects temporal ordering\u2014train on past, validate on future. Shuffling time series creates temporal leakage\u2014using future information to predict the past, which is impossible in production. Models can report 90% accuracy with shuffled validation and 55% with proper temporal validation. Use <code>TimeSeriesSplit</code> to mimic production conditions.</p> <pre><code>from sklearn.model_selection import (\n    cross_val_score, KFold, StratifiedKFold, TimeSeriesSplit\n)\n\n# Basic K-Fold\nscores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\nprint(f\"Mean: {scores.mean():.3f} (+/- {scores.std():.3f})\")\n\n# Stratified for classification\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nscores = cross_val_score(model, X, y, cv=cv)\n\n# Time series\ntscv = TimeSeriesSplit(n_splits=5)\nscores = cross_val_score(model, X, y, cv=tscv)\n</code></pre>"},{"location":"modules/01-foundations/#overfitting-vs-underfitting","title":"Overfitting vs Underfitting","text":"<p>Underfitting (High Bias): - Training error HIGH, Test error HIGH - Model too simple to capture patterns - Solutions: More features, more complex model, less regularization</p> <p>Overfitting (High Variance): - Training error LOW, Test error HIGH - Model memorizes training data including noise - Solutions: More data, simpler model, regularization, early stopping</p> <p>Diagnostic pattern: Look at the gap between training and test error. Both high = underfitting. Training low but test high = overfitting.</p> <p>If both training and test error are low, that's the goal\u2014but it doesn't guarantee perfection. Still check for: data leakage (too good to be true?), non-representative test sets, wrong metrics (high accuracy but terrible minority class performance), or overfitting to the test set from repeated model selection. Monitor performance after deployment for concept drift.</p>"},{"location":"modules/01-foundations/#the-bias-variance-tradeoff","title":"The Bias-Variance Tradeoff","text":"<p>Total Error = Bias\u00b2 + Variance + Irreducible Noise</p> <p>Bias is systematic error\u2014the model's tendency to miss patterns. High bias means underfitting. A linear model trying to fit a curved relationship has high bias.</p> <p>Variance is sensitivity to training data\u2014how much the model changes with different training samples. High variance means overfitting. A very deep decision tree has high variance.</p> <p>The tradeoff: Reducing bias usually increases variance (more complex model). Reducing variance usually increases bias (simpler model). The goal is to find the sweet spot.</p> <p>There are ways to reduce both simultaneously: more data lets you fit complex models without overfitting; ensemble methods like Random Forests reduce variance while boosting reduces bias; better features make patterns easier to learn. Irreducible noise sets a floor on total error, and at any given data size there is still a tradeoff\u2014these techniques shift the curve inward but don't eliminate it.</p> <p>The Dart-Throwing Analogy:</p> Scenario Bias Variance Pattern High bias, low variance Off-center Clustered together Consistent but wrong Low bias, high variance Centered on average Scattered everywhere Right on average but inconsistent Ideal Centered Clustered Accurate and precise"},{"location":"modules/01-foundations/#business-specific-evaluation","title":"Business-Specific Evaluation","text":"<p>Not all errors cost the same!</p> Domain False Positive Cost False Negative Cost Spam filter Important email missed Spam in inbox Medical diagnosis Unnecessary treatment Missed disease Fraud detection Investigation cost Fraud loss Manufacturing QC Discarding good product Shipping defect <p>For spam, a false positive (real email marked as spam) is much worse than a false negative. Optimize for precision.</p> <p>For medical diagnosis, a false negative (missed disease) is much worse. Optimize for recall.</p> <p>Choose metrics that reflect business costs. Accuracy is misleading because it treats all errors as equal when they rarely are.</p> <p>To quantify costs, work backwards from business outcomes: (1) identify the action taken for each prediction (predicted churn \u2192 retention offer), (2) quantify costs for each outcome (false positive blocks a $200 customer, false negative lets $500 fraud through), (3) build a cost matrix multiplying confusion matrix cells by costs, (4) optimize threshold for minimum total cost. Start with rough estimates, get stakeholder buy-in, and refine over time\u2014approximate cost quantification beats implicitly assuming all errors cost the same.</p>"},{"location":"modules/01-foundations/#common-misconceptions_2","title":"Common Misconceptions","text":"Misconception Reality \"Higher R\u00b2 always means better model\" Can overfit to get high R\u00b2. Test set R\u00b2 is what matters. R\u00b2 can be negative! \"Accuracy is the best metric for classification\" Misleading for imbalanced classes. Use precision/recall/F1/AUC instead. \"More complex models are always better\" Complexity increases variance. Simpler models often generalize better. \"Cross-validation eliminates the need for a test set\" CV estimates performance but you should still have a final holdout test set."},{"location":"modules/01-foundations/#14-python-ecosystem-setup","title":"1.4 Python Ecosystem Setup","text":""},{"location":"modules/01-foundations/#jupyter-notebooks-and-google-colab","title":"Jupyter Notebooks and Google Colab","text":"<p>Jupyter Notebooks: - Interactive computing environment - Mix code, output, and documentation - Great for exploration and teaching</p> <p>Google Colab advantages: - No setup required - Free GPU access - Easy sharing - Pre-installed packages</p> <p>Best practices: - Use markdown cells for documentation - Keep cells focused (one logical step per cell) - Restart and run all before sharing - Use consistent naming conventions</p> <p>\"Restart and Run All\" prevents hidden state issues: cells run out of order, deleted cells leaving ghost variables, or imports removed but modules still loaded. If it fails, your notebook has hidden dependencies. If it succeeds, anyone can reproduce your results. Do this before every commit or share.</p>"},{"location":"modules/01-foundations/#marimo-notebooks","title":"marimo Notebooks","text":"<p>marimo is a next-generation Python notebook that solves many of Jupyter's pain points. Unlike Jupyter's manual cell execution, marimo notebooks are reactive\u2014when you change a variable, all cells that depend on it automatically re-execute. This eliminates the hidden state problems that plague Jupyter notebooks.</p> <p>Key differences from Jupyter:</p> Feature Jupyter marimo Execution Manual cell runs Reactive (auto-updates) File format JSON (.ipynb) Pure Python (.py) Reproducibility State can diverge from code Always reproducible Version control Difficult diffs Clean git diffs Cell ordering Can run out of order Execution order enforced <p>When to use each: - Jupyter/Colab: Quick exploration, collaboration with non-technical stakeholders, free GPU access (Colab) - marimo: Production notebooks, version control, reproducibility, teaching environments where you want guaranteed consistency</p> <p>In this course, we use marimo for most notebooks because the reactive execution model prevents the \"run cells out of order\" bugs that commonly confuse students. The pure Python format also means you can use standard development tools like linters and formatters.</p>"},{"location":"modules/01-foundations/#pixi-package-manager","title":"pixi Package Manager","text":"<p>pixi is a modern package manager that handles Python environments with speed and reproducibility. It uses the same package repository as conda but with faster dependency resolution and a cleaner project structure.</p> <p>Why pixi for this course? - Fast: Resolves dependencies in seconds, not minutes - Reproducible: Lock files ensure everyone has identical environments - Cross-platform: Same configuration works on Windows, Mac, and Linux - Simple: One configuration file (<code>pixi.toml</code>) defines your entire project</p> <p>Installation:</p> <pre><code># macOS/Linux\ncurl -fsSL https://pixi.sh/install.sh | bash\n\n# Windows (PowerShell)\niwr -useb https://pixi.sh/install.ps1 | iex\n</code></pre> <p>After installation, restart your terminal and verify with <code>pixi --version</code>.</p> <p>Project structure:</p> <p>A pixi project is defined by a <code>pixi.toml</code> file:</p> <pre><code>[project]\nname = \"ban501-module1\"\nversion = \"0.1.0\"\nchannels = [\"conda-forge\"]\nplatforms = [\"linux-64\", \"osx-arm64\", \"win-64\"]\n\n[dependencies]\npython = \"&gt;=3.11\"\npolars = \"&gt;=1.0\"\nscikit-learn = \"&gt;=1.5\"\nmatplotlib = \"&gt;=3.8\"\nseaborn = \"&gt;=0.13\"\nmarimo = \"&gt;=0.9\"\n</code></pre> <p>Common commands:</p> Command Description <code>pixi install</code> Install all dependencies from pixi.toml <code>pixi add polars</code> Add a new dependency <code>pixi run python script.py</code> Run Python in the project environment <code>pixi run marimo edit notebook.py</code> Open marimo notebook in project environment <code>pixi shell</code> Activate the environment in your shell <p>Typical workflow:</p> <pre><code># Clone or create a project\ncd my-project\n\n# Install dependencies (creates/updates lock file)\npixi install\n\n# Run your code\npixi run python analysis.py\n\n# Or activate the environment for interactive work\npixi shell\npython\n</code></pre>"},{"location":"modules/01-foundations/#polars-essentials","title":"polars Essentials","text":"<p>Core operations you need to know:</p> <pre><code>import polars as pl\nimport numpy as np\n\n# Reading data\ndf = pl.read_csv('data.csv')\n\n# Basic exploration\ndf.head()           # First 5 rows\ndf.schema           # Data types (column name -&gt; dtype mapping)\ndf.describe()       # Statistical summary\ndf.shape            # (rows, columns)\n\n# Selection\ndf.select('column')              # Single column\ndf.select(['col1', 'col2'])      # Multiple columns\ndf.row(0)                        # Single row by index\ndf[0:5]                          # Slice rows\n\n# Filtering\ndf.filter(pl.col('age') &gt; 30)\ndf.filter((pl.col('age') &gt; 30) &amp; (pl.col('income') &gt; 50000))\n\n# Aggregation\ndf.group_by('category').agg(pl.all().mean())\ndf.group_by('category').agg([\n    pl.col('sales').sum(),\n    pl.col('customers').count()\n])\n\n# Handling missing values\ndf.null_count()                    # Count nulls per column\ndf.drop_nulls()                    # Drop rows with any nulls\ndf.fill_null(0)                    # Fill with constant\ndf.fill_null(strategy='mean')      # Fill with mean\n</code></pre>"},{"location":"modules/01-foundations/#basic-eda-workflow","title":"Basic EDA Workflow","text":"<p>Standard exploration sequence:</p> <ol> <li>Load and inspect: Shape, dtypes, head/tail. Check for obvious issues.</li> <li>Missing values: Count per column, visualize missingness patterns.</li> <li>Univariate analysis: Distributions of each feature, identify outliers.</li> <li>Bivariate analysis: Correlations, feature vs. target relationships.</li> <li>Document findings: Key insights, data quality issues, feature engineering ideas.</li> </ol> <p>Write EDA documentation for your future self six months from now\u2014you'll have forgotten everything. Minimum viable documentation: what questions were you answering? What did you find? What decisions did you make based on findings? What concerns remain? Document conclusions and decisions, not every chart. For formal projects, a separate EDA report for stakeholders should tell a story; the working notebook is for reproducibility.</p>"},{"location":"modules/01-foundations/#visualization-with-matplotlibseaborn","title":"Visualization with matplotlib/seaborn","text":"<pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Distribution\nplt.figure(figsize=(10, 6))\nsns.histplot(df['column'], kde=True)\nplt.title('Distribution of Column')\nplt.show()\n\n# Correlation heatmap\nplt.figure(figsize=(12, 8))\nsns.heatmap(df.corr(), annot=True, cmap='coolwarm', center=0)\nplt.title('Correlation Matrix')\nplt.show()\n\n# Scatter plot with hue\nsns.scatterplot(data=df, x='feature1', y='feature2', hue='target')\n\n# Box plot for outliers\nsns.boxplot(data=df, x='category', y='value')\n\n# Pair plot (for small number of features)\nsns.pairplot(df, hue='target')\n</code></pre> <p>Visualization principles: - Always label axes - Use appropriate chart types - Consider colorblind-friendly palettes - Don't clutter\u2014one message per chart</p>"},{"location":"modules/01-foundations/#gpu-computing-and-cuda","title":"GPU Computing and CUDA","text":"<p>In Section 1.1, we mentioned that NVIDIA released CUDA in 2006 and that AlexNet's 2012 ImageNet victory was enabled by GPU training. Understanding why GPUs matter for machine learning is essential context for the deep learning modules later in this course.</p> <p>Why GPUs matter for ML:</p> <p>CPUs (Central Processing Units) are designed for sequential tasks\u2014they have a few powerful cores that excel at complex operations one at a time. GPUs (Graphics Processing Units) take the opposite approach: thousands of simpler cores that perform many calculations simultaneously.</p> <p>Neural network training involves the same operation (multiply-accumulate) applied to millions of numbers. A CPU processes these one at a time; a GPU processes thousands in parallel. This is why training that takes weeks on a CPU can finish in hours on a GPU.</p> <p>CUDA and the deep learning revolution:</p> <p>CUDA (Compute Unified Device Architecture) is NVIDIA's programming interface that lets general software\u2014not just graphics\u2014run on GPUs. Before CUDA, using GPUs for non-graphics tasks required awkward workarounds. CUDA made GPU computing accessible to researchers, and deep learning took off.</p> <p>The 2012 AlexNet breakthrough wasn't just about a better algorithm\u2014the same architecture trained on CPUs would have taken months. GPU training made rapid experimentation possible, accelerating the entire field.</p> <p>Practical considerations:</p> Task Hardware Recommendation Tabular data (small-medium datasets) CPU sufficient Traditional ML (Random Forest, XGBoost) CPU sufficient Deep learning training GPU recommended Large-scale inference GPU beneficial Image/video processing GPU recommended <p>For the early modules of this course (regression, classification, ensemble methods), you won't need GPU access\u2014these algorithms run well on CPUs. When we reach the neural network modules (6-8), GPU access becomes valuable.</p> <p>Accessing GPUs:</p> <ul> <li>Google Colab: Free tier includes limited GPU access\u2014select \"Runtime &gt; Change runtime type &gt; GPU\"</li> <li>Local GPU: Requires an NVIDIA GPU and CUDA toolkit installation</li> <li>Cloud platforms: AWS, GCP, and Azure offer GPU instances for rent</li> </ul> <p>For this course, Colab's free tier is sufficient for the deep learning exercises. If you have a local NVIDIA GPU, you can configure pixi to use CUDA-enabled packages, but this is optional.</p>"},{"location":"modules/01-foundations/#reflection-questions","title":"Reflection Questions","text":"<ol> <li> <p>A company says they're using \"AI\" for their chatbot. Is this AI? Machine Learning? Both? Neither? How would you find out?</p> </li> <li> <p>You have a dataset where 30% of income values are missing. The missingness correlates with age (older people less likely to report). What imputation strategy would you use?</p> </li> <li> <p>A colleague scales the entire dataset before splitting into train/test. Why is this a problem? How would it affect your model evaluation?</p> </li> <li> <p>Your manager asks: \"Is 85% accuracy good?\" How would you respond?</p> </li> <li> <p>A model has R\u00b2 = 0.95 on training data and R\u00b2 = 0.60 on test data. What's happening? What would you do?</p> </li> <li> <p>Which metric would you optimize for a medical diagnosis model where missing a disease (false negative) is much worse than a false alarm?</p> </li> </ol>"},{"location":"modules/01-foundations/#practice-problems","title":"Practice Problems","text":"<ol> <li> <p>Given a dataset description, identify whether each column needs scaling, encoding, or neither.</p> </li> <li> <p>Calculate precision, recall, and F1 from a confusion matrix.</p> </li> <li> <p>Interpret R\u00b2 = 0.75 in business terms.</p> </li> <li> <p>Identify potential data leakage in a described ML pipeline.</p> </li> <li> <p>For each business problem, recommend an appropriate evaluation metric and justify:</p> </li> <li>Predicting next month's revenue</li> <li>Identifying customers likely to cancel</li> <li>Grouping products that are often bought together</li> </ol>"},{"location":"modules/01-foundations/#chapter-summary","title":"Chapter Summary","text":"<p>Six key takeaways from Module 1:</p> <ol> <li> <p>ML learns patterns from data\u2014it's a subset of AI, and it overlaps with Data Science. Don't conflate these terms.</p> </li> <li> <p>Match problem to task type\u2014regression for continuous values, classification for categories, clustering for grouping.</p> </li> <li> <p>Data prep is critical\u2014handle missing values, scale features (when appropriate), encode categoricals. This is 80% of the work.</p> </li> <li> <p>Prevent data leakage\u2014split first, fit on train, transform test. This is the cardinal rule.</p> </li> <li> <p>Evaluate appropriately\u2014choose metrics that reflect business costs. Accuracy is often the wrong choice.</p> </li> <li> <p>Diagnose model issues\u2014use the bias-variance framework. High test error + low train error = overfitting.</p> </li> </ol>"},{"location":"modules/01-foundations/#whats-next","title":"What's Next","text":"<p>In Module 2, we'll dive into Regression Methods: - Linear regression fundamentals - Regularization (Lasso, Ridge) - Polynomial features - Model interpretation</p> <p>You'll apply everything from Module 1: - Data preparation pipelines - Train-test splits - RMSE, MAE, R\u00b2 evaluation - Cross-validation</p> <p>The concepts we've established in this module are the foundation. Module 2 starts building on that foundation.</p>"},{"location":"modules/02-regression/","title":"Module 2: Classical Machine Learning - Regression","text":""},{"location":"modules/02-regression/#introduction","title":"Introduction","text":"<p>Module 1 established the foundation: what ML is, how to prepare data, and how to evaluate models. Now we put that foundation to work.</p> <p>Regression is the workhorse of predictive analytics. When a business wants to predict sales, estimate prices, or forecast demand, regression is often the first tool they reach for. But we're not just going to use regression as a black box\u2014we're going to understand how it works, including implementing gradient descent from scratch.</p> <p>Why learn gradient descent? Because gradient descent is the foundation for training neural networks. Every deep learning model you've heard of\u2014GPT, image classifiers, everything\u2014learns through gradient descent. Understanding it for linear regression means understanding it for neural networks.</p> <p>The closed-form solution for linear regression requires matrix inversion\u2014O(n\u00b3) complexity that's impossible for neural networks with millions of parameters. Neural networks also have non-convex loss surfaces with many local minima; there's no mathematical formula to jump to the optimal weights. Gradient descent works for any differentiable function and scales to billions of parameters\u2014the linear regression closed-form is a special case where we can skip the search.</p>"},{"location":"modules/02-regression/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, you should be able to:</p> <ol> <li>Explain the mechanics of linear regression including the least squares method</li> <li>Implement gradient descent from scratch and understand its trade-offs</li> <li>Interpret regression coefficients for business insights</li> <li>Diagnose model issues through residual analysis</li> <li>Apply regularization techniques (L1, L2, Elastic Net) to prevent overfitting</li> <li>Communicate regression findings to non-technical stakeholders</li> </ol>"},{"location":"modules/02-regression/#21-simple-linear-regression","title":"2.1 Simple Linear Regression","text":""},{"location":"modules/02-regression/#the-three-components-of-every-ml-model","title":"The Three Components of Every ML Model","text":"<p>Before diving into linear regression, here's a framework that applies to every supervised learning algorithm:</p> Component Question It Answers Linear Regression Decision Model How do we transform inputs into predictions? \\(\\hat{y} = \\beta_0 + \\beta_1 x\\) Quality Measure How do we evaluate prediction quality? Sum of Squared Errors (SSE) Update Method How do we improve the model? Gradient descent (or closed-form) <p>This same pattern applies to every algorithm: logistic regression, decision trees, random forests, neural networks. The decision model changes, the quality measure may change, but the structure is always the same.</p> <p>Different quality measures encode different assumptions about what \"good\" means. MSE assumes symmetric, quadratic costs\u2014but in demand forecasting, under-predicting (stockouts) might cost more than over-predicting. For classification, cross-entropy penalizes confident wrong predictions. For outlier-heavy data, MAE or Huber loss are more robust. Choose a quality measure that aligns with your actual business costs.</p>"},{"location":"modules/02-regression/#the-goal-of-linear-regression","title":"The Goal of Linear Regression","text":"<p>Given input features, we want to predict a continuous output. We assume the relationship can be approximated by a line:</p> \\[\\hat{y} = \\beta_0 + \\beta_1 x\\] <p>Where: - \\(\\hat{y}\\) (y-hat) is the predicted value - \\(\\beta_0\\) (beta-zero) is the intercept\u2014the baseline prediction when x = 0 - \\(\\beta_1\\) (beta-one) is the slope\u2014how much y changes for a one-unit change in x - \\(x\\) is the input feature</p> <p>The key assumption is that a straight line is a reasonable approximation of the true relationship.</p> <p>Check linearity visually: scatter plots should show points around an imaginary straight line. Use <code>sns.pairplot()</code> for multiple regression. Correlation measures only linear association\u2014a perfect U-shaped relationship has r=0. Always check residual plots after fitting; curved patterns reveal non-linearity. If non-linear, try transforms (log, square root), polynomial terms (x\u00b2, x\u00b3), or inherently non-linear models (trees, neural networks).</p>"},{"location":"modules/02-regression/#the-least-squares-method","title":"The Least Squares Method","text":"<p>How do we find the best line? We find the coefficients that minimize squared prediction errors:</p> \\[\\text{minimize } \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2\\] <p>For each data point, calculate the error (actual minus predicted), square it, and add them all up. The best line is the one that makes this sum as small as possible.</p> <p>Why squared errors?</p> <ol> <li> <p>Penalizes large errors more than small ones. An error of 10 contributes 100; an error of 1 contributes only 1. The algorithm cares about avoiding big mistakes.</p> </li> <li> <p>Mathematically tractable. The function is differentiable and convex, so we can find the minimum using calculus.</p> </li> <li> <p>Nice statistical properties. Under certain assumptions, least squares gives the Best Linear Unbiased Estimator (BLUE).</p> </li> </ol> <p>For asymmetric costs, use weighted least squares (assign higher weights to observations where errors are more costly) or quantile regression (systematically over/under-predicts\u2014useful for safety stock). In deep learning, you can define arbitrary custom loss functions. Start simple; only add complexity when you have clear business justification for asymmetric costs.</p> <p>Closed-form solution:</p> \\[\\beta_1 = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum(x_i - \\bar{x})^2}\\] \\[\\beta_0 = \\bar{y} - \\beta_1\\bar{x}\\]"},{"location":"modules/02-regression/#linear-regression-assumptions","title":"Linear Regression Assumptions","text":"<p>For statistical inference to be valid, certain assumptions must hold:</p> <ol> <li>Linearity: The relationship between X and Y is linear</li> <li>Independence: Observations are independent of each other</li> <li>Homoscedasticity: Constant variance of residuals across all levels of X</li> <li>Normality: Residuals are normally distributed</li> <li>No multicollinearity: (for multiple regression) Predictors aren't too highly correlated</li> </ol> <p>When assumptions are violated:</p> Violation Symptom Solution Non-linearity Curved pattern in residuals Transform variables, add polynomial terms Heteroscedasticity Fan-shaped residual plot Transform Y, use robust standard errors Non-normality Q-Q plot deviates from line Transform Y, use bootstrap Autocorrelation Patterns in time-ordered residuals Time series methods <p>Common transformations:</p> Transform Formula Best for Log \\(\\log(x)\\) Right-skewed data, multiplicative relationships Square root \\(\\sqrt{x}\\) Count data, mild right skew Box-Cox \\((x^\\lambda - 1)/\\lambda\\) Automated selection\u2014finds optimal \u03bb <p>The log transform is the workhorse\u2014it handles right-skewed distributions (common in business data like income, prices, counts) and converts multiplicative relationships to additive ones.</p> <p>With a log-transformed target, coefficients represent percentage changes: a one-unit increase in x multiplies y by \\(e^{\\beta_1}\\). For small \u03b2 (roughly |\u03b2| &lt; 0.2), this approximates \u03b2 \u00d7 100% change. If both x and y are logged, coefficients represent elasticities: a 1% change in x \u2192 \u03b2\u2081% change in y. Always back-transform predictions before evaluating metrics, and document which scale coefficients are interpreted on.</p>"},{"location":"modules/02-regression/#gradient-descent","title":"Gradient Descent","text":"<p>We could use the closed-form solution, but gradient descent is worth learning because it's the foundation for all neural network training.</p> <p>The algorithm: 1. Start with random values for \\(\\beta_0\\) and \\(\\beta_1\\) 2. Calculate the gradient\u2014the direction of steepest error increase 3. Update parameters in the opposite direction\u2014downhill toward lower error 4. Repeat until convergence</p> <p>The gradients:</p> \\[\\frac{\\partial MSE}{\\partial \\beta_0} = -\\frac{2}{n}\\sum(y_i - \\hat{y}_i)\\] \\[\\frac{\\partial MSE}{\\partial \\beta_1} = -\\frac{2}{n}\\sum(y_i - \\hat{y}_i) \\cdot x_i\\] <p>Update rules:</p> \\[\\beta_0 \\leftarrow \\beta_0 - \\alpha \\cdot \\frac{\\partial MSE}{\\partial \\beta_0}$$ $$\\beta_1 \\leftarrow \\beta_1 - \\alpha \\cdot \\frac{\\partial MSE}{\\partial \\beta_1}\\] <p>Where \\(\\alpha\\) is the learning rate\u2014how big a step we take each iteration.</p> <p>Convergence means parameters have stabilized\u2014further iterations don't meaningfully improve the solution. Common stopping criteria: loss change below threshold (1e-6), small gradient magnitude, or maximum iterations. Use a combination. For linear regression, the loss surface is convex with one global minimum; for neural networks, you'll find a local minimum (usually good enough). Monitor the loss curve\u2014oscillating or increasing loss suggests the learning rate is too high.</p> <p>Implementation:</p> <pre><code>def gradient_descent_linear_regression(\n    X, y,\n    learning_rate=0.01,\n    n_iterations=1000,\n    tolerance=1e-6\n):\n    n = len(X)\n\n    # Initialize parameters randomly\n    beta_0 = np.random.randn()\n    beta_1 = np.random.randn()\n\n    history = []\n\n    for i in range(n_iterations):\n        # Predictions\n        y_pred = beta_0 + beta_1 * X\n\n        # Compute gradients\n        d_beta_0 = -2/n * np.sum(y - y_pred)\n        d_beta_1 = -2/n * np.sum((y - y_pred) * X)\n\n        # Update parameters\n        beta_0 = beta_0 - learning_rate * d_beta_0\n        beta_1 = beta_1 - learning_rate * d_beta_1\n\n        # Track loss\n        mse = np.mean((y - y_pred)**2)\n        history.append(mse)\n\n        # Check convergence\n        if i &gt; 0 and abs(history[-1] - history[-2]) &lt; tolerance:\n            break\n\n    return beta_0, beta_1, history\n</code></pre>"},{"location":"modules/02-regression/#learning-rate-trade-offs","title":"Learning Rate Trade-offs","text":"<p>The learning rate \\(\\alpha\\) is crucial:</p> Too Small Just Right Too Large Very slow convergence Converges in reasonable time Overshoots the minimum Safe but inefficient Reaches good solution Can diverge (loss increases!) <p>If your loss keeps increasing, the learning rate is too high. Reduce it by a factor of 10.</p> <p>Adaptive learning rate methods automatically adjust during training. Learning rate schedules (step decay, exponential decay, cosine annealing) decrease the rate over time\u2014large steps initially, smaller steps later. Adaptive optimizers (AdaGrad, RMSprop, Adam) adjust per parameter based on gradient history. Adam is the default for deep learning\u2014it works well with default learning rate 0.001. For scikit-learn's linear regression, optimization is handled automatically.</p>"},{"location":"modules/02-regression/#using-statsmodels-for-regression","title":"Using statsmodels for Regression","text":"<p>In practice, we use libraries. For regression with good statistical output, use statsmodels:</p> <pre><code>import statsmodels.formula.api as smf\n\n# R-style formula interface\nmodel = smf.ols(\n    formula='sales ~ advertising + price',\n    data=df\n)\nresults = model.fit()\nprint(results.summary())\n</code></pre> <p>Key statistics in the output:</p> <p>R\u00b2 (Coefficient of Determination): $\\(R^2 = 1 - \\frac{\\sum(y_i - \\hat{y}_i)^2}{\\sum(y_i - \\bar{y})^2}\\)$</p> <p>R\u00b2 tells you what proportion of variance your model explains. R\u00b2 = 0.75 means 75% explained, 25% unexplained.</p> <p>p-values for coefficients: - Tests: \"Is this coefficient different from zero?\" - p &lt; 0.05 is conventional threshold for \"statistically significant\" - Caution: p-values don't tell you effect size</p> <p>The 0.05 threshold is a historical convention from R.A. Fisher, not a magic number. Problems: with enough data, trivial effects become \"significant\"; with little data, real effects may not be. Modern practice: report exact p-values, consider effect sizes and confidence intervals, and remember that practical significance matters more than statistical significance in business\u2014a statistically significant 0.1% improvement might not be worth implementing.</p> <p>Confidence intervals: - 95% CI of [1.8, 3.2] means: \"We're 95% confident the true effect is between $1.80 and $3.20\" - If the CI includes zero, the effect is not statistically significant</p> <p>F-statistic: - Tests whether the model as a whole is useful - \"Is this model better than just predicting the mean?\"</p> <p>When to use scikit-learn instead: When building ML pipelines, when prediction is the main goal, when you need cross-validation and hyperparameter tuning.</p>"},{"location":"modules/02-regression/#interpreting-coefficients","title":"Interpreting Coefficients","text":"<p>The standard interpretation: \"A one-unit increase in X is associated with a \\(\\beta_1\\) change in Y, holding all else constant.\"</p> <p>Example: <pre><code>sales = 50,000 + 2.5 \u00d7 advertising + 1,200 \u00d7 sales_staff\n</code></pre></p> <p>Translation: - Baseline sales: $50,000 (when advertising = 0 and sales_staff = 0) - Each $1 in advertising \u2192 $2.50 more in sales (250% ROI) - Each additional sales staff \u2192 $1,200 more in sales</p> <p>Caution: Correlation \u2260 Causation</p> <p>Regression shows association, not causation. When we say \"Each $1 in advertising \u2192 $2.50 more in sales,\" we mean they're associated. We haven't proven advertising causes sales.</p> <p>Classic example: Ice cream sales and drowning deaths are positively correlated. Both are caused by summer heat\u2014a confounding variable.</p> <p>Establishing causation requires experimental design or careful causal inference methods. Randomized experiments (A/B tests) are the gold standard. Natural experiments (policy changes affecting some regions) create quasi-random groups. Instrumental variables find factors that affect treatment but not outcome directly. Causal inference frameworks (propensity score matching, difference-in-differences) try to estimate effects from observational data. With observational regression alone, you have association\u2014to claim causation, you need a convincing argument for why confounders are controlled.</p>"},{"location":"modules/02-regression/#residual-analysis","title":"Residual Analysis","text":"<p>Residuals reveal problems:</p> \\[e_i = y_i - \\hat{y}_i\\] <p>If the model is good and assumptions hold, residuals should look like random noise.</p> <p>Diagnostic plots:</p> <ol> <li>Residuals vs. Fitted Values: Should show random scatter around zero</li> <li>Curve = non-linearity</li> <li> <p>Funnel = heteroscedasticity</p> </li> <li> <p>Q-Q Plot: Residuals vs. theoretical normal quantiles</p> </li> <li>Straight line = normality satisfied</li> <li>Curves at ends = heavy/light tails</li> </ol> <pre><code>import matplotlib.pyplot as plt\nimport scipy.stats as stats\n\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 3, 1)\nplt.scatter(y_pred, residuals)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('Fitted Values')\nplt.ylabel('Residuals')\nplt.title('Residuals vs Fitted')\n\nplt.subplot(1, 3, 2)\nstats.probplot(residuals, dist=\"norm\", plot=plt)\nplt.title('Q-Q Plot')\n\nplt.subplot(1, 3, 3)\nplt.hist(residuals, bins=30, edgecolor='black')\nplt.xlabel('Residuals')\nplt.title('Residual Distribution')\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Key insight: High R\u00b2 with a patterned residual plot is still a bad model. The pattern means you're missing structure in the data.</p> <p>To fix a curved residual pattern: (1) Identify which feature causes it by plotting residuals against each predictor. (2) Try transforms\u2014log for diminishing returns, square root for counts, polynomial terms (x\u00b2, x\u00b3). (3) Use <code>PolynomialFeatures(degree=2)</code> with regularization. (4) If transforms don't help, consider non-linear models (trees, GAMs, neural networks). (5) Verify the fix\u2014residuals should show random scatter.</p>"},{"location":"modules/02-regression/#common-misconceptions","title":"Common Misconceptions","text":"Misconception Reality \"Correlation implies causation\" Regression shows association only. Causation requires experimental design or causal inference methods. \"High R\u00b2 means the model is good\" R\u00b2 can be high due to overfitting. Must check test set performance and residual plots. \"The intercept is always meaningful\" Often it's not (e.g., salary when experience = 0 years). Focus on slopes for interpretation. \"Larger coefficients mean more important features\" Only true if features are on the same scale. Use standardized coefficients to compare."},{"location":"modules/02-regression/#22-multiple-linear-regression","title":"2.2 Multiple Linear Regression","text":""},{"location":"modules/02-regression/#multiple-predictors","title":"Multiple Predictors","text":"<p>In the real world, we rarely have just one predictor:</p> \\[\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_d x_d\\] <p>Why multiple predictors? 1. Single predictor rarely captures the full story 2. Control for confounding variables 3. Improve prediction accuracy</p> <p>Key insight: Each coefficient represents the effect of that variable while controlling for the others. This is different from running separate simple regressions!</p> <p>Multiple regression coefficients are partial effects\u2014the effect of one variable holding others constant. Simple regression captures both direct and indirect effects through correlated variables. If experience and education are correlated, simple regression conflates their effects; multiple regression \"controls for\" education when estimating experience's effect. Sometimes adding variables can even flip coefficient signs (Simpson's paradox)\u2014the Berkeley admissions example showed apparent disadvantage for women overall that reversed within each department.</p>"},{"location":"modules/02-regression/#confounding-variables","title":"Confounding Variables","text":"<p>Example: - Simple regression: Salary ~ Experience \u2192 \\(\\beta = \\$5,000\\) per year - Multiple regression: Salary ~ Experience + Education \u2192 \\(\\beta_{exp} = \\$3,500\\) per year</p> <p>The coefficient for experience dropped because education was confounded with experience. People with more experience often have more education. The simple regression was attributing some of education's effect to experience.</p> <p>To identify confounders, ask: \"What could affect both X and Y?\" Draw causal diagrams\u2014confounders have arrows TO both predictor and outcome. Check correlations, but correlation alone isn't sufficient; you need domain reasoning. Don't throw every variable in\u2014mediators (on the causal path) or colliders (affected by both) can introduce bias. Include variables that theory suggests are confounders and were measured before the treatment.</p>"},{"location":"modules/02-regression/#multicollinearity","title":"Multicollinearity","text":"<p>What is multicollinearity? High correlation between predictors.</p> <p>Symptoms: - Coefficients change dramatically when you add/remove features - High R\u00b2 but few significant individual predictors - Signs of coefficients seem wrong</p>"},{"location":"modules/02-regression/#detecting-multicollinearity-vif","title":"Detecting Multicollinearity: VIF","text":"<p>Variance Inflation Factor (VIF):</p> \\[VIF_j = \\frac{1}{1 - R_j^2}\\] <p>Where \\(R_j^2\\) is R\u00b2 from regressing feature j on all other features.</p> VIF Value Interpretation VIF = 1 No correlation with other features VIF &gt; 5 Moderate multicollinearity\u2014investigate VIF &gt; 10 Serious multicollinearity\u2014must address <pre><code>from statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Calculate VIF for each feature\nfor i in range(X.shape[1]):\n    vif = variance_inflation_factor(X.values, i)\n    print(f\"{feature_names[i]}: VIF = {vif:.2f}\")\n</code></pre> <p>Solutions for multicollinearity: 1. Remove one of the correlated features 2. Combine features (average or PCA) 3. Use regularization (Ridge handles this well) 4. Accept it if prediction is your only goal</p> <p>Multicollinearity doesn't affect prediction accuracy\u2014but it creates problems for interpretation. Coefficients become unstable (jumping around with small data changes), standard errors inflate (true effects appear \"not significant\"), and signs may reverse. If your goal is purely prediction, ignore it. If you need interpretation, address it.</p>"},{"location":"modules/02-regression/#why-regularize","title":"Why Regularize?","text":"<p>Regularization prevents overfitting by penalizing large coefficients:</p> \\[\\text{minimize } \\sum(y_i - \\hat{y}_i)^2 + \\lambda \\cdot \\text{penalty}(\\beta)\\] <p>The parameter \\(\\lambda\\) controls how strong the penalty is.</p> <p>Large coefficients can indicate overfitting\u2014the model making sharp adjustments for individual data points (memorizing). Signs of overfitting: large coefficients only with small samples, huge standard errors, or poor test performance. Regularization forces the model to justify large coefficients\u2014if fitting noise, the penalty isn't worth it; if fitting real patterns, the benefit outweighs the penalty. Note: \"large\" depends on feature scale\u2014always standardize before judging.</p>"},{"location":"modules/02-regression/#l1-regularization-lasso","title":"L1 Regularization (Lasso)","text":"<p>Penalty: \\(\\lambda \\sum|\\beta_j|\\)</p> <p>Effect: Can shrink coefficients exactly to zero \u2192 automatic feature selection!</p> <p>The L1 constraint region is a diamond with corners on the axes; L2 is a smooth circle. Loss function contours typically hit the L1 diamond at corners (coefficients exactly zero), but touch the L2 circle tangentially (rarely on an axis). Additionally, the L1 gradient is constant (\u00b11) regardless of how small \u03b2 gets\u2014always pulling toward zero\u2014while the L2 gradient (2\u03b2) weakens as \u03b2 approaches zero. L1 performs automatic feature selection; L2 keeps all features with small coefficients.</p> <p>When to use: - You suspect many features are irrelevant - You want an interpretable, sparse model - Feature selection is an explicit goal</p> <pre><code>from sklearn.linear_model import Lasso\n\nlasso = Lasso(alpha=0.1, random_state=42)\nlasso.fit(X_train_scaled, y_train)\n\n# See which features were selected (non-zero coefficients)\nselected_features = [f for f, c in zip(feature_names, lasso.coef_) if c != 0]\nprint(f\"Selected {len(selected_features)} features\")\n</code></pre>"},{"location":"modules/02-regression/#l2-regularization-ridge","title":"L2 Regularization (Ridge)","text":"<p>Penalty: \\(\\lambda \\sum\\beta_j^2\\)</p> <p>Effect: Shrinks all coefficients toward zero, but never exactly zero</p> <p>When to use: - Multicollinearity is present - All features are potentially relevant - Prediction accuracy is the main goal</p> <pre><code>from sklearn.linear_model import Ridge\n\nridge = Ridge(alpha=1.0, random_state=42)\nridge.fit(X_train_scaled, y_train)\n</code></pre>"},{"location":"modules/02-regression/#elastic-net-combining-l1-and-l2","title":"Elastic Net: Combining L1 and L2","text":"<p>Penalty: \\(\\lambda_1 \\sum|\\beta_j| + \\lambda_2 \\sum\\beta_j^2\\)</p> <p>Benefits: - Feature selection from L1 (can zero out coefficients) - Stability from L2 (handles correlated features better) - More flexible than either alone</p> <pre><code>from sklearn.linear_model import ElasticNet\n\nelastic = ElasticNet(\n    alpha=0.1,       # Overall regularization strength\n    l1_ratio=0.5,    # Balance between L1 and L2\n    random_state=42\n)\nelastic.fit(X_train_scaled, y_train)\n</code></pre>"},{"location":"modules/02-regression/#choosing-regularization-strength","title":"Choosing Regularization Strength","text":"<p>Use cross-validation:</p> <pre><code>from sklearn.linear_model import LassoCV, RidgeCV\n\n# Automatic alpha selection via CV\nlasso_cv = LassoCV(\n    alphas=np.logspace(-4, 1, 50),\n    cv=5\n)\nlasso_cv.fit(X_train_scaled, y_train)\nprint(f\"Best alpha: {lasso_cv.alpha_}\")\n</code></pre> <p>Important: Always scale your features before regularization! Regularization penalizes large coefficients, and features on different scales are penalized unfairly.</p> <p>To convert scaled coefficients back to original units: \\(\\beta_{original} = \\frac{\\beta_{scaled}}{\\sigma}\\). In code: <code>original_coefs = model.coef_ / scaler.scale_</code>. Back-transform for business communication (\"each $1000 in spending \u2192 X more sales\"); keep scaled for comparing feature importance. Save your scaler object so you have access to <code>mean_</code> and <code>scale_</code> attributes.</p>"},{"location":"modules/02-regression/#linear-regression-as-a-neural-network","title":"Linear Regression as a Neural Network","text":"<p>Here's something that will pay off in Module 6:</p> <pre><code>Input Layer          Output Layer\n\n   x\u2081 \u2500\u2500\u2500\u2500 w\u2081 \u2500\u2500\u2500\u2500\u2510\n                   \u251c\u2500\u2500\u2192 \u03a3 + b \u2500\u2500\u2192 \u0177\n   x\u2082 \u2500\u2500\u2500\u2500 w\u2082 \u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Linear regression is just a neural network with no hidden layers! - The weights (w) are our coefficients (\\(\\beta\\)) - The bias (b) is our intercept (\\(\\beta_0\\)) - We sum the weighted inputs and add the bias</p> <p>When we add hidden layers and non-linear activations, we get deep learning. But the foundation\u2014weighted sums optimized by gradient descent\u2014is exactly what we learned here.</p>"},{"location":"modules/02-regression/#common-misconceptions_1","title":"Common Misconceptions","text":"Misconception Reality \"Regularization always hurts training performance\" True, but that's the point! We sacrifice training fit for better generalization. \"Lasso always performs feature selection\" Only with sufficient regularization. Very small alpha may keep all features. \"More features always improve the model\" Only if they're informative. Irrelevant features add noise and overfitting risk. \"Ridge is inferior because it doesn't zero out coefficients\" Ridge is often better when all features matter. Lasso is for sparse solutions."},{"location":"modules/02-regression/#23-business-application","title":"2.3 Business Application","text":""},{"location":"modules/02-regression/#end-to-end-regression-workflow","title":"End-to-End Regression Workflow","text":"<ol> <li>Business problem definition - What are we predicting? Why does it matter?</li> <li>Data collection and exploration - EDA, quality assessment</li> <li>Feature engineering - Create informative variables from raw data</li> <li>Model building and evaluation - Compare approaches, cross-validate</li> <li>Interpretation and communication - Translate for stakeholders</li> </ol> <p>Most of your time should go into steps 1-3. The modeling itself is almost mechanical once you have good data and features.</p> <p>When is \"enough\" feature engineering? Track validation performance as you add features\u2014stop when new features don't improve it. The 80/20 rule applies: basic features (raw data, simple transforms) usually dominate; exotic interactions rarely add much. Don't exceed n/10 to n/20 features for n samples without regularization. Start simple, add complexity where residual analysis suggests it, and stop when validation performance plateaus.</p>"},{"location":"modules/02-regression/#feature-engineering-patterns","title":"Feature Engineering Patterns","text":"<p>Time-based features: <pre><code>import polars as pl\n\ndf = df.with_columns([\n    pl.col('date').dt.weekday().alias('day_of_week'),\n    pl.col('date').dt.month().alias('month'),\n    pl.col('date').dt.weekday().is_in([5, 6]).cast(pl.Int32).alias('is_weekend'),\n])\n</code></pre></p> <p>Aggregations: <pre><code>customer_features = df.group_by('customer_id').agg([\n    pl.col('amount').sum().alias('total_spend'),\n    pl.col('amount').mean().alias('avg_order'),\n    pl.col('amount').count().alias('order_count'),\n])\n</code></pre></p> <p>Interactions: <pre><code>df = df.with_columns([\n    (pl.col('price') / pl.col('sqft')).alias('price_per_sqft'),\n])\n</code></pre></p> <p>Polynomial features: <pre><code>from sklearn.preprocessing import PolynomialFeatures\n\npoly = PolynomialFeatures(degree=2, include_bias=False)\nX_poly = poly.fit_transform(X)\n</code></pre></p>"},{"location":"modules/02-regression/#translating-statistics-to-business-language","title":"Translating Statistics to Business Language","text":"Statistical Term Business Translation Coefficient = 2.5 \"Every $1 more in advertising is associated with $2.50 more in sales\" R\u00b2 = 0.75 \"Our model explains about 75% of the variation in sales\" p-value &lt; 0.05 \"We're confident this factor genuinely affects sales, not just by chance\" 95% CI: [1.8, 3.2] \"We estimate the effect is between $1.80 and $3.20 per dollar spent\" <p>Standardized vs. unstandardized coefficients: - Unstandardized (original units): For business interpretation\u2014\"Every $1 in advertising \u2192 $2.50 in sales\" - Standardized (z-scores): For comparing feature importance\u2014\"Which variable has the biggest effect?\"</p>"},{"location":"modules/02-regression/#sensitivity-analysis","title":"Sensitivity Analysis","text":"<p>Stakeholders love \"what-if\" scenarios:</p> <pre><code># Base prediction\nbase = model.predict(X_current)\n\n# Modified scenario\nX_modified = X_current.copy()\nX_modified['advertising'] *= 1.10\nnew = model.predict(X_modified)\n\nprint(f\"10% more advertising \u2192 ${new - base:,.0f} more sales\")\n</code></pre>"},{"location":"modules/02-regression/#the-business-memo","title":"The Business Memo","text":"<p>Structure:</p> <ol> <li>Executive Summary - 2-3 sentences. Main finding plus recommendation.</li> <li>Key Findings - Bullet points with business impact.</li> <li>Recommendations - Specific, actionable steps.</li> <li>Limitations - What the analysis cannot tell us.</li> </ol> <p>Rules: - No code - No jargon without explanation - No p-values without context - Always include limitations</p> <p>The limitations section matters. Every analysis has limitations. If you don't acknowledge them, you're either unaware (bad) or hiding them (worse).</p> <p>Frame limitations as scope definition, not weakness. Instead of \"The model doesn't account for competitor pricing,\" say \"The model predicts based on our historical data. For decisions involving major competitor moves, supplement with competitive intelligence.\" Quantify uncertainty (\"accurate to within \u00b115% 80% of the time\") rather than saying \"predictions might be wrong.\" Lead with capabilities, then contextualize what's left. Provide recommendations within limitations (\"Given \u00b115% uncertainty, keep 20% buffer inventory\"). Models presented as perfect lose credibility when they fail; honest limitations build trust.</p>"},{"location":"modules/02-regression/#reflection-questions","title":"Reflection Questions","text":"<ol> <li> <p>You implement gradient descent and the loss keeps increasing. What's likely wrong? How would you fix it?</p> </li> <li> <p>A regression coefficient for 'ice cream sales' on 'drowning deaths' is positive and statistically significant. Should ice cream vendors be concerned about causing drownings?</p> </li> <li> <p>Your model has R\u00b2 = 0.95 but the residual plot shows a clear curved pattern. Is this a good model?</p> </li> <li> <p>When would you prefer Lasso over Ridge regression? Give a business scenario.</p> </li> <li> <p>You add more features to your model and R\u00b2 on training data increases, but test set performance decreases. What's happening?</p> </li> <li> <p>How would you explain regularization to a business stakeholder without using math?</p> </li> </ol>"},{"location":"modules/02-regression/#practice-problems","title":"Practice Problems","text":"<ol> <li> <p>Given a fitted model equation, interpret each coefficient in business terms.</p> </li> <li> <p>Diagnose issues from residual plots (identify non-linearity, heteroscedasticity).</p> </li> <li> <p>Calculate VIF and decide which features to remove.</p> </li> <li> <p>Choose between Lasso, Ridge, and ElasticNet for different scenarios.</p> </li> <li> <p>Write a business memo interpreting regression results for a non-technical audience.</p> </li> </ol>"},{"location":"modules/02-regression/#chapter-summary","title":"Chapter Summary","text":"<p>Six key takeaways from Module 2:</p> <ol> <li> <p>Linear regression minimizes squared errors to find the best-fit line. The math is elegant, but the intuition is simple: find the line that makes predictions as close as possible to reality.</p> </li> <li> <p>Gradient descent iteratively optimizes parameters. This is the foundation for all neural network training. Learning rate matters: too small is slow, too large is unstable.</p> </li> <li> <p>Coefficients show association, not causation. Be careful how you communicate this. \"Associated with\" is not \"causes.\"</p> </li> <li> <p>Residual analysis reveals assumption violations. Always check your residual plots. High R\u00b2 with a patterned residual plot is a bad model.</p> </li> <li> <p>Regularization prevents overfitting. Lasso selects features, Ridge handles multicollinearity, Elastic Net does both.</p> </li> <li> <p>Communication matters. Translate statistics to business impact. Include limitations. Make it actionable.</p> </li> </ol>"},{"location":"modules/02-regression/#whats-next","title":"What's Next","text":"<p>In Module 3, we tackle Classification Methods: - Logistic regression (extends linear regression to classification) - Decision boundaries and probability estimation - Classification metrics in depth - Handling imbalanced classes</p> <p>You'll apply everything from Module 2: - Same data preparation workflow - Gradient descent concepts - Regularization techniques</p> <p>The difference: we're predicting categories instead of numbers.</p>"},{"location":"modules/03-classification/","title":"Module 3: Classification Methods","text":""},{"location":"modules/03-classification/#introduction","title":"Introduction","text":"<p>We've covered a lot of ground\u2014foundations in Module 1, regression in Module 2. Now we move to classification, which is arguably even more prevalent in business applications.</p> <p>Think about the decisions businesses make every day: Should we approve this loan? Is this transaction fraudulent? Will this customer cancel their subscription? Is this email spam? These are all classification problems\u2014predicting a category, not a number.</p> <p>The concepts extend to multiclass classification: logistic regression uses softmax instead of sigmoid; decision trees handle it naturally; evaluation uses per-class precision/recall and NxN confusion matrices. The fundamentals transfer directly\u2014the mechanics get more complex but the reasoning stays the same.</p> <p>This module covers four major topics: logistic regression (extending regression concepts to classification), decision trees (intuitive classifiers that set us up for ensemble methods), handling imbalanced data (because when 99% of transactions are legitimate, accuracy is meaningless), and hyperparameter optimization.</p>"},{"location":"modules/03-classification/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, you should be able to:</p> <ol> <li>Explain the mechanics and interpretation of logistic regression, including log odds and probability</li> <li>Build and interpret decision tree classifiers, understanding their tendency to overfit</li> <li>Apply appropriate techniques for handling imbalanced classification problems</li> <li>Use hyperparameter optimization techniques to improve model performance</li> <li>Select appropriate evaluation metrics based on business context</li> </ol>"},{"location":"modules/03-classification/#31-logistic-regression","title":"3.1 Logistic Regression","text":""},{"location":"modules/03-classification/#three-components-logistic-regression","title":"Three Components: Logistic Regression","text":"<p>Connecting to the framework from Module 2:</p> Component Logistic Regression Decision Model \\(P(Y=1) = \\sigma(\\beta_0 + \\beta_1 x_1 + ...)\\) \u2014 sigmoid of linear combination Quality Measure Cross-entropy (log loss) \u2014 penalizes confident wrong predictions Update Method Gradient descent on log-likelihood <p>The decision model changes from a line to a sigmoid curve, and the quality measure changes from SSE to cross-entropy\u2014but the overall structure is identical to linear regression.</p>"},{"location":"modules/03-classification/#why-linear-regression-fails-for-classification","title":"Why Linear Regression Fails for Classification","text":"<p>Binary outcomes are coded as 0 or 1. If we fit a line, predictions can be less than 0 or greater than 1. \"There's a -15% chance of churn\" is meaningless.</p> <p>The solution: Transform the output so it's always between 0 and 1.</p> <p>Other functions map to (0,1)\u2014probit, scaled tanh\u2014but sigmoid has unique advantages: its derivative is expressible in terms of the output (efficient gradients), its inverse is the logit (clean coefficient interpretation as log-odds), and it arises from maximum entropy principles. Tools and practices are standardized around it.</p>"},{"location":"modules/03-classification/#the-sigmoid-function","title":"The Sigmoid Function","text":"\\[\\sigma(z) = \\frac{1}{1 + e^{-z}}\\] <p>Where \\(z = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ...\\)</p> <p>Key properties: - Output is always between 0 and 1\u2014valid probability - S-shaped curve\u2014small changes in x have the biggest effect near 0.5 - At z=0, output is exactly 0.5</p> <p>The math: - When z is large and positive: \\(e^{-z} \\to 0\\), so \\(\\sigma(z) \\to 1\\) - When z is large and negative: \\(e^{-z} \\to \\infty\\), so \\(\\sigma(z) \\to 0\\) - When z = 0: \\(e^{0} = 1\\), so \\(\\sigma(0) = 0.5\\)</p>"},{"location":"modules/03-classification/#understanding-odds-and-log-odds","title":"Understanding Odds and Log Odds","text":"<p>Step 1: Odds</p> \\[Odds = \\frac{P(Y=1)}{P(Y=0)} = \\frac{p}{1-p}\\] <p>If P(churn) = 0.75, odds = 0.75/0.25 = 3. \"3 to 1 odds of churning.\"</p> <p>Step 2: Log Odds (Logit)</p> \\[\\log\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 x_1 + ...\\] <p>Key insight: The log odds ARE linear in the predictors. This is where the \"linear\" in logistic regression comes from.</p> <p>Example: Model: \\(\\log(odds) = -2 + 0.5 \\times age\\)</p> Age Log Odds Odds Probability 0 -2 \\(e^{-2}\\) \u2248 0.14 12% 20 8 \\(e^8\\) \u2248 2981 99.97% 30 13 \\(e^{13}\\) \u2248 442,413 \u2248100% <p>Log odds change linearly, but probabilities don't. That's the magic of the logit transform.</p> <p>Why log odds? They provide interpretable coefficients (each \u03b2 is \"change in log-odds per unit\"), unbounded range (the linear predictor can take any value while output stays bounded 0-1), and additive effects (effects of multiple variables sum in log-odds space, unlike in probability space). The transformation connects linear models to probability naturally.</p>"},{"location":"modules/03-classification/#coefficient-interpretation","title":"Coefficient Interpretation","text":"<p>The coefficient \\(\\beta_1\\): Change in log odds for a one-unit increase in \\(x_1\\).</p> <p>The odds ratio \\(e^{\\beta_1}\\): Multiplicative change in odds.</p> <p>Example: - If \\(\\beta_1 = 0.5\\), then \\(e^{0.5} \\approx 1.65\\) - \"Each unit increase in X increases the odds by 65%\"</p> <p>Converting to probability: 1. Calculate log-odds: \\(z = \\beta_0 + \\beta_1 x_1 + ...\\) 2. Apply sigmoid: \\(P(Y=1) = \\frac{1}{1 + e^{-z}}\\)</p>"},{"location":"modules/03-classification/#is-it-regression-or-classification","title":"Is It Regression or Classification?","text":"Aspect Answer Name \"Regression\" (historical reasons) What it models Probability (continuous 0-1) What we use it for Classification (discrete classes) How Apply a threshold to the probability <p>Key insight: Logistic regression IS a regression model (predicts continuous probability), but we USE it for classification by thresholding.</p> <p>Probabilities give crucial flexibility over hard class predictions: threshold flexibility (adjust without retraining when costs change), ranking and prioritization (\"which 100 customers are most likely to churn?\"), confidence communication (P=0.95 vs P=0.55 both classify as positive but represent different confidence), and risk quantification (expected value calculations require probabilities). In business, you almost always benefit from probabilities.</p>"},{"location":"modules/03-classification/#decision-thresholds","title":"Decision Thresholds","text":"<p>The default threshold of 0.5 is often NOT optimal!</p> <p>Example - Fraud Detection: - Cost of missing fraud (false negative): $10,000 - Cost of investigating non-fraud (false positive): $100</p> <p>With asymmetric costs, lower the threshold\u2014catch more fraud, accept more false alarms.</p> <p>Cost-based threshold formula: \\(t^* = \\frac{C_{FP}}{C_{FP} + C_{FN}}\\). For fraud costing $10,000 (FN) and investigation costing $100 (FP): threshold \u2248 100/(100+10000) \u2248 0.01\u2014predict fraud for anyone above 1% probability! This assumes well-calibrated probabilities; verify calibration first. Alternative: Youden's J statistic (maximize TPR-FPR) when costs are unknown.</p> <p>Threshold effects:</p> Lower Threshold Higher Threshold More positive predictions Fewer positive predictions Higher recall Higher precision Lower precision Lower recall Fewer false negatives Fewer false positives"},{"location":"modules/03-classification/#roc-curves-and-auc","title":"ROC Curves and AUC","text":"<p>For each possible threshold: 1. Calculate True Positive Rate: \\(TPR = \\frac{TP}{TP + FN}\\) 2. Calculate False Positive Rate: \\(FPR = \\frac{FP}{FP + TN}\\) 3. Plot the point</p> <p>AUC interpretation: - 0.5 = Random guessing - 1.0 = Perfect separation - 0.8 = \"80% chance that a randomly chosen positive ranks higher than a randomly chosen negative\"</p> <p>Note: AUC \u2260 accuracy. AUC measures ranking ability across all thresholds.</p> <p>Ranking ability means correctly ordering examples by likelihood\u2014higher-risk items get higher scores\u2014even if actual probability values are wrong. This matters for resource allocation (\"call top 100 highest-risk customers\"), campaign targeting (top decile by response rate), and prioritization (fraud investigators review by score). A model with AUC=0.9 and poor calibration is often more useful than AUC=0.6 with perfect calibration\u2014you can recalibrate using Platt scaling or isotonic regression; you can't easily fix ranking ability.</p> <p>Choosing optimal threshold: - Youden's J statistic: Maximize (TPR - FPR) - Cost-based: Minimize expected cost given FP/FN costs - Precision-Recall trade-off: Use PR curve for imbalanced data</p> <pre><code>from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, auc\n\nlog_reg = LogisticRegression(penalty='l2', C=1.0, random_state=42)\nlog_reg.fit(X_train, y_train)\n\n# Get probabilities\ny_proba = log_reg.predict_proba(X_test)[:, 1]\n\n# ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_proba)\nroc_auc = auc(fpr, tpr)\n\n# Optimal threshold (Youden's J)\noptimal_idx = (tpr - fpr).argmax()\noptimal_threshold = thresholds[optimal_idx]\n\n# Interpret coefficients as odds ratios\nfor feature, coef in zip(feature_names, log_reg.coef_[0]):\n    odds_ratio = np.exp(coef)\n    print(f\"{feature}: odds ratio = {odds_ratio:.3f}\")\n</code></pre>"},{"location":"modules/03-classification/#common-misconceptions","title":"Common Misconceptions","text":"Misconception Reality \"Logistic regression outputs are well-calibrated probabilities\" Outputs may need calibration (Platt scaling, isotonic regression) for reliable probability estimates. \"Higher AUC always means better model\" AUC ignores calibration and threshold choice. A model with lower AUC but better calibration might be preferable. \"Logistic regression requires linear relationships\" It requires linearity in LOG ODDS, not probability. Add polynomial terms for non-linear relationships. \"Logistic regression can't handle multiple classes\" Multinomial logistic regression extends to multiple classes (one-vs-rest or softmax)."},{"location":"modules/03-classification/#32-decision-trees-cart","title":"3.2 Decision Trees (CART)","text":""},{"location":"modules/03-classification/#three-components-decision-trees","title":"Three Components: Decision Trees","text":"Component Decision Trees Decision Model Tree of if-then rules \u2014 follow branches based on feature thresholds Quality Measure Gini impurity or entropy \u2014 measures class mixture in nodes Update Method Greedy recursive splitting \u2014 find best split at each node <p>Key difference: We're not doing gradient descent. Trees use a greedy algorithm that builds one split at a time.</p>"},{"location":"modules/03-classification/#decision-tree-intuition","title":"Decision Tree Intuition","text":"<p>Imagine you're a loan officer: - First: Is income &gt; $50,000?   - Yes \u2192 Check debt-to-income ratio   - No \u2192 Check employment history...</p> <p>Decision trees formalize this intuitive process. They automatically learn which questions to ask, in what order, and what thresholds to use.</p> <p>The tree picks the feature and threshold that best separates classes (maximally reduces impurity). This is a greedy algorithm\u2014locally best splits without looking ahead. The first feature is often important but not always \"most important\": a feature might matter most after controlling for another, or correlated features might be interchanged. Feature importance scores (aggregating across all nodes) are more reliable than just the root split.</p>"},{"location":"modules/03-classification/#splitting-criteria","title":"Splitting Criteria","text":"<p>Gini Impurity (scikit-learn default): $\\(Gini = 1 - \\sum_{i=1}^{C} p_i^2\\)$</p> <p>Where \\(p_i\\) is the proportion of class \\(i\\) in the node.</p> <ul> <li>Gini = 0: Pure node (all same class)</li> <li>Gini = 0.5: Maximum impurity for binary (50-50)</li> </ul> <p>Entropy: $\\(Entropy = -\\sum_{i=1}^{C} p_i \\log_2(p_i)\\)$</p> <p>Usually produces similar results. Gini is slightly faster (no logarithms).</p> <p>In practice, Gini vs entropy rarely matters. Entropy penalizes near-equal splits slightly more; with many classes, Gini can favor isolating one class while entropy prefers balanced information gain. Default to Gini (slightly faster); if hyperparameter tuning, include criterion and let cross-validation decide.</p>"},{"location":"modules/03-classification/#the-scikit-learn-api-pattern","title":"The scikit-learn API Pattern","text":"<p>This pattern is consistent across almost ALL scikit-learn models:</p> <pre><code># 1. Instantiate\nmodel = DecisionTreeClassifier(max_depth=5)\n\n# 2. Fit\nmodel.fit(X_train, y_train)\n\n# 3. Predict\npredictions = model.predict(X_test)\nprobabilities = model.predict_proba(X_test)\n</code></pre>"},{"location":"modules/03-classification/#decision-boundaries","title":"Decision Boundaries","text":"<p>Trees create rectangular decision regions: - Each split creates a horizontal or vertical line - Deep trees create many small rectangles - Different from logistic regression's smooth boundary</p>"},{"location":"modules/03-classification/#demonstrating-overfitting","title":"Demonstrating Overfitting","text":"<p>Deep Tree (no limit): - Train accuracy: 100% - Test accuracy: 75% - Hundreds of nodes</p> <p>Shallow Tree (depth=3): - Train accuracy: 85% - Test accuracy: 82% - ~15 nodes</p> <p>Key insight: Deep trees memorize training data including noise. 100% training accuracy almost certainly means overfitting.</p> <p>100% training accuracy is occasionally okay: perfectly separable data (predicting even/odd from last digit), very small clean datasets, or memorization tasks. Verify by checking test accuracy (also very high?), the train-test gap (small vs large?), complexity (10 leaves for 10,000 samples = simple rules; 5,000 leaves = memorized), and cross-validation consistency. The heuristic remains useful: 100% training accuracy should trigger suspicion.</p>"},{"location":"modules/03-classification/#pruning-strategies","title":"Pruning Strategies","text":"<p>Pre-pruning (early stopping): - <code>max_depth</code>: Maximum tree depth - <code>min_samples_split</code>: Minimum samples to split a node - <code>min_samples_leaf</code>: Minimum samples in a leaf</p> <p>Post-pruning: - Grow full tree, then prune back - Use <code>ccp_alpha</code> parameter - Higher alpha = more pruning</p> <p>Recommendation: Start with pre-pruning. Set <code>max_depth=5</code> as starting point, use cross-validation to optimize.</p> <pre><code>from sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.model_selection import cross_val_score\n\ntree = DecisionTreeClassifier(\n    criterion='gini',\n    max_depth=5,\n    min_samples_split=10,\n    min_samples_leaf=5,\n    random_state=42\n)\ntree.fit(X_train, y_train)\n\n# Check for overfitting\ntrain_acc = tree.score(X_train, y_train)\ntest_acc = tree.score(X_test, y_test)\nprint(f\"Train: {train_acc:.3f}, Test: {test_acc:.3f}\")\n\n# Cross-validation for depth selection\nfor depth in range(1, 15):\n    tree_cv = DecisionTreeClassifier(max_depth=depth, random_state=42)\n    scores = cross_val_score(tree_cv, X_train, y_train, cv=5)\n    print(f\"Depth {depth}: {scores.mean():.3f} (+/- {scores.std():.3f})\")\n</code></pre>"},{"location":"modules/03-classification/#feature-importance","title":"Feature Importance","text":"\\[Importance = \\sum_{nodes} (impurity\\ reduction \\times samples)\\] <p>Caveats: - Importance is relative (sums to 1) - Correlated features split importance between them - Doesn't indicate direction of effect or causation</p> <p>To understand importance with correlated features: use domain knowledge (which is more causal?), remove one and retrain (does importance transfer?), or use permutation importance (shuffles independently). For prediction, keeping both adds complexity without benefit. For interpretation, report both but note correlation. Consider reporting \"this cluster of correlated features is important\" rather than attributing to one.</p>"},{"location":"modules/03-classification/#why-decision-trees-are-popular","title":"Why Decision Trees Are Popular","text":"<ol> <li>Explainable: Show decision rules to stakeholders</li> <li>No preprocessing: Handle different scales, categorical variables, missing values</li> <li>Non-linear: Capture complex relationships automatically</li> <li>Visual: Tree diagrams are intuitive for non-technical audiences</li> </ol>"},{"location":"modules/03-classification/#common-misconceptions_1","title":"Common Misconceptions","text":"Misconception Reality \"Deeper trees are always better\" Deeper trees overfit. Find the sweet spot via cross-validation. \"Decision trees require feature scaling\" Trees are scale-invariant! One of their advantages. \"Feature importance = causal importance\" Importance only shows predictive power, not causation. \"Trees can't capture interactions\" Trees naturally capture interactions through hierarchical structure."},{"location":"modules/03-classification/#33-handling-imbalanced-data","title":"3.3 Handling Imbalanced Data","text":""},{"location":"modules/03-classification/#why-accuracy-is-misleading","title":"Why Accuracy is Misleading","text":"<p>Fraud detection: - 99.9% of transactions are legitimate - 0.1% are fraudulent</p> <p>A model that predicts \"legitimate\" for EVERYTHING: - Accuracy: 99.9% - Catches zero fraud!</p> <p>Accuracy is useless for imbalanced classes.</p> <p>When is it \"imbalanced\"? 60/40 is typically fine; 70/30 is mild; 80/20 starts requiring attention; 90/10 likely needs specialized techniques; 95/5 definitely needs SMOTE, class weights, or threshold adjustment. But it's not just about ratio\u2014absolute numbers matter (90/10 with 10,000 minority samples is fine; with 100 is problematic). The practical test: does your model learn anything about the minority class? If accuracy comes from ignoring the minority entirely, you have a problem.</p>"},{"location":"modules/03-classification/#better-metrics","title":"Better Metrics","text":"<p>Precision: Of those we flagged as positive, how many actually were? $\\(Precision = \\frac{TP}{TP + FP}\\)$</p> <p>Recall: Of actual positives, how many did we catch? $\\(Recall = \\frac{TP}{TP + FN}\\)$</p> <p>F1 Score: Harmonic mean balancing both $\\(F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}\\)$</p> <p>Why harmonic mean? It punishes extreme imbalance: - Precision = 100%, Recall = 1% \u2192 F1 = 2% - Precision = 50%, Recall = 50% \u2192 F1 = 50%</p>"},{"location":"modules/03-classification/#the-precision-recall-trade-off","title":"The Precision-Recall Trade-off","text":"<p>Usually you can't maximize both: - High precision \u2192 few false alarms but miss some positives - High recall \u2192 catch most positives but more false alarms</p> <p>Business context determines priority: - High precision: Email marketing (don't waste budget) - High recall: Medical screening (don't miss sick patients)</p>"},{"location":"modules/03-classification/#resampling-smote","title":"Resampling: SMOTE","text":"<p>SMOTE (Synthetic Minority Over-sampling Technique): - Creates synthetic minority examples - Interpolates between existing minority points - Better than simple duplication</p> <pre><code>from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n</code></pre> <p>Important: Only apply SMOTE to training data, never test data! The test set must reflect real-world conditions\u2014your deployed model will face the true class distribution. SMOTE is a training trick to help the model learn about the minority class, not a data transformation. The correct workflow: (1) Split data first. (2) Apply SMOTE only to training set. (3) Evaluate on original, imbalanced test set. (4) Use appropriate metrics (F1, precision, recall) that work for imbalanced data.</p>"},{"location":"modules/03-classification/#class-weights","title":"Class Weights","text":"<p>Many algorithms have built-in support:</p> <pre><code>model = LogisticRegression(class_weight='balanced')\nmodel = DecisionTreeClassifier(class_weight='balanced')\n</code></pre> <p>Effect: Increases penalty for misclassifying minority class. Often simpler than resampling.</p>"},{"location":"modules/03-classification/#threshold-adjustment","title":"Threshold Adjustment","text":"<pre><code>y_proba = model.predict_proba(X_test)[:, 1]\nthreshold = 0.3  # Instead of 0.5\ny_pred = (y_proba &gt;= threshold).astype(int)\n</code></pre> <p>Lower threshold \u2192 predict positive more often \u2192 higher recall, lower precision.</p>"},{"location":"modules/03-classification/#business-context-examples","title":"Business Context Examples","text":"Domain Priority Reason Fraud Detection High recall Cost of fraud &gt;&gt; investigation cost Medical Diagnosis High recall Don't miss sick patients Churn Prediction Balance Retention cost vs customer value Manufacturing QC Depends Defect severity vs discard cost"},{"location":"modules/03-classification/#common-misconceptions_2","title":"Common Misconceptions","text":"Misconception Reality \"Always balance classes to 50-50\" Optimal ratio depends on the problem. Original distribution may be meaningful. \"SMOTE is always better than oversampling\" SMOTE can create unrealistic synthetic examples. Test both. \"Class weights and resampling do the same thing\" Similar effect but different mechanisms. Results can differ. \"Imbalanced data is always a problem\" If minority class is well-separated, imbalance may not hurt. Always check metrics."},{"location":"modules/03-classification/#34-hyperparameter-optimization","title":"3.4 Hyperparameter Optimization","text":""},{"location":"modules/03-classification/#parameters-vs-hyperparameters","title":"Parameters vs Hyperparameters","text":"Parameters Hyperparameters Learned during training Set before training Model learns via .fit() You choose before .fit() Example: Coefficients Example: Regularization strength Example: Split points Example: Max tree depth <p>Hyperparameters control HOW the model learns.</p> <p>Finding hyperparameters: Use official documentation (search \"sklearn DecisionTreeClassifier\"), in-code exploration (<code>model.get_params()</code>, <code>help(DecisionTreeClassifier)</code>), or IDE autocomplete. Not all hyperparameters matter equally\u2014most algorithms have 3-5 \"important\" ones: for decision trees, focus on <code>max_depth</code>, <code>min_samples_split</code>, <code>min_samples_leaf</code>; for Random Forests add <code>n_estimators</code>, <code>max_features</code>; for XGBoost: <code>learning_rate</code>, <code>max_depth</code>, <code>n_estimators</code>, <code>subsample</code>.</p>"},{"location":"modules/03-classification/#grid-search","title":"Grid Search","text":"<p>Try every combination in a predefined grid:</p> <pre><code>from sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'max_depth': [3, 5, 7, 10],\n    'min_samples_split': [2, 5, 10],\n}\n# Total: 4 \u00d7 3 = 12 combinations\n\ngrid_search = GridSearchCV(\n    estimator=DecisionTreeClassifier(random_state=42),\n    param_grid=param_grid,\n    cv=5,\n    scoring='f1'\n)\ngrid_search.fit(X_train, y_train)\nprint(f\"Best params: {grid_search.best_params_}\")\n</code></pre> <p>Pros: Exhaustive, reproducible Cons: Exponential growth, wastes time on bad regions</p>"},{"location":"modules/03-classification/#random-search","title":"Random Search","text":"<p>Sample random combinations from distributions:</p> <pre><code>from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\nparam_distributions = {\n    'max_depth': randint(2, 20),\n    'min_samples_split': randint(2, 50),\n}\n\nrandom_search = RandomizedSearchCV(\n    estimator=DecisionTreeClassifier(random_state=42),\n    param_distributions=param_distributions,\n    n_iter=50,\n    cv=5,\n    scoring='f1',\n    random_state=42\n)\nrandom_search.fit(X_train, y_train)\n</code></pre>"},{"location":"modules/03-classification/#why-random-often-beats-grid","title":"Why Random Often Beats Grid","text":"<p>Key insight (Bergstra &amp; Bengio, 2012): - Not all hyperparameters are equally important - Grid search wastes trials on unimportant parameters - Random search explores more values of what matters</p> <p>In practice, random search often beats grid search with the same computational budget.</p> <p>Standard ranges for common hyperparameters: <code>max_depth</code>: 2-20 for trees; <code>n_estimators</code>: 50-500 for forests/boosting; <code>learning_rate</code>: 0.001-0.3 for boosting; <code>min_samples_split</code>: 2-50; <code>C</code> (regularization): 0.001-100 (log scale). If the best value is at the edge of your range, extend that direction. Start with wide, log-spaced ranges, do a coarse search (10 values), then refine in the promising region.</p>"},{"location":"modules/03-classification/#bayesian-optimization-optuna","title":"Bayesian Optimization (optuna)","text":"<p>Use past results to guide future trials:</p> <pre><code>import optuna\n\ndef objective(trial):\n    params = {\n        'max_depth': trial.suggest_int('max_depth', 2, 20),\n        'min_samples_split': trial.suggest_int('min_samples_split', 2, 50),\n    }\n    model = DecisionTreeClassifier(**params, random_state=42)\n    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='f1')\n    return scores.mean()\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=50)\nprint(f\"Best params: {study.best_params}\")\n</code></pre> <p>More efficient than random search\u2014learns from previous trials.</p>"},{"location":"modules/03-classification/#never-use-test-set-for-tuning","title":"Never Use Test Set for Tuning!","text":"<p>Correct workflow: 1. Split into train/test 2. Use cross-validation on training set for tuning 3. Select best hyperparameters via CV score 4. Retrain on full training set 5. Evaluate once on test set</p> <p>If you tune on test set, your estimate is no longer unbiased.</p> <p>After tuning: Retrain on all training data with the best hyperparameters. Cross-validation models were trained on only (K-1)/K of your data. Retraining on 100% gives the model more examples. <code>GridSearchCV</code> does this automatically\u2014<code>grid_search.best_estimator_</code> is already retrained on the full training set.</p>"},{"location":"modules/03-classification/#common-misconceptions_3","title":"Common Misconceptions","text":"Misconception Reality \"More hyperparameter tuning always helps\" Diminishing returns. 50-100 trials often enough. Risk overfitting to validation data. \"Grid search is more thorough\" Grid is exhaustive only for values you specify. Random can find values between grid points. \"Best hyperparameters are universal\" Optimal hyperparameters depend on your specific dataset. \"Use test set to choose hyperparameters\" Never! Use cross-validation on training data."},{"location":"modules/03-classification/#reflection-questions","title":"Reflection Questions","text":"<ol> <li> <p>A model predicts P(churn) = 0.6 for a customer. What does this actually mean? How confident should we be?</p> </li> <li> <p>Why might you choose a threshold other than 0.5? Give scenarios for very low and very high thresholds.</p> </li> <li> <p>A logistic regression coefficient for 'number of support tickets' is 0.3. How would you explain this to a stakeholder?</p> </li> <li> <p>You build a decision tree with 100% training accuracy. Is this good or bad? What would you do next?</p> </li> <li> <p>In fraud detection with 0.1% fraud rate, a model achieves 99.9% accuracy. What's wrong with celebrating this?</p> </li> <li> <p>When would you prefer high precision over high recall? Give a business example.</p> </li> <li> <p>Why might random search find better hyperparameters than grid search with the same budget?</p> </li> </ol>"},{"location":"modules/03-classification/#practice-problems","title":"Practice Problems","text":"<ol> <li> <p>Calculate odds and log-odds for P = 0.8</p> </li> <li> <p>Given coefficients \u03b2\u2080 = -2, \u03b2\u2081 = 0.5, \u03b2\u2082 = -0.3, calculate P(Y=1) when x\u2081 = 4, x\u2082 = 2</p> </li> <li> <p>Draw what a decision tree boundary would look like for 2D data with 2 splits</p> </li> <li> <p>Given a 95% legitimate / 5% fraud dataset: if we predict all legitimate, what's accuracy? Precision for fraud? Recall for fraud?</p> </li> <li> <p>Choose between precision and recall priority for: (a) spam filter, (b) cancer screening, (c) loan approval</p> </li> </ol>"},{"location":"modules/03-classification/#chapter-summary","title":"Chapter Summary","text":"<p>Six key takeaways from Module 3:</p> <ol> <li> <p>Logistic regression outputs probabilities via sigmoid; threshold for classification</p> </li> <li> <p>Odds ratios (exponentiate coefficients) translate to business-friendly interpretation</p> </li> <li> <p>Decision trees are intuitive but overfit easily\u2014use pruning</p> </li> <li> <p>Accuracy is misleading for imbalanced data\u2014use precision/recall/F1</p> </li> <li> <p>Handle imbalance with SMOTE, class weights, or threshold adjustment</p> </li> <li> <p>Hyperparameter tuning via cross-validation, never on test set</p> </li> </ol>"},{"location":"modules/03-classification/#whats-next","title":"What's Next","text":"<p>In Module 4, we tackle Ensemble Methods: - Random Forests (ensembles of decision trees) - Gradient Boosting (XGBoost, LightGBM) - Why combining weak learners creates strong models</p> <p>Understanding decision trees is essential\u2014Random Forests take everything we learned about trees and combine many of them for better performance.</p>"},{"location":"modules/04-ensemble-methods/","title":"Module 4: Ensemble Methods","text":""},{"location":"modules/04-ensemble-methods/#introduction","title":"Introduction","text":"<p>In Module 3, we learned about decision trees\u2014intuitive classifiers that are easy to interpret but prone to overfitting. Deep trees memorize training data; shallow trees underfit.</p> <p>This module answers a natural question: What if we could get the benefits of deep trees without the overfitting?</p> <p>The answer is ensemble methods. Instead of training one model, we train many models and combine their predictions. This simple idea\u2014the wisdom of crowds\u2014turns out to be one of the most powerful techniques in machine learning.</p> <p>By the end of this module, you'll understand two major ensemble paradigms: bagging (where Random Forests come from) and boosting (where XGBoost comes from). These methods dominate tabular data competitions and are workhorses in industry.</p>"},{"location":"modules/04-ensemble-methods/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, you should be able to:</p> <ol> <li>Explain the intuition behind ensemble methods and why combining models outperforms individuals</li> <li>Implement bagging (Random Forests) and boosting (XGBoost)</li> <li>Interpret feature importance for business stakeholders</li> <li>Select appropriate ensemble strategies based on problem characteristics</li> </ol>"},{"location":"modules/04-ensemble-methods/#41-ensemble-learning-concepts","title":"4.1 Ensemble Learning Concepts","text":""},{"location":"modules/04-ensemble-methods/#the-wisdom-of-crowds","title":"The Wisdom of Crowds","text":"<p>Galton's Ox Experiment (1907):</p> <p>At a county fair, 787 people tried to guess the weight of an ox. Individual guesses varied wildly\u2014some way too high, some way too low.</p> <ul> <li>Median of all guesses: 1,207 lbs</li> <li>Actual weight: 1,198 lbs (&lt; 1% error!)</li> </ul> <p>How can a crowd of non-experts outperform individuals?</p> <p>Key insight: Errors cancel out when they're uncorrelated. Some people guessed too high, some too low. The errors went in different directions. When you average, errors cancel and the true signal remains.</p> <p>This is exactly the principle behind ensemble machine learning.</p> <p>Correlation matters: Ensembles work best with uncorrelated errors, but help even with partially correlated errors. If individual models have variance \u03c3\u00b2 and correlation \u03c1 between errors, ensemble variance is \u03c1\u03c3\u00b2 + (1-\u03c1)\u03c3\u00b2/n. With perfect independence (\u03c1=0), variance drops as 1/n. With perfect correlation (\u03c1=1), averaging doesn't help. In practice, even 50% correlation provides substantial benefit.</p>"},{"location":"modules/04-ensemble-methods/#how-ensembles-improve-predictions","title":"How Ensembles Improve Predictions","text":"<p>Variance Reduction (Bagging): - Single decision trees are high-variance estimators - Small changes in training data \u2192 very different trees - Averaging multiple trees reduces instability - Mathematically: \\(Var(average) = Var(individual) / n\\) when predictions are uncorrelated</p> <p>Bias Reduction (Boosting): - Each new model focuses on errors of previous models - The ensemble gradually learns patterns individual weak learners missed - Sequential learning reduces systematic error</p>"},{"location":"modules/04-ensemble-methods/#model-diversity-is-critical","title":"Model Diversity is Critical","text":"<p>Ensembles only help if the models are different!</p> <p>If all models make the same mistakes, averaging doesn't help. Think: if you ask 787 people the same leading question and they all guess the same wrong answer, the median is still wrong.</p> <p>How ensemble methods create diversity: - Random Forests: Random sampling of data AND features - Boosting: Sequential focus on different examples - Different algorithms: Different inductive biases (heterogeneous ensembles)</p> <p>Heterogeneous ensembles combine completely different algorithms (neural network + decision tree + logistic regression). Different algorithms have different inductive biases, making them unlikely to make the same mistakes. The Netflix Prize winning solution combined 107 different models.</p>"},{"location":"modules/04-ensemble-methods/#common-misconceptions","title":"Common Misconceptions","text":"Misconception Reality \"More models always means better results\" Diminishing returns kick in quickly. 1 \u2192 10 trees helps a lot; 100 \u2192 1000 helps little. \"Ensembles are always better than single models\" For simple problems or when interpretability is paramount, single models may be preferable. \"You need sophisticated models in your ensemble\" Ensembles of simple models (shallow trees, stumps) can be remarkably effective."},{"location":"modules/04-ensemble-methods/#42-bagging-methods","title":"4.2 Bagging Methods","text":""},{"location":"modules/04-ensemble-methods/#three-components-random-forest","title":"Three Components: Random Forest","text":"Component Random Forest Decision Model Ensemble of decision trees \u2014 each tree votes, majority wins Quality Measure Gini/entropy for individual trees; OOB error for ensemble Update Method Independent parallel training \u2014 no iteration between trees <p>Key insight: Random Forest doesn't \"update\" traditionally. Each tree trains independently on a bootstrap sample. Learning happens through aggregation\u2014the wisdom of crowds.</p>"},{"location":"modules/04-ensemble-methods/#bootstrap-aggregating-bagging","title":"Bootstrap Aggregating (Bagging)","text":"<p>Algorithm:</p> <ol> <li>Create B bootstrap samples (sample with replacement)</li> <li>Each sample same size as original data</li> <li>Some observations appear multiple times, some not at all</li> <li> <p>~63.2% unique observations per sample</p> </li> <li> <p>Train a separate model on each sample</p> </li> <li> <p>Aggregate predictions:</p> </li> <li>Regression: Average</li> <li>Classification: Majority vote</li> </ol> <p>Why ~63.2%? When sampling n observations with replacement from n, the probability any specific row is never selected is:</p> \\[(1 - \\frac{1}{n})^n \\approx e^{-1} \\approx 0.368\\] <p>So ~36.8% are left out (\"out-of-bag\"), meaning ~63.2% are included.</p> <p>Why replacement? Without replacement at the same size, you'd get identical datasets. With replacement: some observations appear multiple times (emphasized), some don't appear (~36.8%, providing OOB validation), and different trees emphasize different observations\u2014creating diversity. Bootstrap sampling approximates drawing fresh samples from the true population.</p>"},{"location":"modules/04-ensemble-methods/#random-forests-double-randomness","title":"Random Forests: Double Randomness","text":"<p>Random Forests extend bagging with two sources of randomness:</p> <ol> <li> <p>Row sampling (from bagging): Each tree gets a bootstrap sample</p> </li> <li> <p>Feature sampling (unique to RF): At each split, consider only a random subset</p> </li> <li>Default: \\(\\sqrt{d}\\) features for classification (where d = total features)</li> </ol> <p>Why feature sampling matters:</p> <p>Imagine one incredibly predictive feature (credit score for loan default). Without feature sampling, every tree uses it as the root split. All trees become highly correlated.</p> <p>With feature sampling, each split considers a random subset. Sometimes credit score isn't available. The tree finds other splits. This creates diversity.</p> <p>The tradeoff: Ignoring the best feature sometimes hurts individual trees (higher bias), but trees become more diverse (lower correlation). The ensemble variance formula shows reducing correlation (\u03c1) often helps more than the slight increase in individual variance (\u03c3\u00b2). Random Forests typically outperform bagged trees precisely because of this tradeoff. The <code>max_features</code> hyperparameter controls this\u2014default \u221ad is a good starting point.</p>"},{"location":"modules/04-ensemble-methods/#why-bagging-reduces-overfitting","title":"Why Bagging Reduces Overfitting","text":"<ul> <li>A single deep tree overfits to specific patterns</li> <li>Each tree in the forest also overfits, but to DIFFERENT patterns</li> <li>When we average, idiosyncratic overfitting cancels out</li> <li>True signal remains (all trees agree on it)</li> </ul> <p>The ensemble variance formula:</p> \\[Var(ensemble) = \\rho\\sigma^2 + \\frac{(1-\\rho)\\sigma^2}{n}\\] <p>Where: - \\(\\sigma^2\\) = variance of individual tree predictions - \\(\\rho\\) = average correlation between trees (0 = independent, 1 = identical) - \\(n\\) = number of trees</p> <p>Reading this formula: - First term (\\(\\rho\\sigma^2\\)): Irreducible variance from correlation - Second term: Shrinks as you add trees</p> <p>Key insight: Lower correlation between trees = better ensemble. Feature sampling specifically reduces \\(\\rho\\).</p> <p>Number of trees: 100-500 trees usually sufficient. Plot OOB error vs. n_estimators\u2014it decreases rapidly then flattens. Unlike boosting, more RF trees never hurt performance; they just stop helping. More trees mean more memory and slower inference, so balance accuracy against cost.</p>"},{"location":"modules/04-ensemble-methods/#feature-importance","title":"Feature Importance","text":"<p>Mean Decrease in Impurity (MDI): - Sum of impurity decreases from splits using each feature, averaged across trees - Fast to compute - Can favor high-cardinality features</p> <p>Permutation Importance: - Shuffle each feature and measure accuracy decrease - More reliable, slower - Preferred for stakeholder communication</p> <p>Important caveat: Importance \u2260 direction of effect! Importance tells you which features the model relies on, not HOW they affect predictions. For that, use SHAP values (Module 9).</p> <pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.inspection import permutation_importance\n\nrf = RandomForestClassifier(\n    n_estimators=100,\n    max_features='sqrt',\n    oob_score=True,\n    random_state=42\n)\nrf.fit(X_train, y_train)\n\n# OOB score (free validation!)\nprint(f\"OOB Accuracy: {rf.oob_score_:.3f}\")\n\n# MDI importance (fast)\nimportance_mdi = rf.feature_importances_\n\n# Permutation importance (more reliable)\nperm_imp = permutation_importance(rf, X_test, y_test, n_repeats=10)\n</code></pre>"},{"location":"modules/04-ensemble-methods/#out-of-bag-oob-error","title":"Out-of-Bag (OOB) Error","text":"<p>Each bootstrap sample leaves out ~36.8% of observations. These \"out-of-bag\" samples provide free validation:</p> <ul> <li>For each observation, predict using only trees that didn't train on it</li> <li>OOB error \u2248 cross-validation error, but FREE!</li> </ul> <pre><code>rf = RandomForestClassifier(oob_score=True)\nrf.fit(X_train, y_train)\nprint(f\"OOB Accuracy: {rf.oob_score_}\")\n</code></pre>"},{"location":"modules/04-ensemble-methods/#common-misconceptions_1","title":"Common Misconceptions","text":"Misconception Reality \"Random Forest can't overfit\" It can! Deep trees with too few estimators still overfit. Tuning on test set causes overfitting to that. \"More trees is always better\" Diminishing returns. 100-500 usually sufficient. \"Random Forest is a black box\" Feature importance and SHAP make it reasonably interpretable. \"Feature importance = feature effect\" Importance shows reliance, not direction of effect."},{"location":"modules/04-ensemble-methods/#43-boosting-methods","title":"4.3 Boosting Methods","text":""},{"location":"modules/04-ensemble-methods/#three-components-gradient-boosting","title":"Three Components: Gradient Boosting","text":"Component Gradient Boosting (XGBoost) Decision Model Sequential ensemble \u2014 sum of many shallow trees Quality Measure Any differentiable loss + regularization Update Method Gradient descent in function space \u2014 each tree corrects previous errors <p>The update method is fascinating: instead of updating parameters, we add new functions (trees). Each tree predicts the negative gradient (residuals).</p>"},{"location":"modules/04-ensemble-methods/#the-boosting-philosophy","title":"The Boosting Philosophy","text":"<p>Build models sequentially, where each new model focuses on mistakes of previous ones.</p> Bagging Boosting Parallel (independent trees) Sequential (dependent trees) Reduces variance Reduces bias (and variance) Deep trees Shallow trees typical <p>Visual metaphors: - Bagging: Committee of experts who work independently and vote - Boosting: Relay team where each runner covers for previous weaknesses</p>"},{"location":"modules/04-ensemble-methods/#adaboost-adaptive-boosting","title":"AdaBoost: Adaptive Boosting","text":"<ol> <li>Start with equal weights for all training examples</li> <li>Train a weak learner (often a \"stump\"\u2014one split)</li> <li>Identify misclassified examples</li> <li>Increase weights on misclassified examples</li> <li>Train next weak learner on reweighted data</li> <li>Repeat</li> </ol> <p>Key insight: Each subsequent learner specializes in hard examples previous learners got wrong.</p> <p>Boosting and outliers: Boosting can obsess over mislabeled or impossible-to-fit examples. Mitigation: (1) <code>subsample</code> (0.8) so outliers don't appear every round, (2) lower learning rate to limit per-iteration damage, (3) regularization (<code>reg_alpha</code>, <code>reg_lambda</code>) to prevent extreme predictions, (4) early stopping before overfitting to noise. Random Forests are more robust because outliers only affect ~63% of trees and no tree specifically focuses on them.</p>"},{"location":"modules/04-ensemble-methods/#gradient-boosting-machines","title":"Gradient Boosting Machines","text":"<p>Core innovation: Fit each new tree to the residuals (errors).</p> <ol> <li>Make initial prediction (often the mean)</li> <li>Calculate residuals: \\(actual - predicted\\)</li> <li>Fit a tree to predict the residuals</li> <li>Add this tree's predictions (with learning rate)</li> <li>Calculate new residuals</li> <li>Repeat</li> </ol> <p>Why \"gradient\"? For MSE loss:</p> \\[\\frac{\\partial L}{\\partial \\hat{y}} = -(y - \\hat{y}) = -\\text{residual}\\] <p>The residual IS the negative gradient of the loss. When we fit trees to residuals, we're following the gradient in function space.</p> <p>Gradient in function space: Normal gradient descent optimizes parameters (adjust \u03b8). Gradient boosting optimizes functions (add a new tree). For squared error, the negative gradient is simply the residual. Fitting a tree to residuals approximates \"what should I add to reduce error?\" The learning rate works like in gradient descent\u2014taking fractional steps (0.1 \u00d7 tree_prediction) prevents overshooting. So: F_new(x) = F_old(x) + learning_rate \u00d7 new_tree(x). Each tree is a step in function space toward lower loss.</p>"},{"location":"modules/04-ensemble-methods/#key-boosting-hyperparameters","title":"Key Boosting Hyperparameters","text":"Parameter Effect <code>n_estimators</code> More \u2192 more capacity, but overfit risk <code>learning_rate</code> Smaller \u2192 need more trees, often better <code>max_depth</code> Usually 3-8 (much shallower than RF) <p>Trade-off: Lower learning rate + more trees often gives best results but takes longer.</p> <p>Tree depth difference: - Random Forest: Deep, fully-grown trees (low bias, high variance). Averaging reduces variance. - Boosting: Shallow trees (high bias). Sequential correction reduces bias.</p>"},{"location":"modules/04-ensemble-methods/#xgboost-the-competition-champion","title":"XGBoost: The Competition Champion","text":"<p>XGBoost adds optimizations that make it dominant:</p> <ol> <li>Regularization: L1/L2 penalties on leaf weights</li> <li>Parallel processing: Split evaluation parallelized within trees</li> <li>Missing value handling: Learns optimal direction for missing values</li> <li>Histogram-based splitting: Bins features for speed</li> </ol> <p>Why it dominates: Won more Kaggle competitions than any other algorithm. Widely adopted in finance, insurance, tech.</p> <p>When Random Forest is better: (1) Noisy labels\u2014RF more robust, noise doesn't compound; (2) Limited tuning time\u2014RF works well with defaults; (3) Parallelization\u2014RF trees train independently; (4) Small datasets\u2014boosting can overfit quickly. A well-tuned XGBoost beats a well-tuned RF, but default RF often beats default XGBoost. In many real-world scenarios, the difference is 1-2%.</p>"},{"location":"modules/04-ensemble-methods/#xgboost-with-early-stopping","title":"XGBoost with Early Stopping","text":"<p>Always use early stopping with boosting!</p> <pre><code>import xgboost as xgb\n\nxgb_model = xgb.XGBClassifier(\n    n_estimators=1000,\n    learning_rate=0.1,\n    max_depth=5,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    reg_alpha=0.1,\n    reg_lambda=1.0,\n    random_state=42\n)\n\nxgb_model.fit(\n    X_train, y_train,\n    eval_set=[(X_val, y_val)],\n    early_stopping_rounds=10,  # Stop if no improvement for 10 rounds\n    verbose=False\n)\n\nprint(f\"Best iteration: {xgb_model.best_iteration}\")\n</code></pre> <p>Without early stopping, boosting overfits. With it, training stops when validation plateaus.</p> <p>Why early stopping beats fixed n_estimators: The optimal number depends on learning rate, tree depth, data complexity, and sample size\u2014a fixed number can't adapt. Set a large n_estimators as an upper limit, monitor validation loss, stop when no improvement for N consecutive rounds. The model finds its own stopping point, works with any learning rate, and prevents overfitting automatically. Always use a separate validation set for early stopping\u2014not your final test set.</p>"},{"location":"modules/04-ensemble-methods/#lightgbm-and-catboost","title":"LightGBM and CatBoost","text":"<p>LightGBM: - Even faster than XGBoost - Histogram-based splitting - Great for very large datasets</p> <p>CatBoost: - Excellent categorical feature handling - No one-hot encoding needed - Often works well with defaults</p> <p>Rule of thumb: Start with XGBoost. Try LightGBM for very large data. Try CatBoost for many categorical features.</p>"},{"location":"modules/04-ensemble-methods/#bagging-vs-boosting-when-to-use-each","title":"Bagging vs Boosting: When to Use Each","text":"Scenario Recommendation High-variance (deep trees) Bagging (RF) High-bias (shallow trees) Boosting Fast training needed Bagging (parallelizable) Best accuracy needed Boosting (often wins) Noisy labels Bagging (more robust) Need interpretability Random Forest"},{"location":"modules/04-ensemble-methods/#common-misconceptions_2","title":"Common Misconceptions","text":"Misconception Reality \"XGBoost is always best\" No Free Lunch. Linear models beat it on linear data. Neural networks beat it on images/text. \"Boosting can't overfit\" Very much can! Use early stopping. \"More boosting rounds = better\" Unlike RF, more rounds increases overfit risk. \"XGBoost, LightGBM, CatBoost are completely different\" All gradient boosting variants. Similar core ideas."},{"location":"modules/04-ensemble-methods/#44-other-ensemble-techniques","title":"4.4 Other Ensemble Techniques","text":""},{"location":"modules/04-ensemble-methods/#stacking","title":"Stacking","text":"<p>Use model predictions as features for a \"meta-learner.\"</p> <pre><code>Level 0:   RF_pred    XGB_pred    LR_pred\n              \u2193           \u2193          \u2193\nLevel 1:     Meta-model (e.g., Logistic Regression)\n                         \u2193\n                  Final Prediction\n</code></pre> <p>How it works: 1. Train several level-0 models (RF, XGBoost, logistic regression) 2. Generate predictions using cross-validation (out-of-fold) 3. Use predictions as features for level-1 meta-model 4. Meta-model learns which base models to trust</p> <p>Critical: Must use out-of-fold predictions to avoid leakage!</p> <p>Why out-of-fold matters:</p> <p>If you train RF on all training data and use its predictions on that same data as meta-features, RF makes artificially confident predictions (it's seen those examples). This won't generalize.</p> <p>Correct approach: 1. Split into K folds 2. For fold 1: Train on folds 2-5, predict fold 1 3. For fold 2: Train on folds 1,3-5, predict fold 2 4. Continue for all folds 5. Meta-model trains on these honest predictions</p> <p>Multi-level stacking: Going deeper is possible but rarely worthwhile. Two levels is usually sufficient (Netflix Prize used two). Each additional level requires proper out-of-fold predictions (complex bookkeeping), increases overfitting risk, and slows inference. In production, a single well-tuned XGBoost or simple two-level stack is almost always preferred.</p>"},{"location":"modules/04-ensemble-methods/#voting-classifiers","title":"Voting Classifiers","text":"<p>Simpler than stacking: combine predictions directly.</p> <p>Hard voting: Each model votes; majority wins.</p> <p>Soft voting: Average probability estimates; pick highest.</p> <pre><code># Model 1: P(A)=0.7, P(B)=0.3\n# Model 2: P(A)=0.4, P(B)=0.6\n# Model 3: P(A)=0.8, P(B)=0.2\n# Average: P(A)=0.63 \u2192 Class A\n</code></pre> <p>Soft voting usually performs better (uses more information).</p> <pre><code>from sklearn.ensemble import VotingClassifier, StackingClassifier\n\n# Soft Voting\nvoting_clf = VotingClassifier(\n    estimators=[\n        ('rf', RandomForestClassifier(n_estimators=100)),\n        ('gb', GradientBoostingClassifier(n_estimators=100)),\n        ('lr', LogisticRegression())\n    ],\n    voting='soft'\n)\n\n# Stacking\nstacking_clf = StackingClassifier(\n    estimators=[\n        ('rf', RandomForestClassifier(n_estimators=100)),\n        ('gb', GradientBoostingClassifier(n_estimators=100))\n    ],\n    final_estimator=LogisticRegression(),\n    cv=5\n)\n</code></pre>"},{"location":"modules/04-ensemble-methods/#when-to-use-each-approach","title":"When to Use Each Approach","text":"Method Use When Simple voting Models roughly equal; quick solution Weighted voting Some models clearly better Stacking Time for complexity; competition setting <p>Avoid sophisticated ensembles when: - Explainability is crucial - Fast inference needed - Limited compute</p>"},{"location":"modules/04-ensemble-methods/#common-misconceptions_3","title":"Common Misconceptions","text":"Misconception Reality \"Stacking always improves performance\" If base models are highly correlated, stacking adds complexity without benefit. \"More diverse base models = better\" Diversity helps, but models still need to be individually competent. \"Stacking is just averaging with extra steps\" Meta-learner can learn complex patterns like \"trust RF for certain input ranges.\""},{"location":"modules/04-ensemble-methods/#reflection-questions","title":"Reflection Questions","text":"<ol> <li> <p>Why does the wisdom of crowds work? Under what conditions would it fail?</p> </li> <li> <p>A colleague says Random Forest can never overfit. How would you respond?</p> </li> <li> <p>Why sample features at each split rather than once per tree?</p> </li> <li> <p>When might boosting overfit more easily than bagging? What would you adjust?</p> </li> <li> <p>A data scientist says they always use XGBoost because \"it wins Kaggle.\" What's your response?</p> </li> <li> <p>You have 5 models with accuracies 82%, 81%, 79%, 78%, 75%. Would you ensemble all 5? Why or why not?</p> </li> </ol>"},{"location":"modules/04-ensemble-methods/#practice-problems","title":"Practice Problems","text":"<ol> <li> <p>Derive why ~63.2% of observations appear in each bootstrap sample</p> </li> <li> <p>If you have 100 features in a classification problem, how many are considered at each split in Random Forest (default)?</p> </li> <li> <p>Explain why Random Forest feature importance might differ from permutation importance</p> </li> <li> <p>Draw a diagram showing how 5 stumps combine in AdaBoost vs how 5 shallow trees combine in Gradient Boosting</p> </li> <li> <p>You train XGBoost without early stopping and see training accuracy at 99% but test accuracy at 75%. Diagnose and fix.</p> </li> </ol>"},{"location":"modules/04-ensemble-methods/#chapter-summary","title":"Chapter Summary","text":"<p>Six key takeaways from Module 4:</p> <ol> <li> <p>Ensembles work by combining diverse models\u2014errors cancel out</p> </li> <li> <p>Bagging (Random Forests) reduces variance through averaging independent trees</p> </li> <li> <p>Boosting (XGBoost) reduces bias through sequential learning from errors</p> </li> <li> <p>Feature importance shows predictive power, not effect direction</p> </li> <li> <p>Early stopping is essential for boosting methods</p> </li> <li> <p>Choose wisely: Random Forest for robustness, XGBoost for accuracy</p> </li> </ol>"},{"location":"modules/04-ensemble-methods/#whats-next","title":"What's Next","text":"<p>In Module 5, we tackle Unsupervised Learning: - Clustering (K-Means, hierarchical) - Dimensionality reduction (PCA) - Finding structure without labels</p> <p>So far, we've had a target variable to predict. In unsupervised learning, there's no target\u2014we're discovering hidden patterns in the data.</p>"},{"location":"modules/05-unsupervised/","title":"Module 5: Unsupervised Learning","text":""},{"location":"modules/05-unsupervised/#introduction","title":"Introduction","text":"<p>Today marks a significant shift in how we think about machine learning.</p> <p>In Modules 2 through 4, we always had a target variable\u2014sales, churn, fraud. We had labels, and we trained models to predict those labels.</p> <p>Now we throw that away. No labels. No target variable.</p> <p>Unsupervised learning is about discovering structure in data when you don't know what you're looking for. You're exploring, not predicting.</p> <p>This might sound less useful, but unsupervised learning solves critical business problems: customer segmentation, anomaly detection, data visualization, feature extraction. These are problems where labels don't exist or are too expensive to obtain.</p> <p>Validating unsupervised learning: \"Right\" is about usefulness, not correctness. Use internal metrics (silhouette, inertia), check stability across runs, and\u2014most importantly\u2014validate with domain experts. Do clusters suggest actionable strategies? A \"statistically optimal\" 7-cluster solution that marketing can't operationalize is less useful than a 3-cluster solution they can act on.</p>"},{"location":"modules/05-unsupervised/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, you should be able to:</p> <ol> <li>Explain the difference between supervised and unsupervised learning</li> <li>Apply K-means and DBSCAN clustering algorithms and interpret results</li> <li>Determine optimal number of clusters using elbow method and silhouette scores</li> <li>Apply PCA for dimensionality reduction and interpret principal components</li> <li>Use manifold learning techniques (t-SNE, UMAP) for visualization</li> <li>Identify business applications for clustering and dimensionality reduction</li> </ol>"},{"location":"modules/05-unsupervised/#51-clustering","title":"5.1 Clustering","text":""},{"location":"modules/05-unsupervised/#supervised-vs-unsupervised","title":"Supervised vs Unsupervised","text":"Supervised Unsupervised Have labels No labels Learn to predict Discover structure Regression, Classification Clustering, Dim. Reduction <p>Supervised: \"Here are the right answers; learn to predict them.\"</p> <p>Unsupervised: \"Here's the data; find interesting patterns.\"</p>"},{"location":"modules/05-unsupervised/#three-components-k-means","title":"Three Components: K-Means","text":"<p>Even unsupervised algorithms fit our three-component framework:</p> Component K-Means Decision Model Cluster assignments \u2014 each point belongs to nearest centroid Quality Measure Within-cluster sum of squares (inertia) Update Method Iterative assignment-update \u2014 alternate between assigning and moving centroids <p>Key difference from supervised learning: Without labels, we define \"quality\" differently. Instead of prediction error, we measure how compact and well-separated clusters are.</p> <p>Distinguishing real structure from noise: Clustering algorithms will always find clusters\u2014even in random data. Use the gap statistic (compares quality to random data), stability analysis (cluster on subsets\u2014real structure is stable), and multiple algorithms (if K-means, DBSCAN, and hierarchical all find similar groups, structure is more credible). Always verify clusters predict something meaningful.</p>"},{"location":"modules/05-unsupervised/#clustering-applications","title":"Clustering Applications","text":"<ul> <li>Customer segmentation \u2014 Group by purchasing behavior, target marketing per segment</li> <li>Document grouping \u2014 Organize by topic without predefined categories</li> <li>Anomaly detection \u2014 Find observations that don't fit any group</li> <li>Image compression \u2014 Reduce color palettes by clustering similar colors</li> <li>Gene expression \u2014 Group genes with similar activation patterns</li> </ul>"},{"location":"modules/05-unsupervised/#k-means-algorithm","title":"K-Means Algorithm","text":"<p>The algorithm: 1. Choose K (number of clusters) 2. Randomly initialize K centroids 3. Assign: Each point to nearest centroid 4. Update: Move centroids to mean of assigned points 5. Repeat until centroids stop moving</p> <p>The objective: $\\(\\text{minimize } \\sum_{i=1}^{K}\\sum_{x \\in C_i} ||x - \\mu_i||^2\\)$</p> <p>Where \\(\\mu_i\\) is the centroid of cluster \\(C_i\\). Minimize total distance from points to their centroids.</p> <p>Strengths: - Fast and scalable\u2014works on millions of points - Easy to implement and interpret - Works well with spherical clusters</p> <p>Weaknesses: - Must specify K in advance - Sensitive to initialization - Assumes spherical, similar-sized clusters</p> <p>\"Spherical\" clusters: K-means assigns points to the nearest centroid using Euclidean distance, implicitly assuming clusters are ball-shaped with equal spread in all directions. K-means essentially draws Voronoi cells (straight-line boundaries)\u2014any cluster that can't fit in a convex cell will be problematic. For non-spherical shapes, use DBSCAN (any shape), GMMs (elliptical), or spectral clustering (complex manifolds).</p> <pre><code>from sklearn.cluster import KMeans\n\nkmeans = KMeans(\n    n_clusters=5,\n    init='k-means++',    # Smart initialization\n    n_init=10,           # Run 10 times, keep best\n    random_state=42\n)\n\nlabels = kmeans.fit_predict(X_scaled)\ncentroids = kmeans.cluster_centers_\nprint(f\"Inertia: {kmeans.inertia_}\")\n</code></pre>"},{"location":"modules/05-unsupervised/#choosing-k-elbow-method","title":"Choosing K: Elbow Method","text":"<p>Process: 1. Run K-means for K = 1, 2, 3, ..., n 2. Plot inertia vs K 3. Look for the \"elbow\" where adding clusters gives diminishing returns</p> <pre><code>inertias = []\nK_range = range(1, 11)\n\nfor k in K_range:\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit(X_scaled)\n    inertias.append(kmeans.inertia_)\n\nplt.plot(K_range, inertias, 'bo-')\nplt.xlabel('Number of Clusters (K)')\nplt.ylabel('Inertia')\nplt.title('Elbow Method')\n</code></pre>"},{"location":"modules/05-unsupervised/#choosing-k-silhouette-score","title":"Choosing K: Silhouette Score","text":"<p>For each point, measure how similar it is to its own cluster vs. other clusters:</p> \\[s(i) = \\frac{b(i) - a(i)}{\\max(a(i), b(i))}\\] <p>Where: - \\(a(i)\\) = average distance to points in same cluster - \\(b(i)\\) = average distance to points in nearest other cluster</p> <p>Interpretation: - s = 1: Well-clustered (far from other clusters) - s = 0: On boundary between clusters - s = -1: Probably in wrong cluster</p> <pre><code>from sklearn.metrics import silhouette_score, silhouette_samples\n\n# Overall score\nscore = silhouette_score(X_scaled, labels)\n\n# Per-sample (for diagnostics)\nsample_scores = silhouette_samples(X_scaled, labels)\n</code></pre> <p>Individual scores are useful too: - Find misclassified points (negative scores) - Identify boundary cases (scores near 0) - Detect outliers (very low scores)</p> <p>Business consideration: Sometimes the \"right\" K comes from domain knowledge, not just metrics!</p> <p>When elbow and silhouette disagree: Elbow (inertia) measures compactness; silhouette measures both compactness AND separation. Adding clusters always reduces inertia but may not improve silhouette if new clusters aren't well-separated. If elbow says 5 and silhouette says 3, clusters 4-5 might be subdividing natural groups. Look at both metrics, examine cluster profiles, consider business constraints, and check stability. There's rarely a single \"correct\" K.</p>"},{"location":"modules/05-unsupervised/#dbscan-density-based-clustering","title":"DBSCAN: Density-Based Clustering","text":"<p>K-means assumes spherical clusters. DBSCAN handles: - Irregular shapes - Different densities - Noise/outliers</p> <p>Core concepts: - Core point: Has at least <code>min_samples</code> points within <code>eps</code> distance - Border point: Within <code>eps</code> of a core point, but not core itself - Noise point: Neither (labeled -1)</p> <p>Algorithm: 1. Find all core points 2. Connect core points within <code>eps</code> of each other (transitively) 3. Assign border points to nearest core point's cluster 4. Everything else is noise</p> <p>Connecting core points (step 2) in detail:</p> <p>Two core points belong to the same cluster if they're \"density-reachable\": - Direct: Within <code>eps</code> of each other - Transitive: A connects to B, B connects to C \u2192 A and C same cluster</p> <p>This is graph traversal where core points are nodes and edges exist between points within <code>eps</code>. Each connected component becomes a cluster.</p> <p>Choosing eps and min_samples: For eps, use the k-distance plot\u2014compute each point's distance to its k-th nearest neighbor, sort and plot, look for the elbow. For min_samples, start with dimensions + 1 or 2\u00d7dimensions. Larger min_samples = more conservative. If DBSCAN parameter tuning is frustrating, try HDBSCAN\u2014it removes the eps parameter entirely.</p> <pre><code>from sklearn.cluster import DBSCAN\n\ndbscan = DBSCAN(eps=0.5, min_samples=5)\nlabels = dbscan.fit_predict(X_scaled)\n\nn_clusters = len(set(labels)) - (1 if -1 in labels else 0)\nn_noise = list(labels).count(-1)\nprint(f\"Clusters: {n_clusters}, Noise: {n_noise}\")\n</code></pre>"},{"location":"modules/05-unsupervised/#k-means-vs-dbscan","title":"K-Means vs DBSCAN","text":"Aspect K-Means DBSCAN # Clusters Must specify Auto-detected Shapes Spherical Arbitrary Handles noise No Yes (labels -1) Speed Very fast Slower"},{"location":"modules/05-unsupervised/#hdbscan-hierarchical-dbscan","title":"HDBSCAN: Hierarchical DBSCAN","text":"<p>HDBSCAN addresses DBSCAN's sensitivity to the <code>eps</code> parameter:</p> Aspect DBSCAN HDBSCAN Parameters <code>eps</code> and <code>min_samples</code> Just <code>min_samples</code> Cluster densities Assumes uniform Handles varying <p>How it works: 1. Build a hierarchy considering all possible <code>eps</code> values 2. Find stable clusters that persist across <code>eps</code> range 3. Extract flat clustering from the hierarchy</p> <pre><code>import hdbscan\nclusterer = hdbscan.HDBSCAN(min_cluster_size=15)\nlabels = clusterer.fit_predict(X)\n</code></pre>"},{"location":"modules/05-unsupervised/#hierarchical-clustering","title":"Hierarchical Clustering","text":"<p>Build a tree of nested clusters.</p> <p>Agglomerative (bottom-up): 1. Start: Each point is its own cluster 2. Find two closest clusters 3. Merge them 4. Repeat until one cluster remains</p> <p>Linkage methods (distance between clusters): - Single: Minimum distance between any points - Complete: Maximum distance between any points - Average: Average distance between all pairs - Ward: Minimize variance increase when merging</p> <p>Dendrogram: Tree showing merge history - Y-axis = distance at which clusters merged - Cut at any height to get that many clusters</p> <pre><code>from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n\nZ = linkage(X_scaled, method='ward')\n\nplt.figure(figsize=(12, 6))\ndendrogram(Z)\nplt.xlabel('Sample Index')\nplt.ylabel('Distance')\n\n# Cut to get 3 clusters\nlabels = fcluster(Z, t=3, criterion='maxclust')\n</code></pre>"},{"location":"modules/05-unsupervised/#common-misconceptions","title":"Common Misconceptions","text":"Misconception Reality \"There's one correct number of clusters\" Clustering is exploratory. Multiple valid solutions exist. Business context matters. \"K-means always works\" Fails on complex shapes, varying densities, outliers. \"Silhouette = 0.9 means perfect clustering\" Silhouette measures separation, not business meaning. \"More clusters is always better\" Reduces variance but may not be useful. Aim for interpretable segments."},{"location":"modules/05-unsupervised/#52-dimensionality-reduction","title":"5.2 Dimensionality Reduction","text":""},{"location":"modules/05-unsupervised/#why-reduce-dimensions","title":"Why Reduce Dimensions?","text":"<p>The curse of dimensionality: - High-dimensional spaces are sparse - Distance metrics become less meaningful - Models overfit more easily</p> <p>Benefits: - Visualization: Can't plot 50 dimensions. Can plot 2. - Noise reduction: Remove uninformative dimensions - Faster training: Fewer features - Feature extraction: Create meaningful composites</p> <p>Are we losing important information? You ARE losing information\u2014the question is signal vs. noise. If 10 components capture 95% of variance, the last 40 combined contribute 5% (mostly noise). Verify by comparing model performance with/without reduction. Caveats: rare but important patterns may have low variance; PCA doesn't know your target, so captured variance might not be predictive.</p>"},{"location":"modules/05-unsupervised/#principal-component-analysis-pca","title":"Principal Component Analysis (PCA)","text":"<p>Find new axes (principal components) that: 1. Are linear combinations of original features 2. Capture maximum variance 3. Are orthogonal (uncorrelated)</p> <p>Algorithm: 1. Center the data (subtract mean) 2. Find direction of maximum variance \u2192 PC1 3. Find direction of max remaining variance, perpendicular to PC1 \u2192 PC2 4. Continue...</p> <pre><code>from sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\n# Variance explained\nprint(f\"Variance explained: {pca.explained_variance_ratio_}\")\nprint(f\"Total: {sum(pca.explained_variance_ratio_):.2%}\")\n</code></pre>"},{"location":"modules/05-unsupervised/#choosing-number-of-components","title":"Choosing Number of Components","text":"<ul> <li>Scree plot: Variance explained vs component number</li> <li>Cumulative variance: Keep enough for 80-95%</li> <li>Kaiser criterion: Keep components with eigenvalue &gt; 1</li> </ul> <pre><code>pca_full = PCA()\npca_full.fit(X_scaled)\n\n# Cumulative variance plot\nplt.plot(range(1, len(pca_full.explained_variance_ratio_) + 1),\n         np.cumsum(pca_full.explained_variance_ratio_), 'bo-')\nplt.axhline(y=0.95, color='r', linestyle='--')\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Variance')\n</code></pre>"},{"location":"modules/05-unsupervised/#interpreting-pca-loadings","title":"Interpreting PCA Loadings","text":"<p>Loadings show how original features contribute to each component:</p> Feature PC1 PC2 Income 0.8 0.1 Age 0.7 -0.2 Spending 0.6 0.8 <p>Interpretation: - PC1 loads on Income, Age, Spending \u2192 \"Overall affluence\" - PC2 loads mainly on Spending \u2192 \"Spending tendency\"</p> <p>Naming is subjective: Component naming is interpretation, not discovery. Two analysts might name the same loadings differently (\"Wealth\" vs \"Financial Stability\" vs \"Affluence Score\"). Report actual loadings alongside interpretation, acknowledge subjectivity, and validate with domain experts. If you can't tell a coherent story, the component may not be meaningfully interpretable.</p> <pre><code># Loadings are in components_ (rows = components, cols = features)\nloadings = pca.components_\n\nimport polars as pl\nloadings_df = pl.DataFrame(\n    loadings,\n    schema=feature_names\n).with_row_index(\"PC\")\n</code></pre>"},{"location":"modules/05-unsupervised/#t-sne-for-visualization","title":"t-SNE for Visualization","text":"<p>PCA assumes linear relationships. t-SNE handles non-linear manifolds.</p> <p>Goal: Preserve local neighborhoods in 2D - Points close in high-D stay close - Points far apart can move freely</p> <p>Key parameter: <code>perplexity</code> (~5-50) - Roughly expected number of neighbors - Try multiple values</p> <p>Critical caveats: - Stochastic\u2014different runs give different results - Cluster sizes are meaningless (distances distorted) - Can create false patterns in random data - Slow for large datasets - ONLY for visualization, NOT preprocessing!</p> <pre><code>from sklearn.manifold import TSNE\n\ntsne = TSNE(n_components=2, perplexity=30, random_state=42)\nX_tsne = tsne.fit_transform(X_scaled)\n</code></pre> <p>Never use t-SNE coordinates as features for a classifier. Distances are distorted. Use PCA for preprocessing.</p> <p>Trusting t-SNE clusters: t-SNE preserves local neighborhoods but distorts global distances, cluster sizes, and densities. To avoid being fooled: run multiple times with different seeds/perplexity, validate with clustering on the original high-D data (if K-means finds no structure there, t-SNE may be misleading), and check perplexity sensitivity. t-SNE is for visualization and hypothesis generation\u2014always verify clusters with methods on the original data.</p>"},{"location":"modules/05-unsupervised/#umap","title":"UMAP","text":"<p>UMAP (Uniform Manifold Approximation and Projection) is often better than t-SNE:</p> <ul> <li>Faster, especially for large data</li> <li>Preserves global structure better</li> <li>Can be used for preprocessing (not just visualization)</li> <li>More reproducible</li> </ul> <pre><code>import umap\n\nreducer = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1)\nX_umap = reducer.fit_transform(X_scaled)\n</code></pre>"},{"location":"modules/05-unsupervised/#method-comparison","title":"Method Comparison","text":"Method Speed Global Structure Use For PCA Fast Preserved Preprocessing, visualization t-SNE Slow Lost Visualization only UMAP Medium Partially preserved Both <p>Rule of thumb: - PCA for preprocessing and quick visualization - t-SNE or UMAP for beautiful visualizations - UMAP if you want the best of both worlds</p>"},{"location":"modules/05-unsupervised/#mnist-example","title":"MNIST Example","text":"<ul> <li>Original: 784 dimensions (28\u00d728 pixels)</li> <li>PCA to 2D: Blurry separation</li> <li>t-SNE/UMAP to 2D: Clear digit clusters!</li> </ul> <p>Why? The digit manifold is non-linear. PCA's linear assumption can't capture it. t-SNE and UMAP can.</p>"},{"location":"modules/05-unsupervised/#common-misconceptions_1","title":"Common Misconceptions","text":"Misconception Reality \"PCA finds the most important features\" PCA finds linear combinations. Components may not correspond to individual features. \"t-SNE cluster sizes are meaningful\" t-SNE distorts distances. A big cluster in t-SNE might be same size as small one in reality. \"More components = better\" More preserves more info but may include noise. Choose based on task. \"Dimensionality reduction always helps ML models\" Sometimes original features are better. Compare performance."},{"location":"modules/05-unsupervised/#reflection-questions","title":"Reflection Questions","text":"<ol> <li> <p>You're segmenting customers for marketing. K-means suggests 5 clusters, but your team can only create 3 campaigns. What do you do?</p> </li> <li> <p>Your clustering puts 95% of data in one cluster and creates 4 tiny ones. Is this a problem? What might cause this?</p> </li> <li> <p>When would you choose DBSCAN over K-means? Give a business example.</p> </li> <li> <p>A colleague says they found \"the optimal number of clusters.\" Why should you be skeptical?</p> </li> <li> <p>PCA on customer data shows PC1 explains 80% of variance. Should you only use PC1?</p> </li> <li> <p>You run t-SNE twice and get different-looking plots. Is one wrong?</p> </li> <li> <p>A colleague says \"UMAP proves our data has 5 clusters.\" What's wrong with this statement?</p> </li> </ol>"},{"location":"modules/05-unsupervised/#practice-problems","title":"Practice Problems","text":"<ol> <li> <p>Given cluster assignments, calculate silhouette score by hand for a small example</p> </li> <li> <p>Interpret PCA loadings for a business dataset (name the components)</p> </li> <li> <p>Choose between K-means and DBSCAN for different data scenarios</p> </li> <li> <p>Explain why t-SNE shouldn't be used for preprocessing</p> </li> <li> <p>For customer data with features {Income, Age, Transactions, Days_Since_Purchase}, describe what PC1 and PC2 might represent</p> </li> </ol>"},{"location":"modules/05-unsupervised/#chapter-summary","title":"Chapter Summary","text":"<p>Six key takeaways from Module 5:</p> <ol> <li> <p>Unsupervised learning discovers structure without labels</p> </li> <li> <p>K-means is fast but needs spherical clusters and specified K</p> </li> <li> <p>DBSCAN handles arbitrary shapes and identifies outliers</p> </li> <li> <p>Silhouette scores measure cluster quality (but not business meaning)</p> </li> <li> <p>PCA finds linear combinations that maximize variance</p> </li> <li> <p>t-SNE/UMAP reveal non-linear structure\u2014use t-SNE for visualization only!</p> </li> </ol>"},{"location":"modules/05-unsupervised/#whats-next","title":"What's Next","text":"<p>In Module 6, we tackle Neural Networks Fundamentals: - Perceptrons and multi-layer networks - Activation functions - Backpropagation - Deep learning basics</p> <p>Here's an interesting connection: dimensionality reduction is related to neural network feature learning. Neural networks automatically learn compressed representations of inputs\u2014that's partly why deep learning works so well.</p>"},{"location":"modules/06-neural-networks/","title":"Module 6: Neural Networks Fundamentals","text":""},{"location":"modules/06-neural-networks/#introduction","title":"Introduction","text":"<p>Today we cross a threshold\u2014we're entering deep learning.</p> <p>Everything we've covered so far\u2014regression, classification, ensemble methods, unsupervised learning\u2014those are \"classical\" machine learning. Powerful, interpretable, widely used. But deep learning has transformed what's possible with images, text, audio, and complex patterns.</p> <p>Here's the key insight: neural networks are not magic. They're built on the same principles we've been learning. Remember gradient descent from Module 2? You'll see it again. Remember the bias-variance tradeoff from Module 1? It applies here too.</p> <p>What makes neural networks special is their ability to learn hierarchical representations\u2014layer by layer, from simple patterns to complex concepts.</p> <p>Hierarchical learning is automatic: We design the architecture and loss function; the specific representations are discovered, not designed. Through backpropagation, weights organize themselves to extract useful features. Researchers visualizing trained networks find edges in layer 1, textures in layer 2, object parts in later layers\u2014this emerges from optimization as the most efficient solution.</p>"},{"location":"modules/06-neural-networks/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, you should be able to:</p> <ol> <li>Explain the historical development and architecture of neural networks</li> <li>Describe the components of a neural network (weights, biases, activations)</li> <li>Understand backpropagation and gradient-based optimization</li> <li>Implement a simple neural network in PyTorch</li> <li>Train and evaluate networks on classification tasks</li> <li>Apply regularization techniques to prevent overfitting</li> </ol>"},{"location":"modules/06-neural-networks/#61-introduction-to-neural-networks","title":"6.1 Introduction to Neural Networks","text":""},{"location":"modules/06-neural-networks/#three-components-neural-networks","title":"Three Components: Neural Networks","text":"<p>The same framework applies here:</p> Component Neural Network Decision Model Stacked layers with non-linear activations Quality Measure Cross-entropy (classification) or MSE (regression) Update Method Backpropagation + gradient descent (SGD, Adam) <p>In Module 2, you implemented gradient descent for two parameters (\\(\\beta_0\\), \\(\\beta_1\\)). Neural networks apply the same idea to millions of parameters. The algorithm is the same; the scale is different.</p>"},{"location":"modules/06-neural-networks/#historical-context","title":"Historical Context","text":"<p>1957: Frank Rosenblatt invents the Perceptron\u2014a single layer of weights that could learn simple patterns. The New York Times predicted thinking machines within a decade.</p> <p>1969: Minsky and Papert publish \"Perceptrons,\" proving single-layer networks can't learn XOR. Funding dries up. First \"AI Winter.\"</p> <p>1986: Rumelhart, Hinton, and Williams popularize backpropagation\u2014making deep network training practical.</p> <p>2012: AlexNet wins ImageNet by a massive margin, demonstrating that deep networks trained on GPUs could dramatically outperform traditional methods.</p> <p>Today: Transformers, GPT, and large language models.</p> <p>The lesson: Neural networks have existed for 70 years. What changed is data, compute, and better training techniques.</p> <p>Why deep learning works now: Three factors combined: (1) Data\u2014ImageNet provided 14M labeled images; the internet generated billions of documents. (2) GPUs\u2014parallel operations for matrix multiplication, turning weeks into hours. (3) Better techniques\u2014ReLU solved vanishing gradients, dropout provided regularization, batch norm stabilized training, Adam made optimization robust. AlexNet (2012) combined all three and won ImageNet decisively.</p>"},{"location":"modules/06-neural-networks/#the-xor-problem","title":"The XOR Problem","text":"<p>The XOR function outputs 1 if exactly one input is 1:</p> x\u2081 x\u2082 XOR 0 0 0 0 1 1 1 0 1 1 1 0 <p>A single-layer perceptron can only learn linearly separable patterns. XOR isn't linearly separable\u2014you can't draw a single straight line to separate the 1s from the 0s.</p> <p>The solution: Add a hidden layer. The hidden layer \"transforms\" the space to make the problem linearly separable.</p> <p>How the hidden layer transforms space: Each neuron computes a weighted sum (defining a hyperplane) plus activation (bending space around it). For XOR, one neuron might learn \"x\u2081 + x\u2082 &gt; 0.5\" and another \"x\u2081 + x\u2082 &lt; 1.5\"\u2014together creating a representation where (0,1) and (1,0) map similarly while (0,0) and (1,1) map differently. The output layer can now draw a line in this transformed space.</p>"},{"location":"modules/06-neural-networks/#multi-layer-perceptron-mlp-architecture","title":"Multi-Layer Perceptron (MLP) Architecture","text":"<p>Terminology: - Input layer: Raw features (not counted in \"layers\") - Hidden layers: Intermediate representations - Output layer: Final predictions - Depth: Number of hidden layers - Width: Neurons per layer</p> Network Type Hidden Layers Typical Use Shallow 1-2 Simple patterns Deep 3+ Complex patterns Very Deep 50+ State-of-the-art"},{"location":"modules/06-neural-networks/#why-depth-matters","title":"Why Depth Matters","text":"<p>Each layer learns more abstract features: - Layer 1: Edges, simple patterns - Layer 2: Textures, shapes - Layer 3: Object parts - Layer N: Complete concepts</p> <p>Deep networks learn hierarchical representations that match how complex patterns are actually structured.</p>"},{"location":"modules/06-neural-networks/#universal-approximation-theorem","title":"Universal Approximation Theorem","text":"<p>A feedforward network with a single hidden layer can approximate any continuous function, given enough neurons.</p> <p>What it means: With enough neurons, any reasonable function can be approximated.</p> <p>What it doesn't mean: It doesn't tell you how many neurons you need, how to find the weights, or that one layer is optimal.</p> <p>In practice, deep networks represent the same functions more efficiently than wide shallow ones.</p> <p>Why depth over width? A function that a 10-layer network represents with 1,000 neurons might require millions in a single layer. Complex patterns are compositional (faces = eyes + nose + mouth; eyes = curves + colors)\u2014deep networks represent this hierarchy naturally. Shallow networks must learn all combinations directly, which explodes exponentially. Deeper architectures outperform shallow ones with the same parameter count on complex benchmarks.</p>"},{"location":"modules/06-neural-networks/#network-components","title":"Network Components","text":"<p>1. Weights (W): Learnable parameters connecting neurons 2. Biases (b): Learnable offset per neuron 3. Activation functions: Non-linear transformations</p> <p>The computation at each neuron: $\\(output = activation(Wx + b)\\)$</p>"},{"location":"modules/06-neural-networks/#activation-functions","title":"Activation Functions","text":"<p>ReLU (Rectified Linear Unit) \u2014 most common: $\\(\\text{ReLU}(x) = \\max(0, x)\\)$ - Simple: negative \u2192 0, positive \u2192 pass through - Default choice for hidden layers - Helps with vanishing gradients</p> <p>Sigmoid: $\\(\\sigma(x) = \\frac{1}{1 + e^{-x}}\\)$ - Output between 0 and 1 - Good for binary output layer - Suffers from vanishing gradients in deep networks</p> <p>Softmax (for multi-class): $\\(\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}\\)$ - Outputs sum to 1 (probabilities) - Used in final layer for classification</p>"},{"location":"modules/06-neural-networks/#why-non-linear-activations","title":"Why Non-linear Activations?","text":"<p>Without non-linearity: $\\(Layer_2(Layer_1(x)) = W_2(W_1 x) = (W_2 W_1)x = Wx\\)$</p> <p>Multiple linear layers = one linear layer!</p> <p>No matter how many linear layers you stack, the result is still linear. Non-linear activations allow each layer to transform representations in ways linear functions can't.</p> <p>Why ReLU works: (1) Vanishing gradient solution\u2014sigmoid's gradient approaches zero for large inputs; ReLU has gradient 1 for positives, letting gradients pass through unchanged. (2) Computational efficiency\u2014just max(0,x), orders of magnitude faster than sigmoid. (3) Sparse activation\u201450% of neurons may be \"dead\" for any input, improving efficiency. Despite being piecewise linear, stacking many ReLUs can approximate any continuous function.</p>"},{"location":"modules/06-neural-networks/#parameter-counting","title":"Parameter Counting","text":"<p>For a fully connected layer: $\\(Parameters = (input \\times output) + output = weights + biases\\)$</p> <p>Example: Network with layers [784, 256, 128, 10] - Layer 1: 784\u00d7256 + 256 = 200,960 - Layer 2: 256\u00d7128 + 128 = 32,896 - Layer 3: 128\u00d710 + 10 = 1,290 - Total: 235,146 parameters</p> <pre><code>def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n</code></pre> <p>Is 10 million parameters a lot? It depends on your data. If you have 1,000 examples and 10 million parameters, you'll overfit. If you have 10 million examples, it's reasonable. The ratio matters.</p>"},{"location":"modules/06-neural-networks/#common-misconceptions","title":"Common Misconceptions","text":"Misconception Reality \"Deep learning is different from ML\" Deep learning IS machine learning. Same principles apply. \"More layers always better\" Deeper = harder to train, can overfit. Match depth to complexity. \"Neural networks are black boxes\" Many interpretability tools exist. The criticism is overstated. \"Need millions of data points\" Transfer learning enables NNs with small datasets."},{"location":"modules/06-neural-networks/#62-training-neural-networks","title":"6.2 Training Neural Networks","text":""},{"location":"modules/06-neural-networks/#loss-functions","title":"Loss Functions","text":"<p>Regression \u2014 Mean Squared Error (MSE): $\\(L = \\frac{1}{n}\\sum(y_i - \\hat{y}_i)^2\\)$</p> <p>Binary Classification \u2014 Binary Cross-Entropy: $\\(L = -\\frac{1}{n}\\sum[y\\log(\\hat{y}) + (1-y)\\log(1-\\hat{y})]\\)$</p> <p>Multi-class \u2014 Cross-Entropy: $\\(L = -\\frac{1}{n}\\sum_{i}\\sum_{c} y_{ic}\\log(\\hat{y}_{ic})\\)$</p> <p>Why cross-entropy? The log function severely penalizes confident wrong predictions: - \\(\\log(1) = 0\\) \u2014 no penalty for correct confidence - \\(\\log(0.5) \\approx -0.69\\) \u2014 moderate penalty - \\(\\log(0.01) \\approx -4.6\\) \u2014 severe penalty</p>"},{"location":"modules/06-neural-networks/#backpropagation","title":"Backpropagation","text":"<p>The algorithm that makes deep learning possible.</p> <ol> <li>Forward pass: Compute predictions</li> <li>Compute loss: How wrong are we?</li> <li>Backward pass: Compute gradients using chain rule</li> <li>Update: Adjust weights</li> </ol> <p>The chain rule lets us compute how each weight contributed to error, layer by layer, from output back to input.</p> <p>Why gradient computation is fast: Backpropagation reuses computations\u2014when computing gradients for layer 5, you reuse gradient info from layers 6-10. Total cost is ~2\u00d7 the forward pass, O(n) in weights. GPUs parallelize matrix multiplications across thousands of cores. Processing 64 examples in parallel takes almost the same time as 1. A network with 100M parameters takes seconds per batch on modern GPUs.</p> <p>Key point: PyTorch does this automatically!</p> <pre><code>loss.backward()   # Computes all gradients\noptimizer.step()  # Updates all parameters\n</code></pre> <p>One line computes gradients. One line updates weights.</p>"},{"location":"modules/06-neural-networks/#optimization-algorithms","title":"Optimization Algorithms","text":"<p>SGD (Stochastic Gradient Descent): $\\(W \\leftarrow W - \\alpha \\cdot \\nabla L\\)$ Same as Module 2. Simple but can be slow.</p> <p>SGD + Momentum: $\\(v \\leftarrow \\beta v + \\nabla L\\)$ $\\(W \\leftarrow W - \\alpha \\cdot v\\)$</p> <p>Accumulates velocity in consistent directions. Like a ball rolling downhill.</p> <p>Adam (Adaptive Moment Estimation) \u2014 most popular: - Combines momentum with adaptive learning rates - Per-parameter learning rates - Usually works well with defaults</p> <p>How Adam works (simplified): - Track moving average of gradients (momentum) - Track moving average of squared gradients (adapt rates) - Parameters with large gradients get smaller learning rates</p> <pre><code>optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n</code></pre> <p>Practical advice: Start with Adam. Try SGD with momentum if you have time to tune.</p>"},{"location":"modules/06-neural-networks/#learning-rate","title":"Learning Rate","text":"<p>The most important hyperparameter.</p> Too High Just Right Too Low Loss explodes Steady decrease Very slow Diverges Converges Gets stuck <p>Tips: - Start with 0.001 for Adam, 0.01 for SGD - If loss explodes: divide by 10 - If loss barely moves: multiply by 3-10 - Use schedulers to reduce rate during training</p>"},{"location":"modules/06-neural-networks/#batch-size","title":"Batch Size","text":"Variant Batch Size Trade-off Batch GD All data Stable but slow SGD 1 sample Fast but noisy Mini-batch 32-256 Best of both <p>Standard practice: 32, 64, 128, or 256</p> <p>Trade-offs: - Larger: More stable, more memory, may generalize worse - Smaller: Noisier (regularizing), faster per epoch</p>"},{"location":"modules/06-neural-networks/#regularization-dropout","title":"Regularization: Dropout","text":"<p>Randomly zero neurons during training.</p> <pre><code>self.dropout = nn.Dropout(0.5)  # 50% dropout\n</code></pre> <ul> <li>Forces network to not rely on any single neuron</li> <li>Like training an ensemble of sub-networks</li> <li>Only active during training, not inference</li> </ul> <p>Connection to ensembles: Dropout trains many different sub-networks (different neurons dropped each time) and averages at test time. It's bagging for neural networks.</p> <p>How dropout learning works: Each training example sees a different random subset of neurons. Features that depend on one specific neuron won't work consistently (it might be dropped), forcing distributed, robust representations. At test time, ALL neurons are used but scaled by the dropout rate. The ensemble interpretation: training exponentially many sub-networks simultaneously, averaging at test time.</p>"},{"location":"modules/06-neural-networks/#regularization-batch-normalization","title":"Regularization: Batch Normalization","text":"<p>Normalize activations within each mini-batch.</p> <pre><code>self.bn1 = nn.BatchNorm1d(256)\n</code></pre> <ul> <li>Stabilizes training</li> <li>Allows higher learning rates</li> <li>Add after linear layer, before activation</li> </ul>"},{"location":"modules/06-neural-networks/#regularization-early-stopping","title":"Regularization: Early Stopping","text":"<p>Stop when validation loss stops improving.</p> <pre><code>if val_loss &lt; best_val_loss:\n    best_val_loss = val_loss\n    save_model()\nelse:\n    patience_counter += 1\n    if patience_counter &gt;= patience:\n        stop_training()\n</code></pre> <p>Simple and effective.</p>"},{"location":"modules/06-neural-networks/#diagnosing-overfitting","title":"Diagnosing Overfitting","text":"<p>Signs: - Training loss decreasing - Validation loss increasing - Large gap between train/val accuracy</p> <p>Solutions: - More data - Dropout - Early stopping - Simpler architecture - Data augmentation</p>"},{"location":"modules/06-neural-networks/#common-misconceptions_1","title":"Common Misconceptions","text":"Misconception Reality \"Lower training loss is always better\" If validation loss increases, you're overfitting. \"Dropout makes the network weaker\" Only during training. At test, all neurons active. \"Just use Adam defaults\" Tuning learning rate still helps. \"Train until loss is zero\" Zero training loss usually means severe overfitting."},{"location":"modules/06-neural-networks/#63-pytorch-overview","title":"6.3 PyTorch Overview","text":""},{"location":"modules/06-neural-networks/#why-pytorch","title":"Why PyTorch?","text":"<ul> <li>Dynamic computation graphs (easier debugging)</li> <li>Pythonic and intuitive</li> <li>Strong research community</li> <li>Seamless GPU support</li> <li>Great documentation</li> </ul>"},{"location":"modules/06-neural-networks/#tensors-and-autograd","title":"Tensors and Autograd","text":"<p>Tensors: Like NumPy arrays but with GPU support and automatic differentiation.</p> <pre><code>import torch\n\n# Create tensors\nx = torch.randn(3, 4)  # Random normal\n\n# Move to GPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nx = x.to(device)\n</code></pre> <p>Autograd: Automatic differentiation</p> <pre><code>x = torch.tensor([2.0], requires_grad=True)\ny = x ** 2\ny.backward()\nprint(x.grad)  # dy/dx = 2x = 4 at x=2\n</code></pre>"},{"location":"modules/06-neural-networks/#building-models-with-nnmodule","title":"Building Models with nn.Module","text":"<pre><code>import torch.nn as nn\n\nclass MLP(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super().__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, num_classes)\n        self.dropout = nn.Dropout(0.2)\n\n    def forward(self, x):\n        x = x.view(-1, input_size)  # Flatten\n        x = torch.relu(self.fc1(x))\n        x = self.dropout(x)\n        return self.fc2(x)\n</code></pre> <p>Define layers in <code>__init__</code>, define forward pass in <code>forward</code>.</p>"},{"location":"modules/06-neural-networks/#the-training-loop","title":"The Training Loop","text":"<p>This is the heart of neural network training. Learn this pattern:</p> <pre><code>model = MLP(784, 256, 10).to(device)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\nfor epoch in range(num_epochs):\n    model.train()\n    for data, target in train_loader:\n        data, target = data.to(device), target.to(device)\n\n        optimizer.zero_grad()       # 1. Clear gradients\n        output = model(data)        # 2. Forward pass\n        loss = criterion(output, target)\n        loss.backward()             # 3. Backward pass\n        optimizer.step()            # 4. Update weights\n</code></pre> <p>The pattern: 1. <code>optimizer.zero_grad()</code> \u2014 Clear old gradients 2. <code>output = model(data)</code> \u2014 Forward pass 3. <code>loss.backward()</code> \u2014 Compute gradients 4. <code>optimizer.step()</code> \u2014 Update weights</p>"},{"location":"modules/06-neural-networks/#evaluation-mode","title":"Evaluation Mode","text":"<pre><code>model.eval()  # Disables dropout\n\nwith torch.no_grad():  # No gradient tracking\n    for data, target in test_loader:\n        output = model(data)\n        pred = output.argmax(dim=1)\n</code></pre> <p>Key points: - <code>model.eval()</code> disables dropout (uses all neurons) - <code>torch.no_grad()</code> saves memory</p> <p>Always switch to eval mode for validation and testing!</p>"},{"location":"modules/06-neural-networks/#complete-mnist-example","title":"Complete MNIST Example","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\n# Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Data\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\n\ntrain_data = datasets.MNIST('./data', train=True, download=True, transform=transform)\ntest_data = datasets.MNIST('./data', train=False, transform=transform)\n\ntrain_loader = DataLoader(train_data, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_data, batch_size=1000)\n\n# Model\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(784, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, 10)\n        self.dropout = nn.Dropout(0.2)\n\n    def forward(self, x):\n        x = x.view(-1, 784)\n        x = torch.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = torch.relu(self.fc2(x))\n        return self.fc3(x)\n\nmodel = Net().to(device)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\n# Training\ndef train(epoch):\n    model.train()\n    for data, target in train_loader:\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n\ndef test():\n    model.eval()\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            pred = model(data).argmax(dim=1)\n            correct += pred.eq(target).sum().item()\n    return 100. * correct / len(test_loader.dataset)\n\n# Run\nfor epoch in range(10):\n    train(epoch)\n    print(f'Epoch {epoch}: Test Accuracy: {test():.2f}%')\n</code></pre>"},{"location":"modules/06-neural-networks/#reflection-questions","title":"Reflection Questions","text":"<ol> <li> <p>Why couldn't the original perceptron learn XOR? Draw the XOR data and explain.</p> </li> <li> <p>If a neural network can approximate any function with one hidden layer (Universal Approximation), why do we need deep networks?</p> </li> <li> <p>Why do we need non-linear activation functions? What would happen with only linear activations?</p> </li> <li> <p>A model has 10 million parameters. Is that a lot? What determines if this is appropriate?</p> </li> <li> <p>Your training loss is decreasing but validation loss is increasing. What's happening and how do you fix it?</p> </li> <li> <p>Why might Adam work better than vanilla SGD without tuning?</p> </li> <li> <p>How is dropout similar to ensemble methods like Random Forest?</p> </li> </ol>"},{"location":"modules/06-neural-networks/#practice-problems","title":"Practice Problems","text":"<ol> <li> <p>Calculate parameters for a [784, 512, 256, 128, 10] network</p> </li> <li> <p>Identify overfitting from training curves (given a plot description)</p> </li> <li> <p>Choose appropriate activation for: (a) hidden layers, (b) binary output, (c) multi-class output</p> </li> <li> <p>Debug: \"My training loss keeps increasing.\" Most likely cause?</p> </li> <li> <p>Write the PyTorch training loop pattern from memory</p> </li> </ol>"},{"location":"modules/06-neural-networks/#chapter-summary","title":"Chapter Summary","text":"<p>Six key takeaways from Module 6:</p> <ol> <li> <p>Neural networks = stacked layers + non-linear activations</p> </li> <li> <p>Depth enables learning hierarchical features</p> </li> <li> <p>Backpropagation computes gradients via chain rule</p> </li> <li> <p>Adam is a good default optimizer; learning rate is the key hyperparameter</p> </li> <li> <p>Dropout + early stopping prevent overfitting</p> </li> <li> <p>PyTorch pattern: zero_grad \u2192 forward \u2192 backward \u2192 step</p> </li> </ol>"},{"location":"modules/06-neural-networks/#whats-next","title":"What's Next","text":"<p>In Module 7, we tackle Computer Vision &amp; CNNs: - Convolutional layers for images - Pooling and feature maps - Famous architectures (LeNet, VGG, ResNet) - Transfer learning</p> <p>Same training principles, but specialized for images. Instead of fully connected layers, we'll use convolutional layers that exploit spatial structure.</p>"},{"location":"modules/07-computer-vision/","title":"Module 7: Computer Vision &amp; CNNs","text":""},{"location":"modules/07-computer-vision/#introduction","title":"Introduction","text":"<p>Last module we learned neural network fundamentals\u2014layers, activations, backpropagation, PyTorch. Today we specialize those concepts for images.</p> <p>Images are everywhere in business: quality control in manufacturing, inventory management in retail, medical imaging in healthcare, document processing in finance. Computer vision has transformed all of these industries.</p> <p>But images present unique challenges. A single photo is millions of numbers. Fully connected networks can't scale. And we need spatial awareness\u2014a cat in the corner is still a cat, but its pixels are in completely different positions.</p> <p>Convolutional Neural Networks solve these problems. By the end of today, you'll understand how CNNs work, and critically, you'll know how to leverage transfer learning so you don't have to train from scratch.</p> <p>Transfer learning works broadly: Early CNN layers learn universal visual primitives (edges, textures) that transfer to any domain. Studies show ImageNet transfer helps on X-rays, satellite images, even art classification. Train from scratch only with massive domain data AND truly different image statistics\u2014even then, ImageNet weights as initialization usually help.</p>"},{"location":"modules/07-computer-vision/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, you should be able to:</p> <ol> <li>Explain how images are represented as data (matrices, channels)</li> <li>Describe why fully connected networks are inefficient for images</li> <li>Explain the mechanics of convolutional layers and pooling</li> <li>Implement a CNN in PyTorch for image classification</li> <li>Apply transfer learning using pre-trained models</li> <li>Understand modern CV applications (detection, segmentation, ViT)</li> </ol>"},{"location":"modules/07-computer-vision/#71-working-with-images","title":"7.1 Working with Images","text":""},{"location":"modules/07-computer-vision/#how-images-are-represented","title":"How Images Are Represented","text":"<p>Digital images are matrices of numbers.</p> <p>Grayscale: 2D matrix (Height \u00d7 Width). Each pixel is an intensity from 0 (black) to 255 (white).</p> <p>Color (RGB): 3D tensor (Height \u00d7 Width \u00d7 3). Three channels\u2014Red, Green, Blue\u2014each with its own intensity matrix.</p> <p>Example: A 224\u00d7224 color image - Shape: (224, 224, 3) - Total values: 224 \u00d7 224 \u00d7 3 = 150,528 numbers</p> <p>PyTorch convention: (Batch, Channels, Height, Width)\u2014NCHW format.</p> <pre><code>from PIL import Image\nimport numpy as np\n\nimg = Image.open('photo.jpg')\nimg_array = np.array(img)\nprint(f\"Shape: {img_array.shape}\")  # (Height, Width, Channels)\n</code></pre>"},{"location":"modules/07-computer-vision/#imagenet-the-benchmark-that-changed-everything","title":"ImageNet: The Benchmark That Changed Everything","text":"Year Winner Top-5 Error Significance 2010 Traditional 28.2% Pre-deep learning 2012 AlexNet 16.4% CNN breakthrough 2015 ResNet 3.6% Beat humans (~5%) <p>In 2012, AlexNet\u2014a convolutional neural network\u2014crushed the competition. Error dropped from 28% to 16%. That's not incremental improvement; that's a paradigm shift.</p> <p>By 2015, ResNet beat human performance on ImageNet classification.</p>"},{"location":"modules/07-computer-vision/#why-fully-connected-networks-fail","title":"Why Fully Connected Networks Fail","text":"<p>Problem 1: Too many parameters - 224\u00d7224\u00d73 input with 1000 hidden neurons - = 150 million parameters in first layer alone! - Impossible to train, will overfit immediately</p> <p>Problem 2: No spatial understanding - Fully connected layers treat each pixel independently - A cat in the corner has completely different pixel positions than a cat in the center - The network can't generalize</p> <p>The solution: Convolutional Neural Networks</p> <p>Why position matters: A fully connected network treats each pixel independently\u2014\"pixel 1,000 is orange\" vs. \"pixel 50,000 is orange\" are completely different inputs. To recognize cats anywhere, it would need examples at every possible position (billions of configurations). CNNs solve this with weight sharing: the same filter scans all positions, so learning to detect a cat's eye at one position automatically applies everywhere.</p>"},{"location":"modules/07-computer-vision/#72-convolutional-neural-networks","title":"7.2 Convolutional Neural Networks","text":""},{"location":"modules/07-computer-vision/#the-convolution-operation","title":"The Convolution Operation","text":"<p>Instead of connecting every input to every output, we slide a small filter across the image.</p> <p>The operation: 1. Take a small filter (e.g., 3\u00d73) 2. Slide it across the image 3. At each position, compute dot product of filter and patch 4. Output is a \"feature map\"</p> <p>Key parameters: - Filter size: 3\u00d73 or 5\u00d75 typical - Stride: How many pixels to move (1 or 2) - Padding: Zeros around edges to control output size - Number of filters: Each learns a different feature</p>"},{"location":"modules/07-computer-vision/#multi-channel-convolution","title":"Multi-Channel Convolution","text":"<p>Key insight: A \"3\u00d73 filter\" on an RGB image is actually a 3\u00d73\u00d73 tensor.</p> <p>When we say \"3\u00d73 filter,\" we're describing the spatial dimensions. But the filter must match the depth of the input.</p> <p>For an RGB image with 3 channels: - Filter shape: 3 \u00d7 3 \u00d7 3 = 27 weights (plus 1 bias) - Each channel (R, G, B) has its own 3\u00d73 slice</p> <p>How the computation works:</p> <pre><code>At each spatial position:\n1. Extract the 3\u00d73\u00d73 patch from the input\n2. Multiply element-wise with the 3\u00d73\u00d73 filter (27 multiplications)\n3. Sum ALL 27 products + bias \u2192 ONE output value\n</code></pre> <p>Multiple filters \u2192 Multiple output channels:</p> <p>If we want 64 output channels, we need 64 separate filters, each with shape 3\u00d73\u00d73. Total parameters: 64 \u00d7 (27 + 1) = 1,792.</p> <pre><code>conv = nn.Conv2d(\n    in_channels=3,      # RGB input\n    out_channels=64,    # Number of filters\n    kernel_size=3,      # 3\u00d73 filter\n    stride=1,\n    padding=1\n)\n</code></pre>"},{"location":"modules/07-computer-vision/#what-filters-learn","title":"What Filters Learn","text":"<p>Filters automatically learn features through training:</p> <ul> <li>Early layers: Edges, colors, simple textures</li> <li>Middle layers: Textures, patterns, shapes</li> <li>Deep layers: Object parts, semantic concepts</li> </ul> <p>The first layer might learn vertical edges, horizontal edges, color gradients. The second combines those into textures. The third combines textures into shapes. This is hierarchical feature learning.</p> <p>Hierarchy emerges automatically: You don't design what each layer learns. Early layers only see raw pixels (can only learn edges); deep layers receive processed representations (can combine into complex features). When researchers visualize trained networks, they find edges in layer 1, textures in layers 2-3, object parts in mid-layers\u2014discovered, not programmed.</p>"},{"location":"modules/07-computer-vision/#pooling-layers","title":"Pooling Layers","text":"<p>After convolution, we reduce spatial dimensions with pooling.</p> <p>Max Pooling: Take maximum value in each patch - Reduces spatial dimensions (224 \u2192 112 \u2192 56...) - Adds translation invariance\u2014slight shifts don't change output - Keeps strongest activations</p> <pre><code>pool = nn.MaxPool2d(kernel_size=2, stride=2)\n# 224\u00d7224 \u2192 112\u00d7112\n</code></pre> <p>A 2\u00d72 max pool with stride 2 halves each dimension.</p>"},{"location":"modules/07-computer-vision/#classic-cnn-pattern","title":"Classic CNN Pattern","text":"<pre><code>class SimpleCNN(nn.Module):\n    def __init__(self, num_classes=10):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n        self.fc2 = nn.Linear(512, num_classes)\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x):\n        x = self.pool(torch.relu(self.conv1(x)))\n        x = self.pool(torch.relu(self.conv2(x)))\n        x = self.pool(torch.relu(self.conv3(x)))\n        x = x.view(x.size(0), -1)  # Flatten\n        x = torch.relu(self.fc1(x))\n        x = self.dropout(x)\n        return self.fc2(x)\n</code></pre>"},{"location":"modules/07-computer-vision/#parameter-efficiency","title":"Parameter Efficiency","text":"<p>For 32\u00d732 RGB image, 64 outputs:</p> Layer Type Parameters Fully Connected 196,672 Conv2d (3\u00d73) 1,792 <p>~100x fewer parameters!</p> <p>Why? 1. Local connectivity: Each neuron connects only to a small patch 2. Weight sharing: Same filter applied everywhere</p>"},{"location":"modules/07-computer-vision/#historical-architectures","title":"Historical Architectures","text":"<p>AlexNet (2012): 8 layers, ReLU, dropout, GPU training. The breakthrough.</p> <p>VGG (2014): 16-19 layers, all 3\u00d73 convolutions. Showed depth matters.</p> <p>ResNet (2015): Skip connections enabling 150+ layers.</p>"},{"location":"modules/07-computer-vision/#skip-residual-connections","title":"Skip (Residual) Connections","text":"<p>The problem: Very deep networks suffer from vanishing gradients.</p> <p>The solution: Add the input directly to the output.</p> \\[Output = F(x) + x\\] <pre><code>class ResidualBlock(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)\n        self.bn1 = nn.BatchNorm2d(channels)\n        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)\n        self.bn2 = nn.BatchNorm2d(channels)\n\n    def forward(self, x):\n        residual = x\n        out = torch.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += residual  # Skip connection\n        return torch.relu(out)\n</code></pre> <p>If the network can't improve on the input, it can at least pass it through unchanged. This creates direct paths for gradients and enables training 100+ layer networks.</p> <p>Skip connection trade-offs: Memory overhead (must store earlier activations) and architectural constraints (dimensions must match, may need 1\u00d71 convolutions). In shallow networks (3-5 layers), minimal benefit\u2014skip connections solve a deep network problem. For networks &gt;10 layers, skip connections almost always help and are now considered essential in modern architectures.</p>"},{"location":"modules/07-computer-vision/#common-misconceptions","title":"Common Misconceptions","text":"Misconception Reality \"CNNs only work for images\" CNNs work on any grid data: audio, time series, etc. \"Deeper is always better\" Without skip connections, very deep nets fail. Architecture matters. \"You need to design CNNs from scratch\" Transfer learning is usually better."},{"location":"modules/07-computer-vision/#73-transfer-learning","title":"7.3 Transfer Learning","text":""},{"location":"modules/07-computer-vision/#the-core-idea","title":"The Core Idea","text":"<p>Pre-trained ImageNet models learned general visual features: edges, textures, shapes, patterns. These features are useful for almost any image task!</p> <p>Two approaches: 1. Feature extraction: Freeze pre-trained layers, train only new classifier 2. Fine-tuning: Train all layers, but with lower learning rate for pre-trained layers</p> <p>This is how most real-world computer vision is done. You rarely train from scratch anymore.</p>"},{"location":"modules/07-computer-vision/#feature-extraction","title":"Feature Extraction","text":"<p>Freeze pre-trained layers, train only new classifier.</p> <pre><code>import torchvision.models as models\n\nmodel = models.resnet50(pretrained=True)\n\n# Freeze all layers\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Replace final classifier\nmodel.fc = nn.Linear(model.fc.in_features, num_classes)\n\n# Only train new classifier\noptimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n</code></pre> <p>The pre-trained ResNet extracts features. You just train a simple classifier on top.</p>"},{"location":"modules/07-computer-vision/#fine-tuning","title":"Fine-Tuning","text":"<p>Train pre-trained layers with lower learning rate.</p> <pre><code>model = models.resnet50(pretrained=True)\nmodel.fc = nn.Linear(model.fc.in_features, num_classes)\n\n# Different learning rates\noptimizer = optim.Adam([\n    {'params': model.layer4.parameters(), 'lr': 1e-4},\n    {'params': model.fc.parameters(), 'lr': 1e-3}\n])\n</code></pre> <p>Pre-trained layers get smaller learning rate (they're already good). New layers get larger learning rate.</p>"},{"location":"modules/07-computer-vision/#when-to-use-which","title":"When to Use Which","text":"Dataset Size Similarity to ImageNet Approach Small High Feature extraction Small Low Light fine-tuning Large High Fine-tuning Large Low Train from scratch <p>Example: You have 500 X-ray images. Train from scratch or transfer learning?</p> <p>Transfer learning! 500 images isn't enough to train from scratch. Even though X-rays look different from ImageNet photos, early-layer features (edges, textures) are still useful.</p> <p>How similar is \"similar enough\"? There's no bright line\u2014empirically test: train a classifier on frozen pre-trained features vs. random features. If pre-trained beats random, transfer helps. Even domains that seem \"completely different\" (medical imaging, industrial defects) usually benefit. Start with transfer learning, try fine-tuning if unsatisfactory, consider training from scratch only with millions of examples AND truly foreign image statistics.</p>"},{"location":"modules/07-computer-vision/#business-value-of-transfer-learning","title":"Business Value of Transfer Learning","text":"<ul> <li>Cost savings: Days of training \u2192 hours</li> <li>Data efficiency: Good results with hundreds of images (not millions)</li> <li>Time to deployment: Quick proof-of-concept</li> <li>No massive compute: Fine-tuning on a laptop is possible</li> </ul>"},{"location":"modules/07-computer-vision/#74-modern-vision-applications","title":"7.4 Modern Vision Applications","text":""},{"location":"modules/07-computer-vision/#object-detection","title":"Object Detection","text":"<p>Task: Find objects AND their locations (bounding boxes)</p> <p>Not just \"there's a dog\" but \"there's a dog at coordinates (x, y, w, h).\"</p> <p>Key architectures: - YOLO: Fast, single-pass detection (\"You Only Look Once\") - Faster R-CNN: Two-stage, more accurate but slower</p> <p>Applications: Autonomous vehicles, security cameras, retail inventory</p>"},{"location":"modules/07-computer-vision/#image-segmentation","title":"Image Segmentation","text":"<p>Semantic segmentation: Label every pixel with a class (road, car, person)</p> <p>Instance segmentation: Separate individual objects (this car vs that car)</p> <p>Key architecture: U-Net\u2014encoder-decoder with skip connections</p> <p>Applications: Medical imaging, autonomous driving, photo editing</p>"},{"location":"modules/07-computer-vision/#vision-transformers-vit","title":"Vision Transformers (ViT)","text":"<p>The latest revolution: apply transformer architecture to images.</p> <p>How it works: 1. Split image into 16\u00d716 patches 2. Flatten patches into sequences 3. Apply transformer encoder (same architecture as NLP!)</p> <p>Why it matters: - State-of-the-art on many benchmarks - Unified architecture for vision AND language - Enables CLIP, DALL-E, multimodal AI</p>"},{"location":"modules/07-computer-vision/#business-applications","title":"Business Applications","text":"Industry Application Retail Inventory monitoring, checkout-free stores Manufacturing Defect detection, quality control Healthcare Radiology, pathology analysis Agriculture Crop monitoring, disease detection"},{"location":"modules/07-computer-vision/#reflection-questions","title":"Reflection Questions","text":"<ol> <li> <p>An image is 1000\u00d71000 pixels RGB. How many input features? Why is this problematic for fully connected networks?</p> </li> <li> <p>If you shift a cat 10 pixels to the right, how would a fully connected network's perception change vs. a CNN?</p> </li> <li> <p>A 3\u00d73 conv filter has 9 weights per channel. How does this compare to fully connected for the same output?</p> </li> <li> <p>After 3 max pooling layers of 2\u00d72, what happens to a 224\u00d7224 image?</p> </li> <li> <p>How do skip connections help train very deep networks?</p> </li> <li> <p>You have 500 X-ray images. Train from scratch or transfer learning? Why?</p> </li> <li> <p>Why fine-tune later layers before earlier layers?</p> </li> </ol>"},{"location":"modules/07-computer-vision/#practice-problems","title":"Practice Problems","text":"<ol> <li> <p>Calculate output size: 64\u00d764 input, 3\u00d73 kernel, stride=1, padding=0</p> </li> <li> <p>Calculate parameters: Conv2d with in_channels=32, out_channels=64, kernel_size=3</p> </li> <li> <p>Design a CNN for 28\u00d728 grayscale images (MNIST) with 3 conv layers</p> </li> <li> <p>Set up transfer learning code for a 5-class classification problem using ResNet18</p> </li> <li> <p>Explain why a 7\u00d77 filter might be replaced by two 3\u00d73 filters</p> </li> </ol>"},{"location":"modules/07-computer-vision/#chapter-summary","title":"Chapter Summary","text":"<p>Six key takeaways from Module 7:</p> <ol> <li> <p>Images are high-dimensional; FC networks don't scale</p> </li> <li> <p>CNNs use local filters with weight sharing (100x fewer parameters)</p> </li> <li> <p>Pooling reduces dimensions and adds translation invariance</p> </li> <li> <p>Skip connections enable training very deep networks</p> </li> <li> <p>Transfer learning is usually better than training from scratch</p> </li> <li> <p>Modern CV: detection, segmentation, Vision Transformers</p> </li> </ol>"},{"location":"modules/07-computer-vision/#whats-next","title":"What's Next","text":"<p>In Module 8, we tackle Natural Language Processing: - Text as sequences - Word embeddings - Transformers and attention - Pre-trained language models</p> <p>Vision Transformers connect both domains\u2014the same architecture that powers GPT and BERT can also process images!</p>"},{"location":"modules/08-nlp/","title":"Module 8: Natural Language Processing","text":""},{"location":"modules/08-nlp/#introduction","title":"Introduction","text":"<p>Today we tackle natural language processing\u2014teaching machines to understand and generate text.</p> <p>Text is everywhere in business: customer reviews, support tickets, emails, social media, contracts, reports. Being able to automatically classify, extract information from, and generate text is incredibly valuable.</p> <p>In Module 7, we saw how CNNs revolutionized image processing. Today, we'll see how transformers revolutionized NLP. The transformer architecture\u2014introduced in 2017\u2014is the foundation for BERT, GPT, and essentially every language model you've heard of.</p> <p>By the end of this module, you'll understand how text becomes numbers, why transformers work so well, and how to leverage pre-trained models for your own applications.</p> <p>What is machine \"understanding\"? Machines don't understand text like humans\u2014they operate on statistical representations where similar meanings cluster together. What we call \"understanding\" is sophisticated pattern matching: a model that predicts masked words correctly has learned syntax, semantics, and world knowledge encoded as neural network weights. Whether this constitutes \"understanding\" or merely simulates it remains philosophically contested.</p>"},{"location":"modules/08-nlp/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, you should be able to:</p> <ol> <li>Explain different text representation methods (BoW, TF-IDF, embeddings)</li> <li>Understand why word order and context matter in NLP</li> <li>Describe RNN architecture and the vanishing gradient problem</li> <li>Explain the transformer architecture and self-attention mechanism</li> <li>Apply pre-trained language models (BERT, GPT) for NLP tasks</li> <li>Identify appropriate NLP approaches for business problems</li> </ol>"},{"location":"modules/08-nlp/#81-text-representation","title":"8.1 Text Representation","text":""},{"location":"modules/08-nlp/#the-challenge-of-text","title":"The Challenge of Text","text":"<p>Text is fundamentally different from tabular data: - Variable length: Sentences can be 5 words or 500 - Order matters: \"Dog bites man\" \u2260 \"Man bites dog\" - Same word, different meanings: \"bank\" (river) vs \"bank\" (financial) - Vast vocabulary: Hundreds of thousands of words</p> <p>Goal: Convert text to numerical vectors that capture meaning.</p>"},{"location":"modules/08-nlp/#bag-of-words-bow","title":"Bag of Words (BoW)","text":"<p>The simplest approach: count word occurrences.</p> Document \"love\" \"machine\" \"learning\" \"I love machine learning\" 1 1 1 \"Machine learning is great\" 0 1 1 <p>Each document becomes a vector of word counts.</p> <pre><code>from sklearn.feature_extraction.text import CountVectorizer\n\ncorpus = [\n    \"I love machine learning\",\n    \"Machine learning is great\",\n    \"I love deep learning\"\n]\n\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names_out())\n# ['deep', 'great', 'is', 'learning', 'love', 'machine']\n</code></pre> <p>Limitations: - Ignores word order: \"dog bites man\" = \"man bites dog\" - Sparse and high-dimensional - No semantic similarity: \"good\" and \"great\" are unrelated</p>"},{"location":"modules/08-nlp/#tf-idf","title":"TF-IDF","text":"<p>Improvement: Weight words by importance.</p> \\[\\text{TF-IDF} = \\text{TF}(t,d) \\times \\log\\frac{N}{\\text{DF}(t)}\\] <ul> <li>TF (Term Frequency): How often the word appears in this document</li> <li>IDF (Inverse Document Frequency): How rare the word is across all documents</li> </ul> <p>Common words like \"the\" and \"is\" \u2192 low weight Distinctive words \u2192 high weight</p> <p>Why the log in IDF?</p> <ol> <li> <p>Dampening effect: Without log, a word appearing in 1 vs 1,000 documents would have a 1,000x difference. Log compresses this to about 3x.</p> </li> <li> <p>Prevents domination: Extremely rare words would otherwise overwhelm everything else.</p> </li> </ol> <p>Think of it: the difference between appearing in 1 vs 10 documents is more meaningful than 10,000 vs 10,010. The log captures this diminishing-returns intuition.</p> <pre><code>from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf = TfidfVectorizer()\nX = tfidf.fit_transform(corpus)\n</code></pre>"},{"location":"modules/08-nlp/#word-embeddings","title":"Word Embeddings","text":"<p>The breakthrough: Learn dense vectors where similar words are close.</p> <p>Word2Vec (2013): Train a neural network on word prediction. - Skip-gram: Given a word, predict its context words - CBOW: Given context words, predict the target word - Result: 100-300 dimensional vectors per word</p> <p>The key insight: The embedding layer weights ARE the word vectors. Words appearing in similar contexts get similar embeddings.</p> <p>Famous example: $\\(king - man + woman \\approx queen\\)$</p> <pre><code>from gensim.models import Word2Vec\n\nmodel = Word2Vec(sentences, vector_size=100, window=5, min_count=1)\nmodel.wv.most_similar(positive=['king', 'woman'], negative=['man'])\n</code></pre> <p>This works because the embedding captures semantic relationships! \"King\" and \"queen\" differ in the same way that \"man\" and \"woman\" differ.</p> <p>How Word2Vec learns relationships: Word2Vec never sees labeled examples of gender or royalty\u2014these emerge from the distributional hypothesis (words in similar contexts have similar meanings). The model sees \"king\" near \"throne,\" \"crown,\" \"ruled\"; so does \"queen.\" To minimize prediction error, the embedding must encode that \"king \u2192 queen\" is the same direction as \"man \u2192 woman.\" This emergent structure falls out naturally from simple prediction tasks on large corpora.</p>"},{"location":"modules/08-nlp/#why-context-matters","title":"Why Context Matters","text":"<p>Word embeddings are powerful, but they miss context:</p> <p>Word order: - \"Nick ate the pizza\" vs \"The pizza ate Nick\" - Same words, completely different meaning</p> <p>Negation: - \"The movie was good\" vs \"The movie was not good\" - BoW and simple embeddings can't distinguish these</p> <p>Reference: - \"The dog didn't cross the road because it was tired\" - \"The dog didn't cross the road because it was wide\" - What does \"it\" refer to? Depends on context!</p> <p>Key insight: We need models that understand sequences and context.</p>"},{"location":"modules/08-nlp/#common-misconceptions","title":"Common Misconceptions","text":"Misconception Reality \"Word embeddings understand meaning\" Embeddings capture statistical patterns, not true understanding \"Pre-trained embeddings work for any domain\" Domain-specific training often helps (medical, legal) \"More dimensions = better embeddings\" Diminishing returns; 100-300 usually sufficient"},{"location":"modules/08-nlp/#82-recurrent-neural-networks","title":"8.2 Recurrent Neural Networks","text":""},{"location":"modules/08-nlp/#rnn-architecture","title":"RNN Architecture","text":"<p>Problem: Standard neural networks can't handle variable-length sequences or remember previous inputs.</p> <p>Solution: Process sequences one element at a time, maintaining memory.</p> \\[h_t = \\tanh(W_{xh}x_t + W_{hh}h_{t-1} + b)\\] <p></p> <p>The hidden state \\(h\\) carries information through time.</p> <p>Why tanh? 1. Output range [-1, 1]: Can represent \"opposite\" concepts 2. Zero-centered: Helps gradients flow in both directions 3. Stronger gradients: Maximum gradient is 1 (vs 0.25 for sigmoid) 4. Bounded: Prevents hidden states from exploding</p>"},{"location":"modules/08-nlp/#the-vanishing-gradient-problem","title":"The Vanishing Gradient Problem","text":"<p>The challenge: Gradients shrink exponentially through timesteps.</p> <p>If you're processing a 100-word sentence, gradients from word 100 need to flow back to word 1. But multiplied through 100 steps, they become tiny.</p> <p>Result: The RNN \"forgets\" early parts of long sequences.</p>"},{"location":"modules/08-nlp/#lstm-long-short-term-memory","title":"LSTM: Long Short-Term Memory","text":"<p>Solution: Gated architecture with explicit memory.</p> <p>Three gates: 1. Forget gate: What to remove from memory 2. Input gate: What new information to add 3. Output gate: What to output</p> <p>Cell state: A highway for information to flow unchanged through time.</p> <p>The gates learn when to keep information and when to forget it.</p> <pre><code>lstm = nn.LSTM(\n    input_size=100,\n    hidden_size=256,\n    num_layers=2,\n    batch_first=True,\n    bidirectional=True\n)\n</code></pre> <p>Connection to attention: LSTM gates pioneered the idea of selective information access. Attention generalizes this\u2014instead of a single memory cell, attention lets the model look back at any previous position.</p>"},{"location":"modules/08-nlp/#gru-gated-recurrent-unit","title":"GRU: Gated Recurrent Unit","text":"<p>Simplified LSTM with fewer parameters.</p> <p>Two gates: 1. Reset gate: How much past to forget 2. Update gate: How much to update the hidden state</p> <p>Often performs similarly to LSTM but trains faster.</p>"},{"location":"modules/08-nlp/#rnn-limitations","title":"RNN Limitations","text":"<ol> <li>Sequential processing: Can't parallelize\u2014each step depends on the previous</li> <li>Long-range dependencies: Still struggle with very long sequences</li> <li>Fixed representation: A single hidden vector must capture everything</li> </ol> <p>These limitations motivated transformers.</p> <p>Why RNNs dominated before transformers: They were the best available option. Before RNNs: n-gram models (limited context, exponential parameters) and HMMs (restrictive assumptions). LSTMs/GRUs mitigated vanishing gradients; attention mechanisms (2014-2015) addressed the fixed-representation bottleneck. The 2017 transformer paper showed attention alone was sufficient, but required significant innovations (positional encoding, Q/K/V formulation) plus computational resources. Progress looks obvious in retrospect.</p>"},{"location":"modules/08-nlp/#83-transformers","title":"8.3 Transformers","text":""},{"location":"modules/08-nlp/#attention-is-all-you-need-2017","title":"\"Attention Is All You Need\" (2017)","text":"<p>This paper changed everything.</p> <p>The key insight: Replace recurrence with attention.</p> <p>Benefits: - Parallel processing: Process all tokens simultaneously - Direct connections: Any position can attend to any other - Better long-range dependencies: No vanishing gradient through 100 steps</p>"},{"location":"modules/08-nlp/#self-attention","title":"Self-Attention","text":"<p>Core idea: Each word looks at all other words to understand context.</p> <p>Query, Key, Value: - Query (Q): What am I looking for? - Key (K): What do I contain? - Value (V): What information do I provide?</p> \\[\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\\] <p>Intuition: 1. Compute similarity between query and all keys 2. Normalize with softmax \u2192 attention weights 3. Weighted sum of values</p> <p>Example: \"The cat sat on the mat because it was tired\"</p> <p>When processing \"it\": - Compute similarity with all words - \"it\" should attend most strongly to \"cat\" - Copy information from \"cat\" to understand what \"it\" refers to</p> <p>How attention learns coreference: Entirely through training\u2014nothing programmed in. \"It was tired\" makes sense if \"it\" attends to \"cat\" (animals get tired), not \"mat.\" The Q/K/V projection matrices adjust so \"it\" and \"cat\" have high dot product. Different heads specialize: one for coreference, another for syntax, another for local context. The model discovers these patterns; engineers didn't program them.</p>"},{"location":"modules/08-nlp/#why-scale-by-d_k","title":"Why Scale by \u221ad_k?","text":"<p>Dot products grow with dimension. If d_k is large, dot products can be very large, pushing softmax into saturation (all attention on one token). Scaling keeps variance roughly constant.</p>"},{"location":"modules/08-nlp/#multi-head-attention","title":"Multi-Head Attention","text":"<p>Why multiple heads? Different heads can attend to different things.</p> <ul> <li>One head might focus on syntax (subject-verb agreement)</li> <li>Another might focus on semantics (what \"it\" refers to)</li> <li>Another might focus on nearby context</li> </ul> <pre><code>multihead_attn = nn.MultiheadAttention(\n    embed_dim=512,\n    num_heads=8\n)\n</code></pre> <p>Eight heads, each with 64 dimensions, capturing different relationships.</p>"},{"location":"modules/08-nlp/#positional-encoding","title":"Positional Encoding","text":"<p>Problem: Attention is permutation-invariant. It doesn't know word order!</p> <p>Solution: Add position information to embeddings.</p> \\[PE_{pos,2i} = \\sin(pos / 10000^{2i/d})$$ $$PE_{pos,2i+1} = \\cos(pos / 10000^{2i/d})\\] <p>Where: - \\(pos\\) = position in sequence (0, 1, 2, ...) - \\(d\\) = embedding dimension - \\(i\\) = dimension index, ranging from 0 to \\(d/2 - 1\\)</p> <p>Different frequencies let the model learn to attend to relative positions.</p>"},{"location":"modules/08-nlp/#encoder-vs-decoder","title":"Encoder vs Decoder","text":"<p>Encoder (BERT-style): - Processes entire sequence at once - Bidirectional context (see past and future) - Good for understanding and classification</p> <p>Decoder (GPT-style): - Generates sequence left-to-right - Causal masking (can only see past) - Good for text generation</p> <p>Encoder-Decoder (T5): - Encoder processes input - Decoder generates output - Good for translation, summarization</p>"},{"location":"modules/08-nlp/#common-misconceptions_1","title":"Common Misconceptions","text":"Misconception Reality \"Transformers understand language\" They learn statistical patterns, not true understanding \"Attention = interpretability\" Attention weights don't always align with human intuition \"Bigger models are always better\" Diminishing returns; efficiency matters"},{"location":"modules/08-nlp/#84-foundation-models","title":"8.4 Foundation Models","text":""},{"location":"modules/08-nlp/#pre-training-fine-tuning","title":"Pre-training \u2192 Fine-tuning","text":"<p>Pre-training: Train on massive text (expensive!) - Billions of words - Millions of dollars in compute - Done once by big labs</p> <p>Fine-tuning: Adapt to your task (cheap!) - Your data + pre-trained model - Hours, not weeks</p> <p>Zero-shot: Use directly with prompts - No training needed - Just ask the model</p>"},{"location":"modules/08-nlp/#bert","title":"BERT","text":"<p>Bidirectional Encoder Representations from Transformers</p> <p>Pre-training: - Masked Language Modeling: Predict masked words from context - Next Sentence Prediction: Does sentence B follow sentence A?</p> <p>Use cases: - Text classification - Named entity recognition - Question answering - Semantic similarity</p> <pre><code>from transformers import BertTokenizer, BertForSequenceClassification\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertForSequenceClassification.from_pretrained(\n    'bert-base-uncased',\n    num_labels=2\n)\n\ninputs = tokenizer(\n    \"This movie was great!\",\n    return_tensors=\"pt\",\n    padding=True,\n    truncation=True\n)\noutputs = model(**inputs)\n</code></pre>"},{"location":"modules/08-nlp/#gpt-family","title":"GPT Family","text":"<p>Generative Pre-trained Transformer</p> <p>Architecture: Decoder-only (autoregressive)</p> <p>Capabilities: - Text generation - Zero/few-shot learning - Instruction following (ChatGPT)</p> <p>Scale evolution: - GPT (2018): 117M parameters - GPT-2 (2019): 1.5B parameters - GPT-3 (2020): 175B parameters - GPT-4 (2023): Multimodal, even larger</p>"},{"location":"modules/08-nlp/#bert-vs-gpt","title":"BERT vs GPT","text":"Aspect BERT GPT Architecture Encoder Decoder Context Bidirectional Left-to-right Best for Understanding Generation Training Masked LM Next token prediction <p>When to use which?</p> <p>Use BERT for classification, NER, and understanding tasks\u2014especially with labeled training data.</p> <p>Use GPT for generation tasks, or when you want to leverage prompting without training data.</p> <p>Why fine-tune BERT vs. zero-shot GPT? (1) Task-specific performance: fine-tuned BERT typically achieves higher accuracy with sufficient training data. (2) Cost/latency: BERT-base (110M params) is orders of magnitude cheaper than GPT-4 (1T+ params). (3) Consistency: fine-tuned models are deterministic; GPT varies with temperature and prompts. (4) Domain adaptation and data privacy (local training vs. API calls). Use both strategically: GPT for exploration, fine-tuned BERT for production systems.</p>"},{"location":"modules/08-nlp/#business-applications","title":"Business Applications","text":"Application Model Example Sentiment Analysis BERT Product reviews Chatbot GPT Customer support Classification BERT Email routing Named Entity Recognition BERT Extract entities Text Generation GPT Marketing copy Summarization T5, BART Meeting notes"},{"location":"modules/08-nlp/#85-beyond-text","title":"8.5 Beyond Text","text":""},{"location":"modules/08-nlp/#vision-transformers-vit","title":"Vision Transformers (ViT)","text":"<ul> <li>Split images into patches</li> <li>Treat patches as \"tokens\"</li> <li>Apply transformer encoder</li> <li>State-of-the-art on many vision benchmarks</li> </ul>"},{"location":"modules/08-nlp/#audio-processing","title":"Audio Processing","text":"<ul> <li>Whisper: Speech recognition</li> <li>wav2vec: Audio embeddings</li> </ul>"},{"location":"modules/08-nlp/#multimodal-models","title":"Multimodal Models","text":"<ul> <li>CLIP: Connect images and text</li> <li>DALL-E: Generate images from text</li> <li>GPT-4V: Vision + language</li> </ul> <p>Key insight: Transformer architecture is general-purpose, not just for text.</p>"},{"location":"modules/08-nlp/#reflection-questions","title":"Reflection Questions","text":"<ol> <li> <p>Why does 'king - man + woman \u2248 queen' work with word embeddings?</p> </li> <li> <p>A BoW model can't tell 'dog bites man' from 'man bites dog'. Why not? What's needed to fix this?</p> </li> <li> <p>You're building a document search engine. Would you use BoW, TF-IDF, or embeddings? Why?</p> </li> <li> <p>Why can't a standard feedforward network process variable-length text?</p> </li> <li> <p>An LSTM processes a 100-word sentence. How does information from word 1 reach the output?</p> </li> <li> <p>Why is self-attention more parallelizable than RNNs?</p> </li> <li> <p>In \"The animal didn't cross the road because it was tired\", what should 'it' attend to?</p> </li> <li> <p>Why does BERT use bidirectional attention while GPT uses causal attention?</p> </li> <li> <p>When would you fine-tune BERT vs use GPT with prompting?</p> </li> </ol>"},{"location":"modules/08-nlp/#practice-problems","title":"Practice Problems","text":"<ol> <li> <p>For a vocabulary of 10,000 words and a 5-word document, what's the dimensionality of BoW vs a 300-dim embedding?</p> </li> <li> <p>Calculate TF-IDF for a word appearing 3 times in a document, when it appears in 100 of 10,000 documents.</p> </li> <li> <p>Explain why RNNs suffer from vanishing gradients but LSTMs partially solve this.</p> </li> <li> <p>Given Q, K, V matrices, trace through the self-attention computation.</p> </li> <li> <p>A company wants to classify support tickets. Recommend BERT vs GPT and justify.</p> </li> </ol>"},{"location":"modules/08-nlp/#chapter-summary","title":"Chapter Summary","text":"<p>Six key takeaways from Module 8:</p> <ol> <li> <p>Text representation evolves: BoW \u2192 TF-IDF \u2192 embeddings \u2192 contextual embeddings</p> </li> <li> <p>RNNs process sequences but struggle with long-range dependencies</p> </li> <li> <p>Transformers use attention for parallel, effective processing</p> </li> <li> <p>Self-attention lets each token consider all others</p> </li> <li> <p>BERT for understanding, GPT for generation</p> </li> <li> <p>Transfer learning makes NLP accessible</p> </li> </ol>"},{"location":"modules/08-nlp/#whats-next","title":"What's Next","text":"<p>In Module 9, we tackle Model Interpretability: - Why do models make decisions? - SHAP values - Attention visualization - Building trust in ML systems</p> <p>We'll use attention from transformers to understand what NLP models focus on!</p>"},{"location":"modules/09-interpretability/","title":"Module 9: Model Interpretability &amp; Explainability","text":""},{"location":"modules/09-interpretability/#introduction","title":"Introduction","text":"<p>We've covered a wide range of modeling techniques: linear models, decision trees, random forests, XGBoost, neural networks, CNNs, and transformers. Some are simple\u2014you can look at coefficients. Others are complex\u2014millions of parameters that no human can comprehend directly.</p> <p>Here's the challenge: A model that can't be explained often can't be deployed.</p> <p>Think about it. A bank denies someone a loan. A hospital's AI recommends a treatment. An insurance company sets a premium. In all these cases, people deserve to know why. And in many cases, the law requires it.</p> <p>This module bridges the gap between model performance and real-world deployment. You'll learn how to explain any model\u2014black box or not\u2014and how to communicate those explanations to stakeholders who don't know (or care) about gradient descent.</p> <p>Interpretability vs. performance: Modern tools largely eliminate this tradeoff. Train a complex model for maximum performance, then use SHAP/LIME to explain it\u2014you get both accuracy and explanations. Intrinsically interpretable models (linear regression, short decision trees) provide explanations directly if regulations require them. A well-regularized linear model can often match tree ensemble performance anyway.</p>"},{"location":"modules/09-interpretability/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, you should be able to:</p> <ol> <li>Explain why model interpretability matters for business and regulatory compliance</li> <li>Distinguish between global and local interpretability</li> <li>Apply SHAP and LIME to explain model predictions</li> <li>Create effective visualizations of model behavior</li> <li>Communicate model insights to non-technical stakeholders</li> <li>Document models with model cards and limitations</li> </ol>"},{"location":"modules/09-interpretability/#91-why-interpretability-matters","title":"9.1 Why Interpretability Matters","text":""},{"location":"modules/09-interpretability/#the-business-case","title":"The Business Case","text":"<p>The best model in the world is worthless if no one trusts it.</p> <p>You could build a fraud detection system with 99% accuracy. But if the compliance team can't explain why it flagged a transaction, they can't defend that decision to regulators. If loan officers can't explain why an application was denied, they can't legally send that denial letter.</p>"},{"location":"modules/09-interpretability/#regulatory-requirements","title":"Regulatory Requirements","text":"<p>GDPR (EU General Data Protection Regulation): - Citizens have a \"right to explanation\" for automated decisions - If a machine makes a decision that significantly affects someone, they can demand to know why - Applies to credit scoring, hiring, insurance, healthcare</p> <p>Fair Lending Laws (US): - Equal Credit Opportunity Act requires reasons for adverse actions - \"Your application was denied because...\" is legally required - \"The algorithm said no\" doesn't satisfy the law</p> <p>Healthcare Regulations: - FDA scrutinizes AI medical devices - Clinicians need to understand recommendations before acting - Liability concerns: if something goes wrong, why did the AI recommend that?</p>"},{"location":"modules/09-interpretability/#building-stakeholder-trust","title":"Building Stakeholder Trust","text":"<p>Business stakeholders want to know: - Why did the model make this prediction? - Which factors are most important? - Can we trust this prediction? - What would change the prediction?</p> <p>Without trust: - Models won't be adopted\u2014people ignore recommendations - Decisions get overridden\u2014defeating the model's purpose - ML investment value is lost\u2014months of work unused</p>"},{"location":"modules/09-interpretability/#debugging-and-improving-models","title":"Debugging and Improving Models","text":"<p>Interpretability helps identify: - Spurious correlations: Model learned wrong patterns - Data leakage: Model using information it shouldn't have - Bias in training data: Historical biases encoded in predictions - Overfitting: Model memorized patterns that won't generalize</p> <p>The pneumonia example:</p> <p>Researchers trained a model to predict pneumonia severity from X-rays. The model performed exceptionally well\u2014too well.</p> <p>Investigation revealed: The model learned to associate \"portable X-ray\" equipment markers with low risk. Why? Portable X-rays were used for patients well enough to not need a trip to the radiology department. The model was predicting equipment type, not disease severity.</p> <p>Without interpretability tools, this would have been deployed and potentially harmed patients.</p> <p>Catching spurious correlations: For consequential models, investigating what the model learned is a professional responsibility. Use SHAP/LIME/PDP in your standard workflow. Show top features to domain experts (a radiologist would question equipment markers). Ask: \"What shortcuts could the model have taken?\" Test on out-of-distribution data. The investigation level should match the stakes\u2014product recommendations warrant less scrutiny than medical diagnosis.</p>"},{"location":"modules/09-interpretability/#discovering-bias","title":"Discovering Bias","text":"<p>ML models can encode and amplify biases: - Historical bias in training data - Proxy variables for protected attributes - Feedback loops</p> <p>Interpretability reveals: - Which features drive predictions for different groups - Whether protected attributes have indirect influence - Unexpected correlations that might indicate bias</p> <p>Example: A hiring model heavily weights ZIP code. ZIP code correlates with race and income. The model might be making discriminatory decisions even without explicit race features. This is proxy discrimination\u2014often unethical and sometimes illegal.</p>"},{"location":"modules/09-interpretability/#common-misconceptions","title":"Common Misconceptions","text":"Misconception Reality \"Accuracy is all that matters\" Without interpretability, you can't trust, debug, or deploy responsibly \"Deep learning can never be interpreted\" Many techniques exist (SHAP, attention, feature visualization) \"Simple models are always more interpretable\" A 100-feature linear model isn't necessarily interpretable"},{"location":"modules/09-interpretability/#92-interpretation-techniques","title":"9.2 Interpretation Techniques","text":""},{"location":"modules/09-interpretability/#global-vs-local-interpretability","title":"Global vs Local Interpretability","text":"<p>Global interpretability: Understand overall model behavior - Which features are generally important? - What patterns does the model use?</p> <p>Local interpretability: Understand individual predictions - Why was THIS customer predicted to churn? - What would change THIS decision?</p> <p>Both matter. Executives want global insights: \"What drives churn?\" Customer service needs local explanations: \"Why was this specific customer flagged?\"</p>"},{"location":"modules/09-interpretability/#permutation-importance","title":"Permutation Importance","text":"<p>The idea: 1. Train model, measure baseline performance 2. Shuffle one feature's values (break its signal) 3. Measure performance drop 4. Larger drop = more important feature</p> <p>Why it works: If a feature is important, breaking its signal hurts predictions.</p> <pre><code>from sklearn.inspection import permutation_importance\n\nresult = permutation_importance(\n    model,\n    X_test,\n    y_test,\n    n_repeats=10,\n    random_state=42\n)\n\n# Sort by importance\nfor i in result.importances_mean.argsort()[::-1]:\n    print(f\"{feature_names[i]}: {result.importances_mean[i]:.3f}\")\n</code></pre> <p>Advantages: - Works with any model (model-agnostic) - Uses held-out test data (reliable)</p> <p>Disadvantages: - Slow for many features - Misleading with correlated features (shuffling one is compensated by another)</p>"},{"location":"modules/09-interpretability/#partial-dependence-plots-pdp","title":"Partial Dependence Plots (PDP)","text":"<p>PDPs show the average effect of a feature on predictions.</p> <p>How it works: 1. For each value of feature X (e.g., age from 20 to 80) 2. Set ALL samples to that value 3. Average the predictions 4. Plot average prediction vs feature value</p> <pre><code>from sklearn.inspection import PartialDependenceDisplay\n\nPartialDependenceDisplay.from_estimator(\n    model,\n    X_train,\n    features=['age', 'income']\n)\n</code></pre> <p>Interpretation: - Upward slope: Higher feature value \u2192 higher prediction - Flat line: Little average effect - Non-linear shape: Complex relationship</p> <p>Limitation: Assumes feature independence. Can show impossible combinations (20-year-olds with $500K income).</p>"},{"location":"modules/09-interpretability/#shap-shapley-additive-explanations","title":"SHAP (SHapley Additive exPlanations)","text":"<p>Foundation: Shapley values from game theory\u2014fairly distribute \"credit\" among players.</p> <p>Applied to ML: How much did each feature contribute to pushing this prediction away from the average?</p> <p>Key properties (mathematically proven): 1. Local accuracy: SHAP values sum to prediction minus baseline 2. Consistency: More important features get higher values 3. Missingness: Unused features get zero attribution</p> <p>Interpretation: - SHAP &gt; 0: Feature pushed prediction higher - SHAP &lt; 0: Feature pushed prediction lower - Magnitude: Strength of effect</p> <p>The Shapley formula:</p> \\[\\phi_j = \\sum_{S \\subseteq N \\setminus \\{j\\}} \\frac{|S|!(|N|-|S|-1)!}{|N|!} [f(S \\cup \\{j\\}) - f(S)]\\] <p>In plain English: Consider all possible subsets of features. For each subset, measure how much adding feature j changes the prediction. Average these contributions with weights ensuring fairness.</p> <p>Computational complexity: Exact Shapley computation is exponential (2^n subsets). TreeSHAP exploits tree structure for polynomial-time exact values\u2014use it for random forests, XGBoost, LightGBM. DeepSHAP uses gradient approximations for neural networks. KernelSHAP handles arbitrary models but is slow. This often influences model choice: if interpretability + speed are required, tree-based models with TreeSHAP become attractive.</p>"},{"location":"modules/09-interpretability/#shap-in-practice","title":"SHAP in Practice","text":"<pre><code>import shap\n\n# For tree-based models (fast!)\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_test)\n\n# Summary plot (global view)\nshap.summary_plot(shap_values, X_test)\n\n# Force plot (single prediction)\nshap.force_plot(\n    explainer.expected_value,\n    shap_values[0],\n    X_test.iloc[0]\n)\n\n# Waterfall plot (detailed breakdown)\nshap.waterfall_plot(shap.Explanation(\n    values=shap_values[0],\n    base_values=explainer.expected_value,\n    data=X_test.iloc[0],\n    feature_names=feature_names\n))\n</code></pre> <p>SHAP variants: - TreeSHAP: Exact, fast for tree models - KernelSHAP: Model-agnostic, slower - DeepSHAP: For neural networks</p> <p>Use TreeSHAP when possible\u2014it's exact and fast.</p>"},{"location":"modules/09-interpretability/#shap-visualizations","title":"SHAP Visualizations","text":"<p>Summary plot: Global importance with direction - Each dot is one sample - X-axis: SHAP value - Color: Feature value (red = high, blue = low)</p> <p>Force plot: Single prediction breakdown - Starts from baseline - Shows features pushing up and down - Ends at actual prediction</p> <p>Waterfall plot: Step-by-step breakdown - From baseline to prediction - Each bar is one feature's contribution</p> <p>Dependence plot: Feature effect with interactions - Like PDP but shows actual points - Can color by another feature to see interactions</p>"},{"location":"modules/09-interpretability/#lime-local-interpretable-model-agnostic-explanations","title":"LIME (Local Interpretable Model-agnostic Explanations)","text":"<p>Core idea: Approximate complex model locally with a simple one.</p> <p>How it works: 1. Generate perturbed samples around the instance 2. Get complex model's predictions for those samples 3. Fit simple model (linear) weighted by distance 4. Interpret the simple model</p> <p>\"In the neighborhood of THIS prediction, what does the model behave like?\"</p> <pre><code>import lime.lime_tabular\n\nexplainer = lime.lime_tabular.LimeTabularExplainer(\n    X_train.values,\n    feature_names=feature_names,\n    class_names=['Not Churn', 'Churn'],\n    mode='classification'\n)\n\nexplanation = explainer.explain_instance(\n    X_test.iloc[0].values,\n    model.predict_proba,\n    num_features=10\n)\n\nexplanation.show_in_notebook()\n</code></pre>"},{"location":"modules/09-interpretability/#shap-vs-lime","title":"SHAP vs LIME","text":"Aspect SHAP LIME Foundation Game theory Local approximation Consistency Mathematically guaranteed Not guaranteed Speed Fast with TreeSHAP Generally slower Global view Yes (aggregate) Limited <p>Both are valuable. SHAP has stronger theoretical foundations. LIME can be more intuitive.</p>"},{"location":"modules/09-interpretability/#important-caveats","title":"Important Caveats","text":"<p>Feature importance \u2260 causation.</p> <p>When SHAP says \"age is the most important feature,\" it means age most influences predictions. It does NOT mean age causes the outcome.</p> <p>A model might use age as a strong predictor of churn, but that doesn't mean getting older causes churn. There might be a confounder.</p> <p>Don't confuse prediction importance with causal importance.</p>"},{"location":"modules/09-interpretability/#common-misconceptions_1","title":"Common Misconceptions","text":"Misconception Reality \"Feature importance = causation\" Importance shows prediction influence, not causal effect \"SHAP values are always exact\" KernelSHAP is approximate; TreeSHAP is exact only for trees \"High attention = high importance\" Attention weights can be misleading"},{"location":"modules/09-interpretability/#93-communicating-model-insights","title":"9.3 Communicating Model Insights","text":""},{"location":"modules/09-interpretability/#the-communication-challenge","title":"The Communication Challenge","text":"<p>You've learned powerful interpretation techniques. SHAP gives detailed attributions. PDP shows relationships. LIME approximates local behavior.</p> <p>But your stakeholders don't care about SHAP values.</p> <p>The CEO wants: \"Should we invest in this model?\" The marketing VP wants: \"Which customers should we target?\" The compliance officer wants: \"Can we legally use this?\"</p> <p>Your job is to translate technical insights into actionable business recommendations.</p>"},{"location":"modules/09-interpretability/#executive-summary-structure","title":"Executive Summary Structure","text":"<p>Five parts:</p> <ol> <li>Business question: What were we predicting and why?</li> <li>Key finding: What's the main takeaway?</li> <li>Top factors: What drives predictions? (3-5 factors max)</li> <li>Confidence: How reliable? Any limitations?</li> <li>Recommendation: What should we do?</li> </ol> <p>No code. No jargon. Just business value.</p>"},{"location":"modules/09-interpretability/#example-executive-summary","title":"Example Executive Summary","text":"<pre><code>EXECUTIVE SUMMARY: Customer Churn Model\n\nBusiness Question: Which customers are likely to cancel\ntheir subscription in the next 90 days?\n\nKey Finding: We can identify 75% of churning customers\nbefore they leave, with 80% precision\u2014meaning 4 out of 5\ncustomers we flag will actually churn.\n\nTop Factors Driving Churn Risk:\n1. Support tickets in last 30 days (more tickets = higher risk)\n2. Days since last login (longer gap = higher risk)\n3. Contract type (monthly contracts 3x more likely to churn)\n\nConfidence: Model validated on 6 months of holdout data.\nLimitation: Works best for customers with 90+ days of history.\n\nRecommendation: Prioritize retention outreach to customers\nwith churn probability &gt; 70%. Expected ROI: $2.50 saved\nper $1 spent on retention.\n</code></pre> <p>No mention of random forests, SHAP, or cross-validation. Just business-relevant insights.</p>"},{"location":"modules/09-interpretability/#visualizations-for-business-audiences","title":"Visualizations for Business Audiences","text":"<ul> <li>Keep visualizations simple</li> <li>Use familiar formats (bar charts, line plots)</li> <li>Add clear labels and titles</li> <li>Highlight key insights with annotations</li> </ul> <p>Bad: Show a SHAP summary plot with no explanation</p> <p>Good: Show \"Top 5 Factors Driving Churn Risk\" with clear labels</p> <p>You might derive it from SHAP values, but the presentation is business-focused.</p> <p>Ethics of simplification: Simplification is often your professional obligation\u2014communication your audience can't understand serves no one. Distinguish appropriate simplification (\"the model uses engagement patterns\") from misleading omission (\"95% accurate\" without mentioning failure on new customers). Report uncertainty and limitations clearly. The ethical burden is on honesty, not exhaustive technical detail.</p>"},{"location":"modules/09-interpretability/#explaining-individual-predictions","title":"Explaining Individual Predictions","text":"<p>For customer-facing explanations: - Use natural language - Focus on top 2-3 factors - Avoid technical jargon - Provide actionable insights</p>"},{"location":"modules/09-interpretability/#adverse-action-example","title":"Adverse Action Example","text":"<p>When someone is denied credit, they're legally entitled to reasons:</p> <pre><code>Your loan application was declined. The main factors were:\n\n1. Your debt-to-income ratio is above our threshold\n2. Your credit history is shorter than we typically require\n3. Recent credit inquiries suggest high credit-seeking behavior\n\nSteps you can take to improve your chances:\n- Pay down existing debt to lower your debt-to-income ratio\n- Wait 6 months to build more credit history\n- Avoid applying for new credit in the near term\n</code></pre> <p>Specific, actionable, no jargon.</p>"},{"location":"modules/09-interpretability/#model-cards","title":"Model Cards","text":"<p>Model cards are documentation standards for ML models (introduced by Google).</p> <p>Components: 1. Model details: Type, version, date, owner 2. Intended use: What is this model for? What is it NOT for? 3. Factors: Relevant attributes (demographics, etc.) 4. Metrics: Performance overall AND by subgroup 5. Training data: What data was used? 6. Limitations: When does the model fail? 7. Ethical considerations: Potential harms, biases</p> <p>What belongs in a model card that wouldn't be in a technical report?</p> <p>Intended use and ethical considerations. A technical report says \"accuracy is 95%.\" A model card says \"this model is intended for prioritizing retention outreach, not for making final decisions about customer termination. It should not be used for populations under 18.\"</p>"},{"location":"modules/09-interpretability/#documenting-limitations","title":"Documenting Limitations","text":"<p>Being honest about limitations builds trust and prevents misuse.</p> <p>Good: - \"Model performance degrades for customers in the first 30 days\" - \"Validated only on US customers; may not generalize internationally\" - \"Does not account for seasonal effects\"</p> <p>Bad: - \"Model has some limitations\" (too vague) - Nothing at all (dangerous)</p>"},{"location":"modules/09-interpretability/#reflection-questions","title":"Reflection Questions","text":"<ol> <li> <p>A bank's loan approval model has 95% accuracy but can't explain decisions. Why might regulators reject it?</p> </li> <li> <p>You discover your hiring model relies heavily on ZIP code. Why is this concerning?</p> </li> <li> <p>SHAP shows 'age' has highest importance, but PDP shows a flat relationship. How is this possible?</p> </li> <li> <p>You need to explain a loan denial to a customer. Would you use SHAP or LIME? Why?</p> </li> <li> <p>A stakeholder asks \"which feature is most important?\" What clarifying questions should you ask?</p> </li> <li> <p>Your model uses 50 features. How do you explain it to a CEO in 5 minutes?</p> </li> <li> <p>A customer asks why their insurance premium increased. How do you respond without technical jargon?</p> </li> </ol>"},{"location":"modules/09-interpretability/#practice-problems","title":"Practice Problems","text":"<ol> <li> <p>Given SHAP values for a prediction, write the explanation in plain English</p> </li> <li> <p>Identify potential problems from PDP shapes (non-monotonic, discontinuous)</p> </li> <li> <p>Choose appropriate explanation technique for different scenarios</p> </li> <li> <p>Write an adverse action notice from model output</p> </li> <li> <p>Create a model card outline for a fraud detection system</p> </li> </ol>"},{"location":"modules/09-interpretability/#chapter-summary","title":"Chapter Summary","text":"<p>Six key takeaways from Module 9:</p> <ol> <li> <p>Interpretability is required for regulation, trust, and debugging</p> </li> <li> <p>Global shows overall patterns; Local shows individual predictions</p> </li> <li> <p>SHAP provides mathematically principled feature attribution</p> </li> <li> <p>LIME approximates complex models locally with simple ones</p> </li> <li> <p>Executive summaries translate technical findings to business value</p> </li> <li> <p>Model cards standardize documentation including limitations</p> </li> </ol>"},{"location":"modules/09-interpretability/#whats-next","title":"What's Next","text":"<p>In Module 10, we tackle Ethics, Fairness &amp; Deployment: - Bias in ML systems and how it arises - Fairness metrics and definitions - Bias mitigation techniques - Responsible AI practices - Model deployment considerations</p> <p>Interpretability is the foundation for fairness analysis! You can't assess whether a model is fair if you can't understand what it's doing.</p>"},{"location":"modules/10-ethics-deployment/","title":"Module 10: Ethics, Deployment &amp; Real-World ML","text":""},{"location":"modules/10-ethics-deployment/#introduction","title":"Introduction","text":"<p>We've covered an incredible amount of ground in this course. You can build regression models, classification models, ensemble methods, neural networks, CNNs for images, transformers for text. You can interpret models with SHAP and LIME. You know how to evaluate, tune, and avoid common pitfalls.</p> <p>But here's the thing: Building a model is only half the journey.</p> <p>This module tackles the other half\u2014getting models into the real world responsibly and effectively. This means grappling with ethics and fairness, learning time series forecasting, understanding deployment, and calculating business value.</p> <p>These topics bridge technical skills to real-world impact. Every data scientist who wants to make a difference needs to master them.</p> <p>Responsibility for fairness: All three share responsibility. Data scientists are the first line of defense and should raise concerns. Companies set culture, allocate resources for fairness audits, and establish review processes\u2014they're culpable for pressuring fast deployment without ethical review. Regulators provide external accountability that markets fail to create. The healthiest ecosystem has all three layers; relying on any single one is insufficient.</p>"},{"location":"modules/10-ethics-deployment/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, you should be able to:</p> <ol> <li>Identify sources of bias in ML systems and propose mitigation strategies</li> <li>Apply fairness metrics to evaluate model equity across groups</li> <li>Build time series forecasting models using appropriate techniques</li> <li>Explain the basics of model deployment and MLOps</li> <li>Calculate business value and ROI of ML projects</li> <li>Communicate uncertainty and manage stakeholder expectations</li> </ol>"},{"location":"modules/10-ethics-deployment/#101-ethics-responsible-ai","title":"10.1 Ethics &amp; Responsible AI","text":""},{"location":"modules/10-ethics-deployment/#the-stakes-are-high","title":"The Stakes Are High","text":"<p>ML systems are making consequential decisions: who gets a loan, who gets a job, who gets parole, who gets medical treatment. These aren't abstract technical problems\u2014they affect real people's lives.</p> <p>And here's the uncomfortable truth: ML systems can be biased. They can be unfair. They can cause harm.</p> <p>Not because the engineers are malicious, but because bias creeps in through data, design choices, and blind spots. If we're going to deploy these systems, we need to understand how bias arises and how to mitigate it.</p>"},{"location":"modules/10-ethics-deployment/#sources-of-bias-in-ml","title":"Sources of Bias in ML","text":"<p>Historical Bias: Training data reflects past discrimination. - If you train on 10 years of hiring data, and that data reflects historical biases against women or minorities, your model learns those biases. - The model isn't \"biased by itself\"\u2014it's learning patterns from biased data.</p> <p>Selection Bias: Training data doesn't represent the population. - A medical AI trained mostly on data from white patients may perform worse on underrepresented groups. - The model has never learned the patterns for those populations.</p> <p>Measurement Bias: Features are measured differently across groups. - \"Years of experience\" penalizes career gaps, which disproportionately affects women. - \"Arrests\" doesn't mean \"crimes committed\"\u2014it reflects policing patterns.</p> <p>Aggregation Bias: One model for heterogeneous populations. - A single diabetes prediction model may work differently across ethnicities. - Sometimes you need separate models or careful feature engineering.</p> <p>Feedback Loops: Model predictions affect future data. - Predictive policing sends more officers to certain neighborhoods \u2192 more arrests \u2192 more \"crime\" data \u2192 model sends even more officers. - The bias becomes self-reinforcing.</p>"},{"location":"modules/10-ethics-deployment/#case-study-amazon-hiring-tool","title":"Case Study: Amazon Hiring Tool","text":"<p>In 2018, it was reported that Amazon had built a hiring tool trained on 10 years of resume data.</p> <p>What went wrong: - The model learned to penalize words like \"women's\" (as in \"women's chess club captain\") - It downgraded graduates of women's colleges - It effectively discriminated against female applicants</p> <p>The lesson: Historical data encodes historical bias. Amazon's tech workforce was predominantly male. The model learned that being male correlated with getting hired. It wasn't explicitly told \"penalize women,\" but it learned it from the patterns.</p> <p>Amazon scrapped the tool.</p>"},{"location":"modules/10-ethics-deployment/#case-study-compas","title":"Case Study: COMPAS","text":"<p>COMPAS is a recidivism prediction algorithm used in the US criminal justice system to predict whether defendants will reoffend.</p> <p>ProPublica's analysis found: - Black defendants had a higher false positive rate (incorrectly flagged as high risk) - White defendants had a higher false negative rate (incorrectly flagged as low risk) - Same overall accuracy, very different error patterns</p> <p>This shows that identical accuracy can hide profoundly different impacts on different groups.</p> <p>Choosing between fairness criteria: This is an ethical decision, not technical\u2014it shouldn't be made solely by data scientists. The data scientist's role is to make tradeoffs transparent (\"if we optimize for A, here's what happens to X and Y\"), not to unilaterally decide. These decisions should involve domain experts, affected communities, legal experts, and ethicists. Document the decision, reasoning, and who was involved.</p>"},{"location":"modules/10-ethics-deployment/#fairness-metrics","title":"Fairness Metrics","text":"<p>There are multiple mathematical definitions of fairness:</p> <p>Demographic Parity (Statistical Parity): $\\(P(\\hat{Y}=1|A=0) = P(\\hat{Y}=1|A=1)\\)$ - Equal positive prediction rates across groups - If 30% of men get approved, 30% of women should get approved - Limitation: Ignores actual qualification rates</p> <p>Equalized Odds: $\\(P(\\hat{Y}=1|Y=1, A=0) = P(\\hat{Y}=1|Y=1, A=1)\\)$ $\\(P(\\hat{Y}=1|Y=0, A=0) = P(\\hat{Y}=1|Y=0, A=1)\\)$ - Equal true positive rates AND equal false positive rates across groups - If you're qualified, you should have equal chance of being accepted regardless of group - If you're unqualified, you should have equal chance of being rejected</p> <p>Predictive Parity: $\\(P(Y=1|\\hat{Y}=1, A=0) = P(Y=1|\\hat{Y}=1, A=1)\\)$ - Equal precision across groups - If the model says \"yes,\" the probability of actually being qualified should be the same across groups</p>"},{"location":"modules/10-ethics-deployment/#the-impossibility-theorem","title":"The Impossibility Theorem","text":"<p>You cannot satisfy all fairness criteria simultaneously (except in special cases).</p> <p>This isn't a technical limitation\u2014it's mathematically proven. If groups have different base rates (different proportions of positive outcomes), you have to choose which fairness criterion matters most.</p> <p>Example:</p> Group Accuracy FPR FNR A 85% 10% 20% B 85% 25% 5% <p>Same accuracy. But Group B has more false positives (more people incorrectly flagged). Group A has more false negatives (more people incorrectly missed).</p> <p>Which is worse depends on context. In criminal justice, high FPR means innocent people in jail. In medical diagnosis, high FNR means sick people going untreated.</p>"},{"location":"modules/10-ethics-deployment/#calculating-fairness-metrics","title":"Calculating Fairness Metrics","text":"<pre><code>from fairlearn.metrics import MetricFrame\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\n\n# Calculate metrics by group\nmetric_frame = MetricFrame(\n    metrics={\n        'accuracy': accuracy_score,\n        'precision': precision_score,\n        'recall': recall_score\n    },\n    y_true=y_test,\n    y_pred=y_pred,\n    sensitive_features=demographic_feature\n)\n\n# View metrics by group\nprint(metric_frame.by_group)\n\n# View maximum difference between groups\nprint(metric_frame.difference())\n</code></pre> <p>The <code>fairlearn</code> library makes this straightforward. Calculate your metrics by demographic group and look for disparities.</p>"},{"location":"modules/10-ethics-deployment/#proxy-variables","title":"Proxy Variables","text":"<p>\"We don't use race in our model, so it can't be biased.\"</p> <p>This is wrong.</p> <p>Proxy variables are features that correlate with protected attributes: - ZIP code correlates with race and income - Name can indicate gender or ethnicity - Arrest history correlates with race (due to policing patterns) - \"Years since last job\" correlates with gender (career gaps)</p> <p>Removing the protected attribute doesn't remove the bias if proxies remain.</p>"},{"location":"modules/10-ethics-deployment/#when-not-to-use-ml","title":"When NOT to Use ML","text":"<p>Not every problem needs machine learning.</p> <p>Consider avoiding ML when: - Stakes are very high and errors are catastrophic - Accountability and explanation are paramount - Training data is fundamentally biased - The problem is better solved by policy - Human judgment is essential</p> <p>Questions to ask: - Who is affected by this system? - What happens when it's wrong? - Can we explain decisions to affected parties? - Is the training data representative? - Are we automating an already unfair process?</p> <p>Sometimes the right answer is \"don't build this model.\"</p> <p>Pushing back on unethical projects: Document concerns and frame in terms of business risk (legal liability, reputational damage). Escalate through appropriate channels\u2014ethics hotlines, ombudspersons. If internal advocacy fails: comply under protest (documented), refuse the project (accept consequences), or leave. Building a financial cushion gives leverage. Long-term: seek employers whose values align with yours\u2014ask about ethics review processes during interviews.</p>"},{"location":"modules/10-ethics-deployment/#common-misconceptions","title":"Common Misconceptions","text":"Misconception Reality \"ML is objective because it's math\" ML learns patterns from data, including human biases \"Equal accuracy = fairness\" Same accuracy can hide very different error patterns across groups \"Just remove protected attributes\" Proxy variables can encode same information \"Fairness is a purely technical problem\" Requires ethical choices that should involve diverse stakeholders"},{"location":"modules/10-ethics-deployment/#102-time-series-forecasting","title":"10.2 Time Series Forecasting","text":""},{"location":"modules/10-ethics-deployment/#why-time-series-is-different","title":"Why Time Series Is Different","text":"<p>Time series data has a unique property: temporal ordering matters.</p> <p>In standard ML, we assume observations are independent\u2014shuffling rows shouldn't matter. In time series, shuffling destroys the information. Yesterday's sales tell you something about today's sales. January's patterns repeat every January.</p> <p>This changes everything about how we model and validate.</p>"},{"location":"modules/10-ethics-deployment/#time-series-components","title":"Time Series Components","text":"<p>Time series can be decomposed into components:</p> <p>Trend: Long-term direction (sales growing over years)</p> <p>Seasonality: Regular patterns (sales spike every December)</p> <p>Cyclical: Irregular longer-term fluctuations (economic cycles)</p> <p>Noise: Random variation</p> <pre><code>from statsmodels.tsa.seasonal import seasonal_decompose\n\ndecomposition = seasonal_decompose(\n    series,\n    model='additive',\n    period=12\n)\ndecomposition.plot()\n</code></pre> <p>Understanding these components helps you choose the right model and spot problems.</p>"},{"location":"modules/10-ethics-deployment/#arima-models","title":"ARIMA Models","text":"<p>ARIMA is the classic statistical approach to time series.</p> <p>ARIMA(p, d, q): - AR (AutoRegressive): Predict from past values (how many lags to use = p) - I (Integrated): Differencing for stationarity (how many times to difference = d) - MA (Moving Average): Predict from past errors (how many lag errors to use = q)</p> <pre><code>from statsmodels.tsa.arima.model import ARIMA\n\nmodel = ARIMA(train_series, order=(1, 1, 1))\nresults = model.fit()\nforecast = results.forecast(steps=30)\n</code></pre> <p>Choosing p, d, q: - ACF/PACF plots give guidance - AIC/BIC criteria for model selection - Or use auto_arima:</p> <pre><code>from pmdarima import auto_arima\n\nmodel = auto_arima(\n    train_series,\n    seasonal=True,\n    m=12,  # Monthly seasonality\n    trace=True\n)\n</code></pre> <p>Auto_arima searches through parameter combinations and picks the best one.</p>"},{"location":"modules/10-ethics-deployment/#prophet","title":"Prophet","text":"<p>Facebook's Prophet is a popular alternative to ARIMA.</p> <p>Advantages: - Handles seasonality automatically (multiple seasonalities!) - Robust to missing data - Interpretable components - Easy to add holidays and special events</p> <pre><code>from prophet import Prophet\n\n# Data must have columns 'ds' (date) and 'y' (value)\nmodel = Prophet(\n    yearly_seasonality=True,\n    weekly_seasonality=True\n)\nmodel.fit(train_df)\n\nfuture = model.make_future_dataframe(periods=30)\nforecast = model.predict(future)\n\nmodel.plot(forecast)\nmodel.plot_components(forecast)  # Shows trend, seasonality, etc.\n</code></pre> <p>Prophet is particularly good for business applications with strong seasonal patterns.</p>"},{"location":"modules/10-ethics-deployment/#when-to-choose-what","title":"When to Choose What","text":"<ul> <li>Prophet: Multiple seasonalities, missing data, holidays, interpretable components</li> <li>ARIMA: More control, complex series that don't fit Prophet's assumptions, very short series</li> <li>LSTM: Complex non-linear patterns, multiple input features, long sequences</li> </ul>"},{"location":"modules/10-ethics-deployment/#time-series-validation","title":"Time Series Validation","text":"<p>You cannot use standard k-fold cross-validation for time series.</p> <p>Why? Because it would leak future information into training. If your test set contains January 2024 and your training set contains February 2024, you're cheating\u2014you're using the future to predict the past.</p> <p>Walk-forward validation: <pre><code>Train: [----]          Test: [-]\nTrain: [------]        Test: [-]\nTrain: [--------]      Test: [-]\n</code></pre></p> <p>Always train on the past, test on the future. Never the reverse.</p> <pre><code>from sklearn.model_selection import TimeSeriesSplit\n\ntscv = TimeSeriesSplit(n_splits=5)\nfor train_idx, test_idx in tscv.split(data):\n    train = data[train_idx]\n    test = data[test_idx]\n    # Train and evaluate\n</code></pre>"},{"location":"modules/10-ethics-deployment/#business-applications","title":"Business Applications","text":"<p>Time series forecasting is everywhere in business: - Sales forecasting: Budget planning, resource allocation - Demand planning: Inventory management, supply chain - Capacity planning: Staffing, infrastructure - Financial forecasting: Revenue projections, cash flow</p>"},{"location":"modules/10-ethics-deployment/#103-model-deployment","title":"10.3 Model Deployment","text":""},{"location":"modules/10-ethics-deployment/#from-notebook-to-production","title":"From Notebook to Production","text":"<p>You've built a model in a Jupyter notebook. It works great. Now what?</p> <p>The gap between \"model works\" and \"model is deployed\" is significant: - How do other systems call your model? - How do you handle errors? - How do you scale? - How do you update the model? - How do you monitor performance?</p> <p>This is where software engineering meets data science.</p>"},{"location":"modules/10-ethics-deployment/#model-serialization","title":"Model Serialization","text":"<p>First, you need to save your model so it can be loaded elsewhere.</p> <p>Pickle/Joblib (for scikit-learn models): <pre><code>import joblib\n\n# Save model\njoblib.dump(model, 'model.pkl')\n\n# Load model\nmodel = joblib.load('model.pkl')\n</code></pre></p> <p>ONNX (cross-platform format): - Works across different frameworks (PyTorch, TensorFlow, scikit-learn) - Optimized for inference - Useful when production environment differs from development</p> <pre><code>import torch.onnx\n\ntorch.onnx.export(\n    model,\n    dummy_input,\n    \"model.onnx\",\n    input_names=['input'],\n    output_names=['output']\n)\n</code></pre>"},{"location":"modules/10-ethics-deployment/#creating-apis","title":"Creating APIs","text":"<p>To let other systems use your model, wrap it in an API.</p> <p>Flask (simple, widely used): <pre><code>from flask import Flask, request, jsonify\nimport joblib\n\napp = Flask(__name__)\nmodel = joblib.load('model.pkl')\n\n@app.route('/predict', methods=['POST'])\ndef predict():\n    data = request.json\n    features = data['features']\n    prediction = model.predict([features])\n    return jsonify({'prediction': prediction[0]})\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5000)\n</code></pre></p> <p>FastAPI (modern, automatic documentation): <pre><code>from fastapi import FastAPI\nfrom pydantic import BaseModel\nimport joblib\n\napp = FastAPI()\nmodel = joblib.load('model.pkl')\n\nclass PredictionRequest(BaseModel):\n    features: list\n\n@app.post(\"/predict\")\ndef predict(request: PredictionRequest):\n    prediction = model.predict([request.features])\n    return {\"prediction\": prediction[0]}\n</code></pre></p> <p>Now other applications can send HTTP requests to get predictions.</p>"},{"location":"modules/10-ethics-deployment/#containerization-with-docker","title":"Containerization with Docker","text":"<p>Docker packages your application with all its dependencies.</p> <p>Dockerfile: <pre><code>FROM python:3.9-slim\n\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nCOPY model.pkl .\nCOPY app.py .\n\nEXPOSE 5000\nCMD [\"python\", \"app.py\"]\n</code></pre></p> <p>Benefits: - Reproducibility: Same environment everywhere - Portability: Runs on any system with Docker - Scalability: Easy to run multiple containers - Isolation: Dependencies don't conflict</p>"},{"location":"modules/10-ethics-deployment/#cloud-deployment-options","title":"Cloud Deployment Options","text":"Platform Use Case Complexity AWS SageMaker Full ML platform Medium Google Vertex AI Full ML platform Medium Azure ML Full ML platform Medium Heroku Simple web apps Low AWS Lambda Serverless, simple models Low <p>For production ML at scale, the major cloud platforms provide integrated solutions. For simple models or prototypes, Heroku or Lambda can be quick to set up.</p>"},{"location":"modules/10-ethics-deployment/#104-production-business-value","title":"10.4 Production &amp; Business Value","text":""},{"location":"modules/10-ethics-deployment/#monitoring-in-production","title":"Monitoring in Production","text":"<p>Deployment isn't the end\u2014it's the beginning of a new phase.</p> <p>Metrics to track continuously: - Model accuracy over time: Is performance degrading? - Feature distributions: Are inputs changing? - Latency and throughput: Is the system fast enough? - Error rates: Are there failures?</p> <p>Set up alerts. If accuracy drops below a threshold, you want to know immediately.</p>"},{"location":"modules/10-ethics-deployment/#model-drift","title":"Model Drift","text":"<p>Data drift: Input distribution changes. - Customer demographics shift - Seasonality wasn't captured - Data collection process changed</p> <p>Concept drift: The relationship between inputs and outputs changes. - Customer behavior changed (e.g., during COVID) - What used to predict churn no longer does</p> <pre><code>from evidently import Report\nfrom evidently.metrics import DataDriftTable\n\nreport = Report(metrics=[DataDriftTable()])\nreport.run(\n    reference_data=training_data,\n    current_data=production_data\n)\nreport.show()\n</code></pre> <p>Evidently AI and similar tools help detect when your data has shifted.</p>"},{"location":"modules/10-ethics-deployment/#retraining-strategies","title":"Retraining Strategies","text":"<p>When drift is detected (or just periodically), you need to retrain.</p> <p>Scheduled retraining: Weekly, monthly\u2014on a fixed schedule</p> <p>Triggered retraining: When drift exceeds a threshold</p> <p>Continuous training: Update with each new batch of data</p> <p>Automate the pipeline: 1. Data validation 2. Feature engineering 3. Model training 4. Evaluation (reject if metrics don't meet threshold) 5. Deployment</p> <p>This is where MLOps comes in\u2014applying DevOps principles to ML.</p>"},{"location":"modules/10-ethics-deployment/#ab-testing","title":"A/B Testing","text":"<p>Purpose: Compare new model against current model</p> <p>Implementation: 1. Route percentage of traffic to new model 2. Track metrics for both models 3. Statistical test for significance 4. Roll out winner</p> <p>Key considerations: - Sample size requirements - Duration of test - Guardrail metrics (don't harm user experience)</p>"},{"location":"modules/10-ethics-deployment/#roi-calculation","title":"ROI Calculation","text":"<p>How do you justify an ML project?</p> <p>Example: Churn prevention model</p> <pre><code>Annual churning customers: 10,000\nCustomer lifetime value: $500\nChurn cost without model: $5,000,000\n\nModel performance:\n- Identifies 75% of churners\n- Intervention success rate: 30%\n- Customers saved: 10,000 \u00d7 0.75 \u00d7 0.30 = 2,250\n- Value saved: 2,250 \u00d7 $500 = $1,125,000\n\nCosts:\n- Development: $100,000\n- Annual maintenance: $20,000\n- Intervention cost: $50 per flagged customer\n- Total intervention cost: 7,500 \u00d7 $50 = $375,000\n- Total costs: $495,000\n\nFirst year ROI: ($1,125,000 - $495,000) / $495,000 = 127%\n</code></pre> <p>The key: Quantify business impact, not just accuracy. \"95% accuracy\" means nothing to a CFO. \"$630,000 net value in year one\" does.</p>"},{"location":"modules/10-ethics-deployment/#communicating-uncertainty","title":"Communicating Uncertainty","text":"<p>Be honest about limitations: - Model accuracy is not 100% - Performance varies across segments - Future performance is not guaranteed - Edge cases exist</p> <p>Use confidence intervals: - \"We predict revenue of $1.2M \u00b1 $150K\" - \"The model is 85% confident this customer will churn\"</p> <p>Scenario analysis: - Best case / Base case / Worst case</p> <p>Stakeholders appreciate honesty. Overpromising leads to disappointment and loss of trust.</p>"},{"location":"modules/10-ethics-deployment/#managing-expectations","title":"Managing Expectations","text":"<p>Common pitfalls: - Overpromising accuracy - Underestimating timeline - Ignoring maintenance needs - Assuming it's a one-time effort</p> <p>Best practices: - Start with a pilot project - Set realistic expectations upfront - Plan for iteration\u2014first version won't be perfect - Communicate progress regularly - Budget for ongoing maintenance</p> <p>ML models are more like products than projects. The world changes, data shifts, and models degrade. You wouldn't build a website and never update it. Same with ML models.</p> <p>Estimating maintenance costs: Rule of thumb: 15-25% of initial development cost per year. Break down components: monitoring infrastructure, data pipeline maintenance, periodic retraining, model auditing, incident response. Staff time is usually the largest cost (one person at 20% time \u2248 $30-50K/year). Present stakeholders with scenarios: \"minimum maintenance\" costs X with degradation risk; \"recommended maintenance\" costs Y with better reliability.</p>"},{"location":"modules/10-ethics-deployment/#reflection-questions","title":"Reflection Questions","text":"<ol> <li> <p>Amazon's hiring tool was trained on successful employees. Why did it still produce biased results?</p> </li> <li> <p>A model has 85% accuracy for both men and women. Is it fair? What else would you check?</p> </li> <li> <p>Your model uses ZIP code, which correlates with race. Should you remove it? What are the trade-offs?</p> </li> <li> <p>A hospital wants to use ML to allocate scarce medical resources. What ethical considerations arise?</p> </li> <li> <p>Why can't we use regular k-fold cross-validation for time series?</p> </li> <li> <p>When would you choose Prophet over ARIMA?</p> </li> <li> <p>Your model works on your laptop but fails in production. What might cause this?</p> </li> <li> <p>A model improves accuracy by 2% but costs $500K to develop. How do you decide if it's worth it?</p> </li> <li> <p>How do you explain to a CFO that ML requires ongoing investment, not a one-time cost?</p> </li> </ol>"},{"location":"modules/10-ethics-deployment/#practice-problems","title":"Practice Problems","text":"<ol> <li> <p>Calculate fairness metrics from confusion matrices for two demographic groups</p> </li> <li> <p>Identify bias sources in a case study scenario</p> </li> <li> <p>Design a monitoring plan for a deployed fraud detection model</p> </li> <li> <p>Calculate ROI for an ML project given costs and projected benefits</p> </li> <li> <p>Write a brief stakeholder communication explaining a model's limitations</p> </li> </ol>"},{"location":"modules/10-ethics-deployment/#chapter-summary","title":"Chapter Summary","text":"<p>Six key takeaways from Module 10:</p> <ol> <li> <p>Bias enters ML through data and design; measure and mitigate it actively</p> </li> <li> <p>Fairness has multiple incompatible definitions\u2014you must choose</p> </li> <li> <p>Time series requires temporal validation; never use k-fold</p> </li> <li> <p>Deployment needs APIs, containers, and monitoring infrastructure</p> </li> <li> <p>Drift happens; plan for detection and retraining</p> </li> <li> <p>Business value must be quantified and communicated in dollars, not accuracy</p> </li> </ol>"},{"location":"modules/10-ethics-deployment/#course-summary","title":"Course Summary","text":"<p>Take a moment to appreciate how far you've come.</p> <p>You've learned to: - Build ML models (regression, classification, clustering) - Train ensemble methods and understand their power - Train neural networks for structured data, images, and text - Interpret and explain model decisions - Deploy responsibly with fairness considerations and monitoring</p> <p>You understand: - The ML workflow from data to deployment - Evaluation, validation, and avoiding common pitfalls - When to use which technique - How to communicate with stakeholders</p> <p>You're ready for the Capstone Project!</p>"},{"location":"modules/10-ethics-deployment/#whats-next-capstone-project","title":"What's Next: Capstone Project","text":"<p>The capstone project is your chance to apply everything you've learned to a real problem.</p> <p>You'll: - Define a business problem - Collect and prepare data - Build and evaluate models - Interpret and explain results - Create deployment and monitoring plans - Present to stakeholders</p> <p>This is the culmination of the course\u2014showing that you can take a problem from start to finish.</p>"}]}