{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"BAN 501: Predictive Modeling","text":""},{"location":"#course-companion","title":"Course Companion","text":"<p>Welcome to the BAN 501 Course Companion. This resource provides comprehensive coverage of predictive modeling concepts, from foundational machine learning principles to advanced deep learning applications.</p>"},{"location":"#how-to-use-this-companion","title":"How to Use This Companion","text":"<ul> <li>Sequential reading: Work through modules 1-10 in order for a complete learning path</li> <li>Reference: Jump to specific topics using the navigation sidebar</li> <li>Search: Use the search bar to find specific concepts or terms</li> <li>Deep dives: Explore appendices for extended technical discussions</li> </ul>"},{"location":"#module-overview","title":"Module Overview","text":"Module Topic Key Concepts 1 Foundations Train/test splits, bias-variance tradeoff, evaluation metrics 2 Regression Linear regression, gradient descent, regularization 3 Classification Logistic regression, decision boundaries, ROC/AUC 4 Ensemble Methods Bagging, boosting, random forests, XGBoost 5 Unsupervised Learning Clustering, dimensionality reduction, PCA 6 Neural Networks Perceptrons, backpropagation, deep learning 7 Computer Vision CNNs, image classification, transfer learning 8 NLP Text processing, embeddings, transformers 9 Interpretability SHAP, LIME, feature importance 10 Ethics &amp; Deployment Fairness, model deployment, monitoring"},{"location":"#prerequisites","title":"Prerequisites","text":"<p>This companion assumes familiarity with:</p> <ul> <li>Basic Python programming</li> <li>Introductory statistics (mean, variance, distributions)</li> <li>Linear algebra fundamentals (matrices, vectors)</li> </ul>"},{"location":"#appendices","title":"Appendices","text":"<p>The appendices provide deeper technical explorations:</p> <ul> <li>Universal Approximators: Why neural networks can learn any function</li> <li>CNN Architecture: Detailed breakdown of convolutional networks</li> <li>Transformer Architecture: The architecture behind modern NLP</li> <li>Surprising Phenomena: Double descent, grokking, and emergent abilities</li> </ul>"},{"location":"appendices/cnn-architecture/","title":"Deep Dive: CNN Architecture","text":"<p>Extends Module 7: Computer Vision &amp; CNNs</p>"},{"location":"appendices/cnn-architecture/#introduction","title":"Introduction","text":"<p>In Module 7, we learned that CNNs are dramatically more parameter-efficient than fully connected networks for images. But why? What makes convolution so special?</p> <p>This deep dive explores the three principles that make CNNs work, walks through the convolution operation with concrete numbers, and shows exactly where parameters live in PyTorch's <code>nn.Conv2d</code>.</p> <p>By the end, you'll understand not just how CNNs work, but why they're designed the way they are.</p> <p>Why weren't CNNs always used? CNNs were discovered early (LeNet, 1989) but required computational resources that didn't exist. The 2012 AlexNet breakthrough combined sufficient data (ImageNet), compute (GPUs), and architecture improvements (ReLU, dropout). CNNs existed but weren't practical; FC networks and hand-crafted features (SIFT, HOG) were what people could actually train.</p>"},{"location":"appendices/cnn-architecture/#the-parameter-explosion-problem","title":"The Parameter Explosion Problem","text":""},{"location":"appendices/cnn-architecture/#why-fully-connected-networks-fail-for-images","title":"Why Fully Connected Networks Fail for Images","text":"<p>Consider a standard image classification task: - Input: 224 \u00d7 224 RGB image - Goal: Classify into 1000 categories</p> <p>If we flatten the image and connect to a hidden layer:</p> <pre><code>Input pixels: 224 \u00d7 224 \u00d7 3 = 150,528 features\nHidden layer: 1,000 neurons\nParameters: 150,528 \u00d7 1,000 = 150,528,000 (just the first layer!)\n</code></pre> <p>That's 150 million parameters for ONE layer!</p> <p>A typical network might have several hidden layers:</p> Layer Input Size Output Size Parameters FC1 150,528 4,096 616,562,688 FC2 4,096 4,096 16,781,312 FC3 4,096 1,000 4,097,000 Total 637,441,000 <p>637 million parameters just to process a 224\u00d7224 image!</p>"},{"location":"appendices/cnn-architecture/#the-problems-this-creates","title":"The Problems This Creates","text":"<ol> <li>Memory: Each parameter is 4 bytes (float32)</li> <li>637M params \u00d7 4 bytes = 2.5 GB just for weights</li> <li> <p>Plus gradients, optimizer states: 10+ GB</p> </li> <li> <p>Overfitting: More parameters than training examples</p> </li> <li>ImageNet has 1.2M training images</li> <li> <p>637M parameters would massively overfit</p> </li> <li> <p>Computation: Each forward pass multiplies huge matrices</p> </li> <li> <p>Billions of multiply-add operations per image</p> </li> <li> <p>No spatial understanding:</p> </li> <li>Pixel (0,0) and pixel (223,223) are equally \"distant\"</li> <li>A cat in the corner looks completely different than a cat in the center</li> </ol> <p>Numerical Example: FC vs CNN Parameter Comparison</p> <pre><code># Fully connected layer for 224\u00d7224\u00d73 image\ninput_size = 224 * 224 * 3  # 150,528\nfc_hidden = 1000\nfc_params = input_size * fc_hidden + fc_hidden  # 150,529,000\n\n# Convolutional layer: 64 filters, 3\u00d73 kernel\nconv_params = (3 * 3 * 3 * 64) + 64  # 1,792\n\nprint(f\"FC layer (1000 neurons): {fc_params:,} parameters\")\nprint(f\"Conv layer (64 3\u00d73):     {conv_params:,} parameters\")\nprint(f\"Ratio: {fc_params / conv_params:,.0f}\u00d7\")\n</code></pre> <p>Output: <pre><code>FC layer (1000 neurons): 150,529,000 parameters\nConv layer (64 3\u00d73):     1,792 parameters\nRatio: 84,001\u00d7\n</code></pre></p> <p>Interpretation: A single FC layer needs 84,000\u00d7 more parameters than a conv layer for the same input. This is why FC networks are impractical for images\u2014even one layer would have 150 million parameters.</p> <p>Source: <code>slide_computations/deep_dive_cnn_examples.py</code> - <code>demo_fc_vs_cnn_parameters()</code></p>"},{"location":"appendices/cnn-architecture/#the-three-principles-that-save-parameters","title":"The Three Principles That Save Parameters","text":"<p>CNNs achieve 99.9%+ parameter reduction through three key principles:</p>"},{"location":"appendices/cnn-architecture/#principle-1-local-connectivity-sparse-connections","title":"Principle 1: Local Connectivity (Sparse Connections)","text":"<p>The insight: Nearby pixels are related; distant pixels are less so.</p> <p>Instead of connecting every input pixel to every hidden neuron, each neuron only \"sees\" a small local region called the receptive field.</p> <p>Connectivity Comparison:</p> <p>Fully Connected: <pre><code>Every input pixel \u2192 Every hidden neuron\nConnections: 150,528 \u00d7 1,000 = 150,528,000\n</code></pre></p> <p>Locally Connected (3\u00d73 receptive field): <pre><code>Each hidden neuron sees only 3\u00d73\u00d73 = 27 input values\nConnections per neuron: 27\n</code></pre></p> <p></p> <p>Reading the diagram: The left side shows fully connected architecture where all 25 input pixels (5\u00d75 grid) connect to a single output neuron\u201425 connections total. The right side shows local connectivity where only the 9 orange pixels (3\u00d73 region) connect to the output\u2014a 64% reduction in connections. The orange highlighting shows the receptive field: the region of input that influences this particular output. In a fully connected layer, every output \"sees\" every input. In a locally connected layer, each output only \"sees\" its local neighborhood. This mirrors how your eye works\u2014you focus on a small region at a time, not the entire visual field simultaneously.</p> <p>This mirrors the visual cortex: neurons in V1 respond to small regions of the visual field. Hubel &amp; Wiesel won the Nobel Prize for discovering this.</p> <p>Brain inspiration: CNNs were directly inspired by neuroscience\u2014Hubel &amp; Wiesel showed visual cortex neurons respond to simple patterns in small regions. Other brain-inspired ideas: the neuron model (McCulloch-Pitts), ReLU (firing thresholds), dropout (neural noise), attention (selective focus). Caution warranted: backpropagation has no clear biological analog; transformers aren't obviously brain-like.</p>"},{"location":"appendices/cnn-architecture/#principle-2-weight-sharing-translation-equivariance","title":"Principle 2: Weight Sharing (Translation Equivariance)","text":"<p>The insight: The same pattern can appear anywhere in an image.</p> <p>A \"vertical edge detector\" should work whether the edge is in the top-left or bottom-right. Why learn separate detectors for each position?</p> <p>Without Weight Sharing (Locally Connected):</p> <p>If we had local connectivity but different weights at each position:</p> <pre><code>For 224\u00d7224 output with 3\u00d73 filters:\nPositions: 224 \u00d7 224 = 50,176\nParameters per position: 3 \u00d7 3 \u00d7 3 = 27\nTotal: 50,176 \u00d7 27 = 1,354,752 parameters (per filter)\n</code></pre> <p>Still a lot!</p> <p>With Weight Sharing (Convolutional):</p> <p>The same 3\u00d73\u00d73 filter is applied at every position:</p> <pre><code>Parameters: 3 \u00d7 3 \u00d7 3 + 1 (bias) = 28 parameters (per filter)\n</code></pre> <p>That's a 50,000\u00d7 reduction!</p> <p>The cookie cutter analogy: Think of a convolutional filter as a cookie cutter. Without weight sharing, you'd need to design 50,176 different cookie cutters\u2014one custom shape for every position on your baking sheet. With weight sharing, you use ONE cookie cutter and just move it around. The cookie cutter (filter weights) is the same everywhere; only its position changes. This is why a vertical edge detector learned in the top-left corner automatically works in the bottom-right corner\u2014it's literally the same detector, applied in a different location.</p> <p>Numerical Example: Weight Sharing Savings</p> <pre><code># Setup: 224\u00d7224 RGB image, 64 filters, 3\u00d73 kernel\npositions = 222 * 222  # output positions (no padding)\nparams_per_position = 3 * 3 * 3  # 27 weights per 3\u00d73\u00d73 filter\n\n# Locally connected: different weights at each position\nlocally_connected = positions * params_per_position * 64\n# = 85,162,752 parameters\n\n# Convolutional: same weights everywhere\nconvolutional = params_per_position * 64\n# = 1,728 parameters\n\nprint(f\"Locally connected: {locally_connected:,}\")\nprint(f\"Convolutional:     {convolutional:,}\")\nprint(f\"Reduction: {locally_connected / convolutional:,.0f}\u00d7\")\n</code></pre> <p>Output: <pre><code>Locally connected: 85,162,752\nConvolutional:     1,728\nReduction: 49,284\u00d7\n</code></pre></p> <p>Interpretation: Weight sharing reduces parameters by nearly 50,000\u00d7. The same 27 weights (one 3\u00d73\u00d73 filter) are reused at 49,284 different positions\u2014like using one cookie cutter 49,284 times instead of making 49,284 different cookie cutters.</p> <p>Source: <code>slide_computations/deep_dive_cnn_examples.py</code> - <code>demo_weight_sharing_savings()</code></p> <p>What This Enables: Translation Equivariance</p> <p>If you shift the input, the output shifts by the same amount:</p> \\[f(\\text{shift}(x)) = \\text{shift}(f(x))\\] <p>What this means in practice: Suppose your network learned a vertical edge detector. If there's a vertical edge at pixel position (10, 10), the detector fires and produces a strong activation at output position (10, 10). Now shift the entire image 100 pixels to the right. The edge is now at (10, 110). Because the same filter weights are used everywhere, the detector fires at output position (10, 110)\u2014the activation \"moved with\" the edge. This is translation equivariance: the output shifts when the input shifts. The network doesn't need to learn separate edge detectors for 50,000 different positions; one detector works everywhere.</p> <p>A cat detector fires whether the cat is at (10, 10) or (200, 200).</p>"},{"location":"appendices/cnn-architecture/#principle-3-hierarchical-feature-learning","title":"Principle 3: Hierarchical Feature Learning","text":"<p>The insight: Complex features are built from simpler ones.</p> <p>Through multiple layers, CNNs build a hierarchy:</p> Layer Depth Receptive Field What's Learned Example Layer 1 3\u00d73 Edges, colors Horizontal line, blue patch Layer 2 7\u00d77 Textures, corners Fur texture, eye corner Layer 3 15\u00d715 Parts Eye, ear, nose Layer 4 31\u00d731 Objects Cat face, dog face Layer 5 63\u00d763 Scenes Cat on couch <p>Effective Receptive Field Calculation:</p> <p>For a network with L layers of K\u00d7K convolutions:</p> \\[\\text{Receptive Field} = 1 + L \\times (K - 1)\\] <p>Example with 3\u00d73 convolutions: - After 1 layer: 1 + 1\u00d72 = 3\u00d73 - After 2 layers: 1 + 2\u00d72 = 5\u00d75 - After 5 layers: 1 + 5\u00d72 = 11\u00d711 - After 10 layers: 1 + 10\u00d72 = 21\u00d721</p> <p>With pooling (stride 2), receptive field grows much faster\u2014each pooling layer doubles the effective receptive field.</p> <p>Why receptive field matters: Think of what each layer \"sees.\" Layer 1 neurons have a 3\u00d73 receptive field\u2014they detect tiny patterns like single edges or color gradients. By layer 5, neurons have an 11\u00d711 receptive field\u2014now they can recognize textures, corners, and simple shapes. By layer 10, with pooling, neurons may have a 100+ pixel receptive field\u2014enough to see an entire eye, nose, or ear. This is why depth matters: shallow networks can only detect local patterns; deep networks can recognize objects by combining those patterns. A single 3\u00d73 filter can never see a cat; but many 3\u00d73 filters stacked together eventually can.</p> <p>Numerical Example: Receptive Field Growth</p> <pre><code># RF formula (no pooling): RF = 1 + L \u00d7 (K - 1)\n# Using 3\u00d73 convolutions (K=3)\n\nfor layers in [1, 2, 5, 10, 20]:\n    rf = 1 + layers * (3 - 1)\n    print(f\"After {layers:2} layers: RF = {rf}\u00d7{rf}\")\n</code></pre> <p>Output: <pre><code>After  1 layers: RF = 3\u00d73\nAfter  2 layers: RF = 5\u00d75\nAfter  5 layers: RF = 11\u00d711\nAfter 10 layers: RF = 21\u00d721\nAfter 20 layers: RF = 41\u00d741\n</code></pre></p> <p>Interpretation: Each 3\u00d73 layer adds 2 pixels to the receptive field. After 5 layers, a neuron can \"see\" an 11\u00d711 region of the input\u2014enough to detect texture patterns. After 20 layers (without pooling), RF reaches 41\u00d741. With pooling, RF grows much faster\u2014a typical VGG network reaches 200+ pixel receptive fields.</p> <p>Source: <code>slide_computations/deep_dive_cnn_examples.py</code> - <code>demo_receptive_field_growth()</code></p> <p>Seeing the whole object: The calculation above is without pooling\u2014practical networks reach 200+ pixel receptive fields via pooling. More importantly, the network doesn't need every neuron to see the whole object. Hierarchical structure means different neurons specialize at different scales. Global average pooling aggregates information across all positions, giving the classifier access to features detected anywhere.</p>"},{"location":"appendices/cnn-architecture/#the-savings-breakdown","title":"The Savings Breakdown","text":"<p>Let's compare a CNN to an equivalent fully connected network:</p>"},{"location":"appendices/cnn-architecture/#cnn-architecture-vgg-16-style-simplified","title":"CNN Architecture (VGG-16 style, simplified)","text":"Layer Output Shape Parameters Calculation Input 224\u00d7224\u00d73 0 - Conv1 (3\u00d73, 64) 224\u00d7224\u00d764 1,792 3\u00d73\u00d73\u00d764 + 64 Conv2 (3\u00d73, 64) 224\u00d7224\u00d764 36,928 3\u00d73\u00d764\u00d764 + 64 MaxPool 112\u00d7112\u00d764 0 - ... ... ... ... Flatten 25,088 0 7\u00d77\u00d7512 FC1 4,096 102,764,544 25,088\u00d74,096 + 4,096 Conv layers total ~9.5M FC layers total ~123M <p>The convolution layers have ~9.5 million parameters vs potential trillions for equivalent FC layers.</p> Principle Savings Factor Local Connectivity ~150,000\u00d7 (150,528 \u2192 ~27 connections) Weight Sharing ~50,000\u00d7 (one filter for all positions) Combined ~7,500,000,000\u00d7 (billions)"},{"location":"appendices/cnn-architecture/#why-convolution-works-for-images","title":"Why Convolution Works for Images","text":"<p>A concrete example: finding a cat in an image. Consider the three CNN design principles in action: - Spatial locality: To detect a cat's whisker, you only need to look at a few adjacent pixels. The whisker is defined by a local pattern of dark lines on a lighter background\u2014distant pixels don't matter. - Stationarity: Whiskers look similar whether they're on the left or right side of the image. Fur texture repeats across the cat's body. You don't need to learn \"left-side fur\" and \"right-side fur\" separately. - Compositionality: Whiskers + nose + eyes \u2192 face. Face + ears + body \u2192 cat. Complex objects are built from simpler parts. Early layers detect edges; middle layers combine edges into parts; deep layers recognize the whole cat.</p> <p>CNNs succeed because images naturally have these three properties. When these properties don't hold (e.g., satellite imagery where \"up\" means north), CNNs struggle.</p>"},{"location":"appendices/cnn-architecture/#property-1-spatial-locality","title":"Property 1: Spatial Locality","text":"<p>Nearby pixels are statistically related</p> <ul> <li>Adjacent pixels often have similar values (smooth regions)</li> <li>Edges are local phenomena (sharp transitions between neighbors)</li> <li>Texture is defined by local patterns</li> </ul> <p>This justifies local connectivity: Most useful information is local.</p>"},{"location":"appendices/cnn-architecture/#property-2-stationarity","title":"Property 2: Stationarity","text":"<p>The same patterns appear throughout the image</p> <ul> <li>Edges can occur anywhere</li> <li>Textures repeat across regions</li> <li>Objects can appear at any location</li> </ul> <p>This justifies weight sharing: One detector works everywhere.</p>"},{"location":"appendices/cnn-architecture/#property-3-compositionality","title":"Property 3: Compositionality","text":"<p>Complex patterns are built from simpler ones</p> <ul> <li>Eyes are made of edges, curves, and colors</li> <li>Faces are made of eyes, nose, mouth</li> <li>Scenes are made of objects</li> </ul> <p>This justifies hierarchical layers: Build complexity gradually.</p>"},{"location":"appendices/cnn-architecture/#when-convolution-doesnt-work-well","title":"When Convolution Doesn't Work Well","text":"<p>CNNs struggle when these properties don't hold:</p> <ol> <li>Absolute position matters:</li> <li>Satellite imagery where \"top\" means north</li> <li>Document layouts where header is always at top</li> <li> <p>Solution: Add positional encoding or coordinates</p> </li> <li> <p>Global context needed immediately:</p> </li> <li>Recognizing objects from single distinctive features</li> <li>Tasks requiring reasoning about entire image at once</li> <li> <p>Solution: Attention mechanisms, Vision Transformers</p> </li> <li> <p>Non-grid data:</p> </li> <li>Graphs, point clouds, irregular meshes</li> <li>Solution: Graph neural networks, PointNet</li> </ol>"},{"location":"appendices/cnn-architecture/#the-convolution-operation-in-depth","title":"The Convolution Operation In-Depth","text":"<p>Let's trace through a convolution with concrete numbers.</p>"},{"location":"appendices/cnn-architecture/#setup","title":"Setup","text":"<ul> <li>Input: 1 image, 3 channels (RGB), 8\u00d78 pixels</li> <li> <p>Shape: <code>(1, 3, 8, 8)</code> - (batch, channels, height, width)</p> </li> <li> <p>Filter: 1 filter, 3\u00d73 kernel</p> </li> <li> <p>Shape: <code>(1, 3, 3, 3)</code> - (out_channels, in_channels, kernel_h, kernel_w)</p> </li> <li> <p>Output: 1 feature map, 6\u00d76</p> </li> <li>Shape: <code>(1, 1, 6, 6)</code> - (batch, out_channels, height, width)</li> </ul>"},{"location":"appendices/cnn-architecture/#the-sliding-window-process","title":"The Sliding Window Process","text":"<p>In plain English: At each output position, the filter performs three steps: 1. Multiply: Overlay the filter on the input patch and multiply each filter weight by the corresponding input value 2. Sum: Add up all those products across all channels (R, G, B) and all spatial positions (3\u00d73) 3. Slide: Move to the next position and repeat</p> <p>The result is a single number at each output location. This number is large and positive when the input patch \"matches\" the filter pattern, near zero when there's no match, and large and negative when the pattern is inverted.</p> <p>For each position (i, j) in the output:</p> \\[\\text{output}[0, 0, i, j] = \\sum_{c=0}^{2} \\sum_{m=0}^{2} \\sum_{n=0}^{2} \\text{filter}[0, c, m, n] \\times \\text{input}[0, c, i+m, j+n] + \\text{bias}[0]\\]"},{"location":"appendices/cnn-architecture/#concrete-example","title":"Concrete Example","text":"<p>Let's compute one output value at position (0, 0):</p> <p>Input patch (top-left 3\u00d73 of each channel): <pre><code>Red channel:        Green channel:      Blue channel:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 0.1 0.2 0.3 \u2502     \u2502 0.4 0.5 0.6 \u2502     \u2502 0.7 0.8 0.9 \u2502\n\u2502 0.2 0.3 0.4 \u2502     \u2502 0.5 0.6 0.7 \u2502     \u2502 0.8 0.9 1.0 \u2502\n\u2502 0.3 0.4 0.5 \u2502     \u2502 0.6 0.7 0.8 \u2502     \u2502 0.9 1.0 1.1 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <p>Filter weights (Sobel-like vertical edge detector): <pre><code>Red weights:        Green weights:      Blue weights:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 1   0  -1   \u2502     \u2502 1   0  -1   \u2502     \u2502 1   0  -1   \u2502\n\u2502 2   0  -2   \u2502     \u2502 2   0  -2   \u2502     \u2502 2   0  -2   \u2502\n\u2502 1   0  -1   \u2502     \u2502 1   0  -1   \u2502     \u2502 1   0  -1   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <p>Computation: <pre><code># Red channel contribution\nred = (0.1\u00d71 + 0.2\u00d70 + 0.3\u00d7(-1) +\n       0.2\u00d72 + 0.3\u00d70 + 0.4\u00d7(-2) +\n       0.3\u00d71 + 0.4\u00d70 + 0.5\u00d7(-1))\n    = 0.1 - 0.3 + 0.4 - 0.8 + 0.3 - 0.5\n    = -0.8\n\n# Similarly for green and blue...\n# Total (sum across channels) + bias\noutput[0,0,0,0] = red + green + blue + bias = -2.4\n</code></pre></p> <p>The filter slides across all valid positions to fill the 6\u00d76 output.</p> <p>Numerical Example: Convolution Verification</p> <pre><code>import numpy as np\nimport torch.nn as nn\n\n# Input patch (from document)\nred = np.array([[0.1, 0.2, 0.3], [0.2, 0.3, 0.4], [0.3, 0.4, 0.5]])\ngreen = np.array([[0.4, 0.5, 0.6], [0.5, 0.6, 0.7], [0.6, 0.7, 0.8]])\nblue = np.array([[0.7, 0.8, 0.9], [0.8, 0.9, 1.0], [0.9, 1.0, 1.1]])\n\n# Sobel vertical edge detector\nsobel = np.array([[1, 0, -1], [2, 0, -2], [1, 0, -1]])\n\n# Manual computation\nred_contrib = np.sum(red * sobel)    # -0.8\ngreen_contrib = np.sum(green * sobel) # -0.8\nblue_contrib = np.sum(blue * sobel)   # -0.8\ntotal = red_contrib + green_contrib + blue_contrib  # -2.4\n\nprint(f\"Red channel:   {red_contrib:.1f}\")\nprint(f\"Green channel: {green_contrib:.1f}\")\nprint(f\"Blue channel:  {blue_contrib:.1f}\")\nprint(f\"Total:         {total:.1f}\")\n</code></pre> <p>Output: <pre><code>Red channel:   -0.8\nGreen channel: -0.8\nBlue channel:  -0.8\nTotal:         -2.4\n</code></pre></p> <p>Interpretation: The Sobel filter produces -0.8 for each channel, totaling -2.4. A negative value indicates the input patch has the opposite of the pattern the filter detects (here: a leftward gradient instead of rightward). PyTorch's <code>nn.Conv2d</code> produces the identical result, confirming our manual calculation.</p> <p>Source: <code>slide_computations/deep_dive_cnn_examples.py</code> - <code>demo_convolution_verification()</code></p>"},{"location":"appendices/cnn-architecture/#output-size-formula","title":"Output Size Formula","text":"\\[\\text{output\\_size} = \\left\\lfloor \\frac{\\text{input\\_size} - \\text{kernel\\_size} + 2 \\times \\text{padding}}{\\text{stride}} \\right\\rfloor + 1\\] <p>For our example:</p> \\[\\frac{8 - 3 + 2 \\times 0}{1} + 1 = 6\\] <p>Numerical Example: Output Size Formula</p> <pre><code># Formula: output = floor((input - kernel + 2\u00d7padding) / stride) + 1\n\nconfigs = [\n    (8, 3, 0, 1, \"Basic (from document)\"),\n    (224, 3, 1, 1, \"Same padding\"),\n    (224, 3, 0, 1, \"No padding\"),\n    (224, 3, 0, 2, \"Stride 2\"),\n    (224, 7, 3, 2, \"7\u00d77 kernel (AlexNet)\"),\n]\n\nfor inp, k, p, s, desc in configs:\n    output = (inp - k + 2*p) // s + 1\n    print(f\"{desc:&lt;25} ({inp}-{k}+2\u00d7{p})/{s}+1 = {output}\")\n</code></pre> <p>Output: <pre><code>Basic (from document)     (8-3+2\u00d70)/1+1 = 6\nSame padding              (224-3+2\u00d71)/1+1 = 224\nNo padding                (224-3+2\u00d70)/1+1 = 222\nStride 2                  (224-3+2\u00d70)/2+1 = 111\n7\u00d77 kernel (AlexNet)      (224-7+2\u00d73)/2+1 = 112\n</code></pre></p> <p>Interpretation: With padding=1 and stride=1, output equals input size (\"same\" padding). Without padding, each layer shrinks the output by (kernel-1) pixels. Stride 2 halves spatial dimensions\u2014often used instead of pooling. The 7\u00d77 kernel with stride 2 is AlexNet's first layer, reducing 224\u2192112.</p> <p>Source: <code>slide_computations/deep_dive_cnn_examples.py</code> - <code>demo_output_size_formula()</code></p>"},{"location":"appendices/cnn-architecture/#annotated-nnconv2d-walkthrough","title":"Annotated nn.Conv2d Walkthrough","text":"<pre><code>import torch\nimport torch.nn as nn\n\n# Create a convolutional layer\nconv = nn.Conv2d(\n    in_channels=3,      # RGB input\n    out_channels=64,    # Number of filters (feature maps)\n    kernel_size=3,      # 3\u00d73 filter\n    stride=1,           # Move 1 pixel at a time\n    padding=1,          # Add 1 pixel border of zeros\n    bias=True           # Include bias term\n)\n\n# WHERE THE PARAMETERS LIVE\nprint(f\"Weight shape: {conv.weight.shape}\")\n# Output: torch.Size([64, 3, 3, 3])\n# Interpretation: [out_channels, in_channels, kernel_height, kernel_width]\n# - 64 different filters\n# - Each filter has 3 channels (one for R, G, B)\n# - Each channel is 3\u00d73\n\nprint(f\"Weight parameters: {conv.weight.numel()}\")\n# Output: 1728\n# Calculation: 64 \u00d7 3 \u00d7 3 \u00d7 3 = 1,728\n\nprint(f\"Bias shape: {conv.bias.shape}\")\n# Output: torch.Size([64])\n# One bias per output channel (filter)\n\nprint(f\"Total parameters: {sum(p.numel() for p in conv.parameters())}\")\n# Output: 1792\n# = 1,728 (weights) + 64 (biases)\n</code></pre>"},{"location":"appendices/cnn-architecture/#understanding-the-weight-tensor","title":"Understanding the Weight Tensor","text":"<pre><code># Access filter 0 (first of 64 filters)\nfilter_0 = conv.weight[0]  # Shape: [3, 3, 3]\n\n# Filter 0's red channel weights\nfilter_0_red = conv.weight[0, 0]  # Shape: [3, 3]\n\n# Filter 0's green channel weights\nfilter_0_green = conv.weight[0, 1]  # Shape: [3, 3]\n\n# Filter 0's blue channel weights\nfilter_0_blue = conv.weight[0, 2]  # Shape: [3, 3]\n</code></pre>"},{"location":"appendices/cnn-architecture/#visualizing-parameter-sharing","title":"Visualizing Parameter Sharing","text":"<pre><code># The SAME weights are used for EVERY spatial position\n# This is what makes CNNs parameter-efficient\n\n# These two output positions use the SAME filter weights:\n# y[0, 0, 0, 0] uses conv.weight[0] applied at position (0,0)\n# y[0, 0, 5, 5] uses conv.weight[0] applied at position (5,5)\n\n# Without weight sharing, we'd need:\n# 64 filters \u00d7 8\u00d78 positions \u00d7 3\u00d73\u00d73 weights = 884,736 parameters\n# With weight sharing:\n# 64 filters \u00d7 3\u00d73\u00d73 weights = 1,728 parameters\n# That's a 512\u00d7 reduction!\n</code></pre>"},{"location":"appendices/cnn-architecture/#pooling-trading-resolution-for-invariance","title":"Pooling: Trading Resolution for Invariance","text":""},{"location":"appendices/cnn-architecture/#what-max-pooling-does","title":"What Max Pooling Does","text":"<p>Max pooling takes the maximum value in each patch:</p> <p></p> <p>Reading the diagram: The left shows a 4\u00d74 input grid with numerical values; the orange lines divide it into four 2\u00d72 regions. The right shows the 2\u00d72 output after max pooling. For each 2\u00d72 region, we take the maximum value: the top-left region contains [1,3,0,2] \u2192 max is 3; top-right [2,4,1,3] \u2192 max is 4; bottom-left [5,6,4,1] \u2192 max is 6; bottom-right [7,8,5,2] \u2192 max is 8. Notice the output is exactly half the size in each dimension (4\u00d74 \u2192 2\u00d72), reducing the data by 4\u00d7. The color intensity in the output corresponds to the value magnitude.</p>"},{"location":"appendices/cnn-architecture/#why-max-pooling-works","title":"Why Max Pooling Works","text":"<ol> <li>Translation invariance: The feature's exact position within the pooling region doesn't matter</li> <li>Dimensionality reduction: Halves spatial dimensions, reducing computation</li> <li>Increased receptive field: Each pooled output sees a larger input region</li> <li>Slight noise robustness: Small perturbations don't change the max</li> </ol> <p>Understanding each benefit: - Translation invariance: If an edge is at pixel (10,10) or (11,11), both fall in the same pooling region\u2014the output is the same. This makes the network less sensitive to exact feature positions. - Dimensionality reduction: A 224\u00d7224 feature map becomes 112\u00d7112 after one pool\u20144\u00d7 fewer values to process in subsequent layers. - Increased receptive field: Each output pixel in the pooled layer corresponds to a 2\u00d72 region of input. After pooling, a 3\u00d73 convolution on the pooled output effectively \"sees\" a 6\u00d76 region of the original input. - Noise robustness: If pixel values are [4.9, 5.0, 5.1, 5.2], the max is 5.2. Add small noise, get [4.8, 5.1, 5.0, 5.3]\u2014max is 5.3. The change is minimal compared to the signal.</p>"},{"location":"appendices/cnn-architecture/#pooling-vs-stride-trade-off","title":"Pooling vs Stride Trade-off","text":"<p>Modern networks often use strided convolutions instead of pooling:</p> Approach Operation Parameters Learned Max Pool Take max 0 No Avg Pool Take mean 0 No Strided Conv Conv with stride 2 kernel\u00b2 \u00d7 channels\u00b2 Yes <p>Strided convolution can learn what information to preserve, but requires more parameters.</p> <p>Numerical Example: Pooling Dimension Reduction</p> <pre><code>import torch\nimport torch.nn as nn\n\nx = torch.randn(1, 64, 224, 224)  # Starting: 224\u00d7224\u00d764\npool = nn.MaxPool2d(kernel_size=2, stride=2)\n\nprint(f\"Input:       {x.shape}\")\nfor i in range(5):\n    x = pool(x)\n    print(f\"After pool {i+1}: {x.shape}\")\n</code></pre> <p>Output: <pre><code>Input:       torch.Size([1, 64, 224, 224])\nAfter pool 1: torch.Size([1, 64, 112, 112])\nAfter pool 2: torch.Size([1, 64, 56, 56])\nAfter pool 3: torch.Size([1, 64, 28, 28])\nAfter pool 4: torch.Size([1, 64, 14, 14])\nAfter pool 5: torch.Size([1, 64, 7, 7])\n</code></pre></p> <p>Interpretation: Each 2\u00d72 max pool halves spatial dimensions: 224\u2192112\u219256\u219228\u219214\u21927. After 5 pooling layers, we've reduced 224\u00d7224=50,176 pixels to 7\u00d77=49 pixels\u2014a 1,024\u00d7 reduction in spatial data. The channel count (64) stays the same. This is why VGG-style networks can have huge 7\u00d77\u00d7512 feature maps at the end: the spatial dimensions have been aggressively reduced.</p> <p>Source: <code>slide_computations/deep_dive_cnn_examples.py</code> - <code>demo_pooling_dimensions()</code></p>"},{"location":"appendices/cnn-architecture/#common-misconceptions","title":"Common Misconceptions","text":"Misconception Reality \"CNNs learn the filters by hand\" Filters are learned automatically through backpropagation, not designed by humans \"More filters is always better\" Diminishing returns; excess filters learn redundant or noise patterns \"CNNs work on any image task\" CNNs assume spatial locality and translation equivariance; they struggle when these don't hold \"The first layer must have 3 input channels\" Input channels match your data: 1 for grayscale, 3 for RGB, any number for multi-spectral \"Pooling is necessary\" Modern architectures often use strided convolutions instead \"Deeper is always better\" Without residual connections, very deep CNNs fail to train"},{"location":"appendices/cnn-architecture/#reflection-questions","title":"Reflection Questions","text":"<ol> <li> <p>If we didn't share weights across positions, how many parameters would a 3\u00d73 conv layer have for a 224\u00d7224 image with 64 filters?</p> </li> <li> <p>Why might a CNN struggle with satellite imagery where \"up\" always means north?</p> </li> <li> <p>How does the receptive field grow through layers? Why does this matter?</p> </li> <li> <p>A 7\u00d77 conv has 49 weights per channel. Two stacked 3\u00d73 convs have only 18 weights but achieve the same receptive field. What's the trade-off?</p> </li> <li> <p>What happens if you set padding=0 with a 3\u00d73 kernel on a 224\u00d7224 input?</p> </li> <li> <p>Why do we typically increase the number of filters as we go deeper while decreasing spatial size?</p> </li> </ol>"},{"location":"appendices/cnn-architecture/#practice-problems","title":"Practice Problems","text":"<ol> <li> <p>Calculate parameters for VGG-16 from scratch</p> </li> <li> <p>Compute output size for: input 64\u00d764, kernel 5\u00d75, stride 2, padding 2</p> </li> <li> <p>Design a CNN for 64\u00d764 grayscale images with target receptive field of 32\u00d732</p> </li> </ol>"},{"location":"appendices/cnn-architecture/#summary","title":"Summary","text":"<p>Key takeaways:</p> <ol> <li> <p>Local connectivity reduces connections from millions to dozens per neuron</p> </li> <li> <p>Weight sharing reduces parameters by 50,000\u00d7 for a single filter</p> </li> <li> <p>Hierarchical learning builds complex features from simple ones</p> </li> <li> <p>These principles work because images have spatial locality, stationarity, and compositionality</p> </li> <li> <p>The total savings is on the order of billions of parameters</p> </li> <li> <p>Understanding where parameters live helps you reason about model capacity and design</p> </li> </ol>"},{"location":"appendices/surprising-phenomena/","title":"Deep Dive: Surprising Phenomena in Modern Deep Learning","text":"<p>Extends Module 6: Neural Networks</p>"},{"location":"appendices/surprising-phenomena/#introduction","title":"Introduction","text":"<p>In Module 6, we learned how neural networks work: layers of neurons, activation functions, backpropagation, and gradient descent. We also learned the classical story of model complexity\u2014that there's a \"sweet spot\" between underfitting and overfitting, captured by the bias-variance tradeoff.</p> <p>But modern deep learning doesn't quite follow that script.</p> <p>Over the past several years, researchers have discovered phenomena that challenge our classical understanding of how machine learning works. Neural networks with billions of parameters don't overfit the way theory predicts. Models sometimes learn to generalize long after they've memorized their training data. And capabilities can appear suddenly at scale, rather than improving gradually.</p> <p>These aren't just academic curiosities\u2014they affect practical decisions about model selection, training duration, and when to trust small-scale experiments.</p> <p>This deep dive explores three such phenomena: 1. Double Descent: Why more parameters can actually reduce overfitting 2. Grokking: Why generalization can occur long after memorization 3. Emergent Abilities: Why capabilities can appear suddenly at scale</p> <p>Why this matters for practitioners: These phenomena suggest that intuitions built on classical ML may mislead you when working with modern neural networks. Understanding them helps you make better decisions about model size, training time, and when to trust (or distrust) your experiments.</p>"},{"location":"appendices/surprising-phenomena/#the-classical-view-a-brief-recap","title":"The Classical View: A Brief Recap","text":"<p>Before exploring what's surprising, let's recall what we expect.</p>"},{"location":"appendices/surprising-phenomena/#the-bias-variance-tradeoff","title":"The Bias-Variance Tradeoff","text":"<p>Classical machine learning theory tells us that prediction error has two components:</p> \\[\\text{Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Noise}\\] <ul> <li>Bias: Error from overly simple models that can't capture the true pattern</li> <li>Variance: Error from models that are too sensitive to training data</li> </ul> <p>As model complexity increases: - Bias decreases (the model can fit more patterns) - Variance increases (the model becomes more sensitive to noise)</p> <p>This creates the famous U-shaped curve:</p> <pre><code>Test Error\n    \u2502\n    \u2502    *\n    \u2502      *\n    \u2502        *       *   *   *\n    \u2502          *   *           *  *  *\n    \u2502            *\n    \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n              Model Complexity \u2192\n         Underfitting  \u2190\u2192  Overfitting\n</code></pre> <p>Reading the diagram: The vertical axis shows test error (higher is worse), and the horizontal axis shows model complexity (number of parameters, polynomial degree, tree depth, etc.). The asterisks trace a U-shaped curve: error is high on the left (models too simple to capture the pattern), decreases to a minimum in the middle (the \"sweet spot\"), then increases again on the right (models so complex they memorize noise). The labels below the axis remind us that the left side corresponds to underfitting (high bias) and the right side to overfitting (high variance). Classical ML wisdom says: find the bottom of the U and stop there.</p> <p>Numerical Example: Bias-Variance Tradeoff with Polynomial Regression</p> <pre><code>import numpy as np\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# True function: quadratic with noise\nnp.random.seed(42)\nX = np.random.uniform(-3, 3, 50).reshape(-1, 1)\ny_true = 0.5 * X.ravel()**2 - X.ravel() + 1\ny = y_true + np.random.randn(50) * 0.5\n\n# Split and fit polynomials of increasing degree\nX_train, X_test = X[:35], X[35:]\ny_train, y_test = y[:35], y[35:]\n\nfor degree in [1, 2, 3, 5, 8, 12, 15]:\n    poly = PolynomialFeatures(degree=degree)\n    X_train_poly = poly.fit_transform(X_train)\n    X_test_poly = poly.transform(X_test)\n    model = LinearRegression().fit(X_train_poly, y_train)\n    train_mse = mean_squared_error(y_train, model.predict(X_train_poly))\n    test_mse = mean_squared_error(y_test, model.predict(X_test_poly))\n    # Record results...\n</code></pre> <p>Output: <pre><code>Degree     Train MSE    Test MSE     Regime\n------------------------------------------------------\n1          2.1182       1.8187       Underfitting (high bias)\n2          0.2128       0.1968       Sweet spot\n3          0.2109       0.1974       Slight overfitting\n5          0.2050       0.1808       Slight overfitting\n8          0.1592       0.2805       Overfitting (high var)\n12         0.1267       0.3458       Overfitting (high var)\n15         0.1249       0.3610       Overfitting (high var)\n</code></pre></p> <p>Interpretation: Test error follows the U-curve. Degree 2 (matching the true quadratic function) achieves the lowest test error. Higher degrees drive training error down but test error up\u2014the classic signature of overfitting.</p> <p>Source: <code>slide_computations/deep_dive_surprising_phenomena_examples.py</code> - <code>demo_bias_variance_tradeoff()</code></p> <p>The classical prescription: Find the sweet spot. Don't make your model too simple (high bias) or too complex (high variance).</p>"},{"location":"appendices/surprising-phenomena/#early-stopping","title":"Early Stopping","text":"<p>A related principle: stop training when validation loss starts increasing. If you keep training after that point, you're just memorizing noise.</p> <pre><code>Loss\n    \u2502\n    \u2502 *\n    \u2502   *  Training Loss\n    \u2502     *\n    \u2502       *  *  *  *  *  *  *  *\n    \u2502\n    \u2502   *      Validation Loss\n    \u2502     *  *\n    \u2502           *\n    \u2502              *  *  *  *  *  *  \u2190 Stop here!\n    \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                  Epochs \u2192\n</code></pre> <p>Reading the diagram: Two curves show how training and validation loss evolve as training progresses (epochs increase to the right). The upper curve (training loss) drops quickly and then flattens\u2014the model fits the training data well. The lower curve (validation loss) initially drops in parallel, but then starts increasing while training loss stays flat. This divergence is the signature of overfitting: the model is memorizing training-specific noise rather than learning generalizable patterns. The arrow marks where validation loss bottoms out\u2014the classical prescription says to stop training here and use this model.</p> <p>The classical prescription: Monitor validation loss and stop when it starts increasing.</p> <p>These principles served us well for decades. But modern deep learning has revealed their limitations.</p>"},{"location":"appendices/surprising-phenomena/#phenomenon-1-double-descent","title":"Phenomenon 1: Double Descent","text":""},{"location":"appendices/surprising-phenomena/#the-discovery","title":"The Discovery","text":"<p>In 2019, Mikhail Belkin and colleagues published a paper that reconciled a puzzling observation: modern deep learning practitioners were using models with far more parameters than classical theory suggested\u2014and getting better results, not worse.</p> <p>They showed that if you keep increasing model complexity past the \"interpolation threshold\" (the point where the model has just enough capacity to perfectly fit the training data), test error can decrease again.</p> <p>Key paper: Belkin et al. (2019), \"Reconciling modern machine learning practice and the bias-variance trade-off\"</p>"},{"location":"appendices/surprising-phenomena/#visualizing-double-descent","title":"Visualizing Double Descent","text":"<p>The phenomenon looks like this:</p> <pre><code>Test Error\n    \u2502\n    \u2502  *\n    \u2502    *                                  Classical U-curve\n    \u2502      *                               /\n    \u2502        *       *                    /\n    \u2502          *   *   *                 /\n    \u2502            *       *     *        \u2190 Interpolation threshold\n    \u2502                      *      *\n    \u2502                          *    *  *  \u2190 Second descent!\n    \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n              Model Complexity \u2192\n\n    |-- Underparameterized --|-- Overparameterized --|\n</code></pre> <p>Reading the diagram: This extends the classical U-curve to much higher model complexity. Moving left to right: test error initially decreases (classical improvement), reaches a minimum (the classical sweet spot), then increases toward a peak at the \"interpolation threshold\" (marked with an arrow). This threshold is where the model has exactly enough parameters to perfectly fit the training data. But the key surprise is what happens after this peak: as we keep adding parameters, test error decreases again\u2014the \"second descent.\" The bottom labels divide the x-axis into two regimes: underparameterized (classical territory) and overparameterized (modern deep learning territory). The annotation shows that classical theory (dashed line) would predict continued increase, but reality shows descent.</p> <p>Three regimes:</p> <ol> <li> <p>Underparameterized (classical regime): Model has fewer parameters than needed to fit the data. Test error follows the classical U-curve.</p> </li> <li> <p>Interpolation threshold: Model has exactly enough parameters to fit the training data perfectly. Test error often peaks here\u2014the model fits the noise.</p> </li> <li> <p>Overparameterized (modern regime): Model has far more parameters than data points. Surprisingly, test error decreases again.</p> </li> </ol>"},{"location":"appendices/surprising-phenomena/#why-does-this-happen","title":"Why Does This Happen?","text":"<p>The key insight is that there are many ways to interpolate (perfectly fit) training data when you have excess capacity. Not all interpolating solutions are equal.</p> <p>Implicit regularization from SGD: Stochastic gradient descent doesn't just find any solution\u2014it tends to find solutions with certain properties: - Flatter loss landscape (better generalization) - Smaller weight norms - Simpler decision boundaries</p> <p>The \"benign overfitting\" concept: When you have many more parameters than data points, the model can fit both the signal and the noise\u2014but the noise fits get \"diluted\" across many parameters, so they don't dominate predictions on new data.</p> <p>Analogy: Imagine fitting a curve through 10 points. With exactly 10 parameters, you're forced to fit every point exactly\u2014including the noise. But with 1,000 parameters, you have many ways to fit those points. SGD tends to find the \"smoothest\" solution, which often generalizes better.</p> <p>The \"Many Roads\" Intuition</p> <p>Think of it this way: you need to get from A to B (fit the training data). At the interpolation threshold, there's exactly one road\u2014you must take it, and it goes through every muddy patch (noise) along the way. But in the overparameterized regime, there are thousands of roads. SGD naturally tends toward the widest, smoothest highways rather than the narrow, winding paths. The smooth highways generalize better because they don't encode every bump and pothole (noise) in the training data. This is why more parameters can actually help: more roads means SGD can be more selective.</p> <p>Numerical Example: Double Descent in Random Features</p> <pre><code>import numpy as np\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\n\n# Setup: 100 training points, varying number of random features\nn_train, n_test = 100, 1000\nnp.random.seed(42)\n\n# True function: y = sin(x) + noise\nX_train = np.random.randn(n_train, 1)\ny_train = np.sin(3 * X_train.ravel()) + 0.3 * np.random.randn(n_train)\nX_test = np.random.randn(n_test, 1)\ny_test = np.sin(3 * X_test.ravel())\n\n# Random Fourier features to increase complexity\ndef random_features(X, n_features, seed=0):\n    np.random.seed(seed)\n    W = np.random.randn(X.shape[1], n_features)\n    b = np.random.uniform(0, 2*np.pi, n_features)\n    return np.cos(X @ W + b)\n\n# Vary number of features from 10 to 2000\nfeature_counts = [10, 50, 80, 100, 120, 200, 500, 1000, 2000]\n# Fit ridge regression with tiny regularization\nfor n_feat in feature_counts:\n    Phi_train = random_features(X_train, n_feat)\n    Phi_test = random_features(X_test, n_feat)\n    model = Ridge(alpha=1e-8)\n    model.fit(Phi_train, y_train)\n    test_mse = mean_squared_error(y_test, model.predict(Phi_test))\n    # Record test MSE...\n</code></pre> <p>Output: <pre><code>  Features    Ratio to n    Test MSE         Regime\n--------------------------------------------------------\n        10         0.10       0.342      Underparameterized\n        50         0.50       0.089      Underparameterized\n        80         0.80       0.052      Underparameterized\n       100         1.00       0.487      Interpolation peak!\n       120         1.20       0.156      Just overparameterized\n       200         2.00       0.067      Overparameterized\n       500         5.00       0.041      Overparameterized\n      1000        10.00       0.038      Overparameterized\n      2000        20.00       0.035      Overparameterized\n</code></pre></p> <p>Interpretation: Test error peaks at 100 features (the interpolation threshold, where features = samples). As we add more features, error decreases\u2014the second descent. With 2000 features (20\u00d7 the data), we get lower error than the classical sweet spot.</p> <p>Source: <code>slide_computations/deep_dive_surprising_phenomena_examples.py</code> - <code>demo_double_descent_random_features()</code></p> <p>Numerical Example: The Interpolation Threshold Up Close</p> <pre><code># Zoom in around n_features = n_samples to see the peak\nn_train = 50\n# ... setup similar to above ...\n\nfor n_feat in [30, 40, 45, 48, 50, 52, 55, 60, 80, 100]:\n    # Fit model with n_feat features\n    ratio = n_feat / n_train\n    # ... compute test MSE ...\n</code></pre> <p>Output: <pre><code>Features   Ratio      Test MSE     Note\n--------------------------------------------------------------\n30         0.60       1.2311\n40         0.80       3.6692\n45         0.90       6.1485       Near threshold\n48         0.96       6.6199       Near threshold\n50         1.00       4.9464       &lt;&lt;&lt; PEAK: exactly n features\n52         1.04       3.9456       Near threshold\n55         1.10       2.8352       Descending...\n60         1.20       2.6002       Descending...\n80         1.60       5.5102       Descending...\n100        2.00       1.6794       Descending...\n</code></pre></p> <p>Interpretation: The peak in test error occurs right at or near the interpolation threshold (features = samples). Even adding just 2-5 extra features past the threshold starts the descent. This is why \"barely enough capacity\" is dangerous\u2014you're forced to fit every data point exactly, including the noise.</p> <p>Source: <code>slide_computations/deep_dive_surprising_phenomena_examples.py</code> - <code>demo_interpolation_threshold_peak()</code></p>"},{"location":"appendices/surprising-phenomena/#practical-implications","title":"Practical Implications","text":"<ol> <li> <p>Don't fear large models: If you have enough data, a larger model might generalize better, not worse.</p> </li> <li> <p>The interpolation threshold is dangerous: Having just barely enough capacity to fit the data is often the worst regime.</p> </li> <li> <p>Implicit regularization matters: How you train (SGD vs. exact solutions) affects which interpolating solution you find.</p> </li> <li> <p>But regularization still helps: Even in the overparameterized regime, explicit regularization (dropout, weight decay) often improves results further.</p> </li> </ol>"},{"location":"appendices/surprising-phenomena/#phenomenon-2-grokking","title":"Phenomenon 2: Grokking","text":""},{"location":"appendices/surprising-phenomena/#the-discovery_1","title":"The Discovery","text":"<p>In 2022, researchers at OpenAI discovered something unexpected while training neural networks on simple algorithmic tasks like modular arithmetic.</p> <p>They observed networks that: 1. Quickly achieved perfect training accuracy (memorization) 2. Showed no improvement in test accuracy for thousands of epochs 3. Then suddenly achieved perfect test accuracy (generalization)</p> <p>They called this phenomenon grokking.</p> <p>Key paper: Power et al. (2022), \"Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets\"</p>"},{"location":"appendices/surprising-phenomena/#visualizing-grokking","title":"Visualizing Grokking","text":"<pre><code>Accuracy\n   100%\u2502              Training \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       \u2502         ************************************\n       \u2502       **\n       \u2502     **\n       \u2502   **\n    50%\u2502  *\n       \u2502  \u2502\n       \u2502  \u2502                              Test\n       \u2502  \u2502                         ********\n       \u2502  \u2502                    *****\n       \u2502  \u2502             *******\n     0%\u2502**\u2502*************\u2502\n       \u2514\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n          \u2502             \u2502\n       Memorization   Grokking!\n         (fast)       (delayed)\n\n       Epoch 0    1000   10000   20000   30000\n</code></pre> <p>Reading the diagram: The vertical axis shows accuracy (0-100%), and the horizontal axis shows training epochs on a log-like scale (note the jump from 1,000 to 30,000). Two curves tell dramatically different stories. The training curve (upper) shoots up quickly\u2014by epoch 100-500, the network achieves 100% training accuracy. It then stays flat at 100% forever. The test curve (lower) starts near 0% and stays there for thousands of epochs (the flat line on the bottom). Then, around epoch 10,000-20,000, it suddenly climbs to 100%. The vertical dashed lines mark two events: \"Memorization\" (when training accuracy hits 100%) and \"Grokking!\" (when test accuracy finally catches up). The gap between these events\u2014thousands of epochs\u2014is the puzzle.</p> <p>The network memorizes quickly but generalizes much later\u2014sometimes 100\u00d7 as many epochs.</p>"},{"location":"appendices/surprising-phenomena/#why-does-this-happen_1","title":"Why Does This Happen?","text":"<p>The leading explanation involves the competition between memorization circuits and generalization circuits in the network.</p> <p>Memorization is easy: The network can quickly learn to store input-output pairs as a lookup table. This requires little structure\u2014just associate each input with its output.</p> <p>Generalization requires structure: To generalize, the network must learn the underlying rule (e.g., how modular arithmetic works). This requires discovering and encoding the mathematical structure.</p> <p>Weight decay tips the balance: Without regularization, the network stops at memorization\u2014it works, so gradients are small. But weight decay keeps pushing the weights toward simpler solutions. Eventually, the simpler generalizing solution becomes preferable.</p> <p>The circuit formation hypothesis: Research in mechanistic interpretability suggests that during the grokking transition, networks form specific circuits that implement the underlying algorithm, replacing the memorization lookup.</p> <p>The \"Parallel Construction\" Intuition</p> <p>Imagine two teams racing to solve the same problem. Team Memorization works fast: they just build a giant lookup table, matching each input to its output. Done in a few epochs! Team Generalization works slowly: they're trying to discover the underlying mathematical rule, which is harder. If there's no pressure to economize, Team Memorization wins and we're stuck with a lookup table forever. But weight decay acts like a tax on resources: the bigger your solution, the more you pay. Team Memorization's huge lookup table becomes expensive to maintain. Team Generalization's compact rule-based solution, though slower to build, eventually becomes cheaper. Around epoch 10,000-20,000, the tax makes the compact solution preferable, and the network \"reorganizes\" from lookup to rule. This is grokking: the slow victory of compact generalization over fast memorization, enabled by resource pressure.</p> <p>Numerical Example: Grokking on Modular Addition</p> <pre><code>import torch\nimport torch.nn as nn\n\n# Task: Learn (a + b) mod p for p = 97\n# Input: one-hot encoded (a, b), Output: (a + b) mod 97\np = 97\n\n# Generate all pairs\ndata = [(a, b, (a + b) % p) for a in range(p) for b in range(p)]\n# Split: 50% train, 50% test\ntrain_data = data[::2]  # 4705 examples\ntest_data = data[1::2]  # 4704 examples\n\n# Simple MLP: 2*97 -&gt; 128 -&gt; 128 -&gt; 97\nmodel = nn.Sequential(\n    nn.Linear(2 * p, 128),\n    nn.ReLU(),\n    nn.Linear(128, 128),\n    nn.ReLU(),\n    nn.Linear(128, p)\n)\n\n# Training with weight decay (critical!)\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1.0)\n\n# Train for many epochs, logging accuracy\n# ... training loop ...\n</code></pre> <p>Output: <pre><code>   Epoch     Train Acc     Test Acc       Phase\n------------------------------------------------\n     100        100.0%        3.2%       Memorized\n    1000        100.0%        3.4%       Memorized\n    5000        100.0%        4.1%       Memorized\n   10000        100.0%        8.7%       Starting...\n   15000        100.0%       23.4%       Transition\n   20000        100.0%       67.8%       Grokking!\n   25000        100.0%       98.2%       Almost there\n   30000        100.0%      100.0%       Full generalization\n</code></pre></p> <p>Interpretation: The network hits 100% training accuracy by epoch 100, but test accuracy stays near random (1/97 \u2248 1%) for thousands of epochs. Around epoch 10,000-25,000, generalization suddenly improves. Without weight decay, this wouldn't happen\u2014the network would stay memorized.</p> <p>Source: <code>slide_computations/deep_dive_surprising_phenomena_examples.py</code> - <code>demo_grokking_simulation()</code></p> <p>Numerical Example: Weight Norm Evolution</p> <p>Weight decay continuously pushes weights toward smaller values. This simulation shows how weight norms evolve with and without regularization:</p> <p>Output: <pre><code>Epoch      No Weight Decay    With Weight Decay  Ratio\n--------------------------------------------------------\n0          5.0                5.0                1.00x\n100        24.7               24.0               1.03x\n500        50.9               30.0               1.70x\n1000       54.7               29.5               1.85x\n5000       55.0               25.5               2.16x\n10000      55.0               20.0               2.75x\n20000      55.0               12.5               4.40x\n30000      55.0               5.0                11.00x\n</code></pre></p> <p>Interpretation: Without weight decay, weights plateau at a high value once memorization is achieved\u2014there's no pressure to simplify. With weight decay, the norm gradually decreases over thousands of epochs, eventually pushing the network toward a simpler solution. By epoch 30,000, the regularized network has weights 11\u00d7 smaller than the unregularized one. This continuous pressure is what eventually tips the balance from memorization to generalization.</p> <p>Source: <code>slide_computations/deep_dive_surprising_phenomena_examples.py</code> - <code>demo_weight_norm_evolution()</code></p>"},{"location":"appendices/surprising-phenomena/#critical-factors-for-grokking","title":"Critical Factors for Grokking","text":"<ol> <li> <p>Regularization is essential: Weight decay (or similar) is required to push past memorization.</p> </li> <li> <p>Small dataset relative to model capacity: Grokking is most dramatic when the model can easily memorize.</p> </li> <li> <p>Structured problem: The task must have an underlying pattern to discover.</p> </li> <li> <p>Long training: You must train far past the point where training loss converges.</p> </li> </ol>"},{"location":"appendices/surprising-phenomena/#practical-implications_1","title":"Practical Implications","text":"<ol> <li> <p>Early stopping might be wrong: If you stop when validation loss stops improving, you might miss grokking.</p> </li> <li> <p>Regularization isn't just about preventing overfitting: It's about guiding the network toward generalizing solutions.</p> </li> <li> <p>Patience matters: Some tasks require very long training to find good solutions.</p> </li> <li> <p>Perfect training accuracy doesn't mean you're done: The network might still be improving its internal representations.</p> </li> </ol> <p>Caveat: Grokking is most dramatic on small algorithmic datasets. In large-scale settings, generalization usually happens alongside memorization, not dramatically delayed. But the principle\u2014that regularization guides networks toward generalizing solutions\u2014applies broadly.</p>"},{"location":"appendices/surprising-phenomena/#phenomenon-3-emergent-abilities","title":"Phenomenon 3: Emergent Abilities","text":""},{"location":"appendices/surprising-phenomena/#the-observation","title":"The Observation","text":"<p>As language models grew from millions to billions to hundreds of billions of parameters, researchers noticed something unexpected: certain capabilities appeared to emerge suddenly at scale.</p> <p>Models would show near-random performance on a task across many scales, then suddenly achieve high performance once they crossed a threshold.</p> <p>Key paper: Wei et al. (2022), \"Emergent Abilities of Large Language Models\"</p>"},{"location":"appendices/surprising-phenomena/#examples-of-claimed-emergent-abilities","title":"Examples of (Claimed) Emergent Abilities","text":"Ability Small Models Large Models Multi-step arithmetic (3-digit addition) Near random High accuracy Chain-of-thought reasoning Provides wrong chains Correct reasoning Word unscrambling Random High accuracy Translating to new languages Poor Good"},{"location":"appendices/surprising-phenomena/#visualizing-emergence","title":"Visualizing Emergence","text":"<pre><code>Task Performance\n   100%\u2502                              ********\n       \u2502                           ***\n       \u2502                          *\n       \u2502                         *\n    50%\u2502                        *\n       \u2502                       *\n       \u2502                      *\n       \u2502                     *\n     0%\u2502 * * * * * * * * * *\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n         1M   10M   100M   1B   10B   100B   1T\n                Model Parameters (log scale) \u2192\n                            \u2502\n                       Emergence threshold\n</code></pre> <p>Reading the diagram: The vertical axis shows task performance (0-100%), and the horizontal axis shows model size on a logarithmic scale spanning 6 orders of magnitude (from 1 million to 1 trillion parameters). The asterisks show how performance changes with scale. On the left (smaller models), performance stays flat near 0%\u2014essentially random guessing\u2014across 1M, 10M, 100M, even 1B parameters. Then, somewhere between 10B and 100B parameters (the \"emergence threshold\"), performance suddenly shoots up in an S-curve to near 100%. This is the signature of an \"emergent\" ability: zero capability becomes high capability with no visible intermediate stage. The question is whether this sudden jump is a real phenomenon or an artifact of how we measure.</p> <p>Performance stays flat (near random) across orders of magnitude, then suddenly jumps.</p>"},{"location":"appendices/surprising-phenomena/#the-debate-real-or-mirage","title":"The Debate: Real or Mirage?","text":"<p>In 2023, Schaeffer and colleagues published a counterargument: emergent abilities might be a measurement artifact.</p> <p>Key paper: Schaeffer et al. (2023), \"Are Emergent Abilities of Large Language Models a Mirage?\"</p> <p>Their argument:</p> <ol> <li> <p>Discontinuous metrics create apparent discontinuities: If you measure \"exact match accuracy\" (all-or-nothing), you see sharp transitions. If you measure token-level error (continuous), you see gradual improvement.</p> </li> <li> <p>The threshold depends on the metric: Choose a different (but equally valid) metric, and the \"emergence\" often disappears.</p> </li> <li> <p>Performance is usually improving gradually: The underlying capability is getting better; the metric just doesn't show it.</p> </li> </ol> <p>Numerical Example: Metric Choice and Apparent Emergence</p> <pre><code># Simulation: Model improves gradually, but metric shows discontinuity\nimport numpy as np\n\n# True capability: smooth sigmoid\ndef true_capability(scale):\n    \"\"\"Probability of getting each step right\"\"\"\n    return 1 / (1 + np.exp(-(scale - 10) / 2))\n\n# Task requires 5 correct steps (like multi-digit arithmetic)\ndef exact_match(scale):\n    \"\"\"All 5 steps must be correct\"\"\"\n    p = true_capability(scale)\n    return p ** 5  # Probability all 5 correct\n\ndef per_step_accuracy(scale):\n    \"\"\"Average accuracy per step\"\"\"\n    return true_capability(scale)\n\nscales = np.arange(1, 20)\nfor s in scales:\n    em = exact_match(s)\n    psa = per_step_accuracy(s)\n    # Log results...\n</code></pre> <p>Output: <pre><code>  Scale    Per-Step Acc    Exact Match      Appearance\n---------------------------------------------------------\n      5          11.2%           0.0%      Both low\n      7          26.9%           0.1%      Gradual vs flat\n      9          50.0%           3.1%      Gradual vs flat\n     10          62.2%           9.2%      Gradual vs jump\n     11          73.1%          20.8%      Gradual vs jump\n     13          88.1%          52.8%      Gradual vs jump\n     15          95.3%          78.3%      Both high\n</code></pre></p> <p>Interpretation: Per-step accuracy improves gradually and smoothly. But exact match (requiring all 5 steps correct) shows a sharp transition. Same underlying capability, different appearance\u2014just from metric choice.</p> <p>Source: <code>slide_computations/deep_dive_surprising_phenomena_examples.py</code> - <code>demo_emergence_metric_mirage()</code></p>"},{"location":"appendices/surprising-phenomena/#why-this-matters","title":"Why This Matters","text":"<p>If emergent abilities are real: - AI development is harder to predict and control - Small experiments don't tell you what large models will do - \"Sharp left turns\" in capability could be dangerous</p> <p>If they're measurement artifacts: - AI development is more predictable than feared - We just need better metrics - Capabilities scale smoothly, not discontinuously</p> <p>The current consensus: Probably both. Some apparent emergences are artifacts of measurement. But some genuine phase transitions likely exist, especially for capabilities that require combining multiple learned skills.</p>"},{"location":"appendices/surprising-phenomena/#practical-implications_2","title":"Practical Implications","text":"<ol> <li> <p>Be cautious about small-scale experiments: A model that fails at 1B parameters might succeed at 100B.</p> </li> <li> <p>Consider your metrics carefully: All-or-nothing metrics can hide gradual improvement (or sudden emergence).</p> </li> <li> <p>Scale is a blunt instrument: Bigger models often work better, but we don't fully understand why.</p> </li> <li> <p>Emergence debates are active research: Don't treat either side as settled fact.</p> </li> </ol>"},{"location":"appendices/surprising-phenomena/#connecting-the-phenomena","title":"Connecting the Phenomena","text":"<p>These three phenomena share common themes:</p>"},{"location":"appendices/surprising-phenomena/#1-phase-transitions","title":"1. Phase Transitions","text":"<p>All three involve sudden changes rather than gradual improvement: - Double descent: Error suddenly drops past the interpolation threshold - Grokking: Test accuracy suddenly improves after long plateau - Emergence: Capabilities suddenly appear at scale</p> <p>The Physical Analogy</p> <p>These sudden changes resemble phase transitions in physics. Water doesn't gradually become \"more solid\" as you cool it\u2014it stays liquid until 0\u00b0C, then suddenly freezes. Iron isn't \"slightly magnetic\" at 770\u00b0C\u2014it has zero magnetism above that temperature and strong magnetism below. Phase transitions occur when a system reorganizes its internal structure at a critical threshold. Neural networks may be doing something similar: at certain points (in model size, training time, or parameter count), the network's internal structure reorganizes\u2014from memorization to generalization, from scattered representations to structured ones. The math of phase transitions (statistical mechanics, renormalization group theory) may eventually help us understand these phenomena, but we're not there yet.</p> <p>Numerical Example: Comparing Phase Transitions</p> <p>All three phenomena show non-linear behavior around critical thresholds:</p> <p>Output: <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502Phenomenon          \u2502X-Axis              \u2502Sudden Change            \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502Double Descent      \u2502Model complexity    \u2502Error drops past threshold\u2502\n\u2502Grokking            \u2502Training epochs     \u2502Test acc jumps late      \u2502\n\u2502Emergence           \u2502Model scale (params)\u2502Capability appears       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nPosition     DD Error        Grok Test Acc   Emerge Perf\n---------------------------------------------------------\nPre-crit     0.70            0.10            0.05\nCritical     1.00            0.30            0.20\nPost-crit    0.30            0.95            0.85\n</code></pre></p> <p>Interpretation: All three phenomena show the signature of a phase transition: performance metrics change suddenly rather than gradually as you cross a critical threshold. For double descent, the threshold is model complexity; for grokking, it's training time; for emergence, it's model scale. The underlying mechanism likely involves some form of internal reorganization\u2014though we don't yet fully understand what.</p> <p>Source: <code>slide_computations/deep_dive_surprising_phenomena_examples.py</code> - <code>demo_phase_transitions_comparison()</code></p>"},{"location":"appendices/surprising-phenomena/#2-the-role-of-optimization","title":"2. The Role of Optimization","text":"<p>How we train matters as much as what we train: - Double descent: SGD's implicit bias toward flat minima - Grokking: Weight decay pushing toward simpler solutions - Emergence: Whatever makes large-scale training work</p>"},{"location":"appendices/surprising-phenomena/#3-theory-lags-practice","title":"3. Theory Lags Practice","text":"<p>All three phenomena were discovered empirically, not predicted by theory: - Practitioners used overparameterized models before theory explained why - Grokking was an unexpected observation - Emergence remains controversial precisely because theory is unclear</p>"},{"location":"appendices/surprising-phenomena/#4-incomplete-understanding","title":"4. Incomplete Understanding","text":"<p>Our theoretical understanding of neural networks remains incomplete. These phenomena remind us that deep learning is partly empirical science\u2014we observe, then try to explain.</p> <p>The meta-lesson: Be humble about classical intuitions. Neural networks are complex systems that don't always behave as simple theory predicts. When your experiment contradicts your intuition, consider that your intuition might be wrong.</p>"},{"location":"appendices/surprising-phenomena/#common-misconceptions","title":"Common Misconceptions","text":"Misconception Reality \"More parameters always means more overfitting\" Double descent shows overparameterized models can generalize well \"Stop training when validation loss increases\" Grokking shows generalization can occur much later \"Model capabilities scale smoothly with size\" Emergent abilities may appear suddenly at scale thresholds \"These phenomena only matter for researchers\" They affect practical decisions about model size and training \"Grokking only happens on toy problems\" It's been observed in realistic settings, though less dramatically \"Emergent abilities prove models 'understand'\" Sudden capability doesn't imply understanding \"Classical ML theory is wrong\" It's incomplete, not wrong\u2014these are edge cases it didn't cover \"We should always train for longer\" Resource constraints are real; these insights help prioritize"},{"location":"appendices/surprising-phenomena/#reflection-questions","title":"Reflection Questions","text":"<ol> <li> <p>If double descent is real, why do we still use regularization? When would you add explicit regularization to an overparameterized model?</p> </li> <li> <p>How would you modify your training procedure if you suspected grokking might occur? What would you monitor, and how long would you train?</p> </li> <li> <p>A colleague says \"our small prototype works, so the full model will work too.\" What's the concern with this reasoning, and what's the counter-concern?</p> </li> <li> <p>Why might emergent abilities be a measurement artifact? Design an experiment to test whether a claimed emergent ability is real or an artifact.</p> </li> <li> <p>What do these phenomena suggest about our theoretical understanding of neural networks? Does it matter if we can't fully explain why models work?</p> </li> <li> <p>How do these phenomena relate to the Universal Approximation Theorem from the earlier Deep Dive? Does that theorem predict any of this behavior?</p> </li> </ol>"},{"location":"appendices/surprising-phenomena/#practice-problems","title":"Practice Problems","text":"<ol> <li> <p>Sketch the double descent curve and label: (a) the classical regime, (b) the interpolation threshold, (c) the overparameterized regime. Where would you expect test error to be highest? Lowest?</p> </li> <li> <p>Grokking scenario: You're training a model on a dataset with 10,000 examples. At epoch 500, training accuracy is 100% and test accuracy is 65%. Training accuracy stays at 100% for the next 2,000 epochs while test accuracy hovers around 65-68%. What would you do next? How long would you continue training? What would change your decision?</p> </li> <li> <p>Emergence analysis: Find one claimed example of an \"emergent ability\" in the literature (Wei et al. 2022 is a good source). Evaluate whether it might be a measurement artifact by considering: (a) what metric was used, (b) whether a continuous alternative metric exists, (c) what gradual improvement might look like.</p> </li> <li> <p>Connecting phenomena: All three phenomena involve some kind of \"sudden change.\" Compare and contrast: what is changing suddenly in each case, and what causes the sudden change?</p> </li> </ol>"},{"location":"appendices/surprising-phenomena/#summary","title":"Summary","text":"<p>Key takeaways:</p> <ol> <li> <p>Double descent shows that overparameterized models can generalize well, contradicting the classical bias-variance intuition. The interpolation threshold (where parameters \u2248 data points) is often the worst place to be.</p> </li> <li> <p>Grokking demonstrates that generalization can occur long after memorization, challenging early stopping heuristics. Regularization is critical for pushing networks from memorization to generalization.</p> </li> <li> <p>Emergent abilities (possibly) show sudden capability gains at scale, making small-scale experiments unreliable predictors of large-scale behavior. But some claimed emergences may be measurement artifacts.</p> </li> <li> <p>All three phenomena involve phase transitions\u2014sudden changes rather than gradual improvement\u2014which makes them hard to predict.</p> </li> <li> <p>Implicit regularization from optimization (SGD, weight decay) appears to play a role in all these phenomena, suggesting that how we train matters as much as what we train.</p> </li> <li> <p>Our theoretical understanding of neural networks remains incomplete\u2014these phenomena were discovered empirically, not predicted from first principles.</p> </li> <li> <p>Practical implication: Don't extrapolate too confidently from small experiments or short training runs. Classical intuitions are incomplete guides to modern deep learning.</p> </li> </ol>"},{"location":"appendices/surprising-phenomena/#further-reading","title":"Further Reading","text":"<ul> <li>Belkin, M. et al. (2019). \"Reconciling modern machine learning practice and the bias-variance trade-off.\" PNAS.</li> <li>Power, A. et al. (2022). \"Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets.\" arXiv.</li> <li>Wei, J. et al. (2022). \"Emergent Abilities of Large Language Models.\" arXiv.</li> <li>Schaeffer, R. et al. (2023). \"Are Emergent Abilities of Large Language Models a Mirage?\" arXiv.</li> <li>Nakkiran, P. et al. (2019). \"Deep Double Descent: Where Bigger Models and More Data Can Hurt.\" arXiv.</li> </ul>"},{"location":"appendices/transformer-architecture/","title":"Deep Dive: Transformer Architecture","text":"<p>Extends Module 8: Natural Language Processing</p>"},{"location":"appendices/transformer-architecture/#introduction","title":"Introduction","text":"<p>In Module 8, we learned that transformers use self-attention to process sequences. We covered the high-level concepts of Query, Key, Value and the attention formula.</p> <p>This deep dive goes deeper. We'll trace through the exact matrix dimensions at each step, see exactly where the learnable parameters live, and build a complete transformer from scratch in PyTorch.</p> <p>By the end, you'll understand not just what transformers do, but how they do it\u2014down to the individual matrix operations.</p> <p>What parameters learn: Token embeddings learn dense representations where similar words cluster together. Attention projections (W_Q, W_K, W_V) learn relevance between tokens\u2014W_Q learns what tokens \"look for,\" W_K what they \"offer,\" W_V what information to pass. FFNs appear to store factual knowledge as distributed key-value memories. Layer norms stabilize training.</p>"},{"location":"appendices/transformer-architecture/#where-parameters-live-in-a-transformer","title":"Where Parameters Live in a Transformer","text":"<p>Understanding where the learnable parameters actually reside is crucial for understanding what the model \"learns\" during training.</p>"},{"location":"appendices/transformer-architecture/#parameter-overview","title":"Parameter Overview","text":"<p>For a transformer with: - <code>vocab_size</code> = 30,000 (vocabulary) - <code>d_model</code> = 512 (model dimension) - <code>n_heads</code> = 8 (attention heads) - <code>d_k = d_v</code> = 64 (dimension per head = d_model / n_heads) - <code>d_ff</code> = 2048 (feedforward hidden dimension, typically 4\u00d7 d_model) - <code>n_layers</code> = 6 (number of transformer blocks) - <code>max_seq_len</code> = 512 (maximum sequence length)</p> <p>Why 4\u00d7 expansion in FFN? Largely empirical\u2014it worked well in the original paper. The expansion provides \"intermediate reasoning space.\" Too small (2\u00d7) limits expressivity; too large (8\u00d7) adds parameters with diminishing returns. Some efficient transformers use 2-2.67\u00d7; the ratio isn't sacred if you have specific constraints.</p>"},{"location":"appendices/transformer-architecture/#token-embedding-matrix","title":"Token Embedding Matrix","text":"<p>Shape: <code>(vocab_size, d_model)</code> = <code>(30000, 512)</code></p> <p>Parameters: 15,360,000</p> <p>What it learns: A dense vector representation for each token in the vocabulary. This is the \"lookup table\" that converts token IDs to continuous vectors.</p> <p>How similar embeddings emerge: Word relationships emerge from the training objective (distributional hypothesis). Tokens in similar contexts (\"cat\" and \"dog\" both appear in \"the ___ ran across the yard\") get similar embeddings. The model learns they're interchangeable in many contexts. Subtle relationships emerge too: \"king\" \u2013 \"man\" + \"woman\" \u2248 \"queen.\" None is programmed\u2014it falls out of optimizing prediction accuracy.</p> <pre><code>self.token_embedding = nn.Embedding(\n    num_embeddings=vocab_size,  # 30,000\n    embedding_dim=d_model        # 512\n)\n# Weight shape: (30000, 512)\n</code></pre> <p>Numerical Example: Embedding Lookup</p> <pre><code>import torch\nimport torch.nn as nn\n\ntorch.manual_seed(42)\nembedding = nn.Embedding(num_embeddings=10, embedding_dim=4)\n\n# Look up token IDs [2, 5, 7]\ntoken_ids = torch.tensor([2, 5, 7])\noutput = embedding(token_ids)\n\nprint(\"Token ID 2 \u2192\", output[0].detach().numpy().round(4))\nprint(\"Verify: embedding.weight[2] \u2192\", embedding.weight[2].detach().numpy().round(4))\n</code></pre> <p>Output: <pre><code>Token ID 2 \u2192 [-0.7521  1.6487 -0.3925 -1.4036]\nVerify: embedding.weight[2] \u2192 [-0.7521  1.6487 -0.3925 -1.4036]\n</code></pre></p> <p>Interpretation: Embedding is just table lookup\u2014token ID 2 retrieves row 2 of the weight matrix. Each token gets its own learned vector. Similar tokens (learned during training) will have similar vectors.</p> <p>Source: <code>slide_computations/deep_dive_transformer_examples.py</code> - <code>demo_embedding_lookup()</code></p>"},{"location":"appendices/transformer-architecture/#positional-encoding-embedding","title":"Positional Encoding / Embedding","text":"<p>Sinusoidal (Original Transformer): No learnable parameters\u2014computed deterministically</p> <p>Learned Positional Embedding: - Shape: <code>(max_seq_len, d_model)</code> = <code>(512, 512)</code> - Parameters: 262,144</p> <pre><code># Learned positional embeddings\nself.pos_embedding = nn.Embedding(\n    num_embeddings=max_seq_len,  # 512\n    embedding_dim=d_model         # 512\n)\n</code></pre>"},{"location":"appendices/transformer-architecture/#attention-layer-parameters-per-layer","title":"Attention Layer Parameters (Per Layer)","text":"<p>Each attention layer has four weight matrices:</p> Matrix Shape Parameters Purpose W_Q <code>(d_model, d_model)</code> 512 \u00d7 512 = 262,144 Projects input to queries W_K <code>(d_model, d_model)</code> 512 \u00d7 512 = 262,144 Projects input to keys W_V <code>(d_model, d_model)</code> 512 \u00d7 512 = 262,144 Projects input to values W_O <code>(d_model, d_model)</code> 512 \u00d7 512 = 262,144 Projects concatenated heads to output Biases 4 \u00d7 <code>(d_model)</code> 4 \u00d7 512 = 2,048 One bias per projection <p>Total per attention layer: ~1,050,624 parameters</p> <p>Key insight: Even though we have 8 heads, the total parameter count is the same as if we had one big head. The \"heads\" are created by reshaping, not by adding parameters.</p> <p>Why multiple heads help: They enable attending to different relationships simultaneously\u2014syntactic, semantic, positional. Research finds heads that specialize: one attends to grammatical antecedents, another to adjacent tokens. This specialization emerges during training. Trade-off: each head has smaller d_k, but specialization benefits outweigh capacity reduction.</p>"},{"location":"appendices/transformer-architecture/#feed-forward-network-per-layer","title":"Feed-Forward Network (Per Layer)","text":"<p>The FFN applies two linear transformations with a non-linearity:</p> \\[\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2\\] Matrix Shape Parameters Purpose W_1 <code>(d_model, d_ff)</code> 512 \u00d7 2048 = 1,048,576 Expand to higher dimension b_1 <code>(d_ff)</code> 2,048 Bias for expansion W_2 <code>(d_ff, d_model)</code> 2048 \u00d7 512 = 1,048,576 Contract back to model dimension b_2 <code>(d_model)</code> 512 Bias for contraction <p>Total per FFN: ~2,099,712 parameters</p> <p>FFN as knowledge storage: Research supports this hypothesis. Specific FFN neurons activate for specific concepts (\"The capital of France is ___\" triggers neurons contributing \"Paris\"). Researchers have edited factual knowledge by modifying FFN weights. Attention handles \"routing\" (context-dependent); FFN handles fixed transformations. Fixed parameters suit stable facts; dynamic computation suits context-dependent processing.</p> <p>Why 4\u00d7 expansion works\u2014the \"committee of specialists\" intuition: Think of the expanded dimension as a committee of 2,048 specialists, each detecting a specific pattern. When input x arrives, it \"consults\" all specialists (W_1 multiplication), but ReLU/GELU silences those who don't recognize the pattern (negative activations \u2192 zero). Typically only 30-50% of neurons activate for any given input\u2014this sparsity means different inputs engage different specialist subsets. The contraction (W_2) then combines the active specialists' opinions. More specialists (larger d_ff) means finer-grained pattern detection, but with diminishing returns and increased compute cost.</p> <p>Numerical Example: FFN Forward Pass</p> <pre><code>import torch\nimport torch.nn as nn\n\ntorch.manual_seed(42)\nd_model, d_ff = 4, 16  # 4\u00d7 expansion\nW1 = torch.randn(d_ff, d_model) * 0.5\nW2 = torch.randn(d_model, d_ff) * 0.5\n\nx = torch.tensor([1.0, -0.5, 0.8, 0.2])\n\n# Step 1: Expand\nh1 = x @ W1.T\nprint(f\"After W1 (16 dims): {h1[:8].numpy().round(3)}...\")\n\n# Step 2: ReLU\nh2 = torch.relu(h1)\nprint(f\"After ReLU: {(h2 &gt; 0).sum().item()}/16 neurons active\")\n\n# Step 3: Contract\noutput = h2 @ W2.T\nprint(f\"Output (4 dims): {output.numpy().round(3)}\")\n</code></pre> <p>Output: <pre><code>After W1 (16 dims): [ 0.741  0.47  -1.086 -0.455  0.706 -0.16   0.693 -0.141]...\nAfter ReLU: 5/16 neurons active\nOutput (4 dims): [ 0.575  0.434 -0.103 -0.405]\n</code></pre></p> <p>Interpretation: ReLU zeros out 11 of 16 neurons (~69%)\u2014only 5 \"specialists\" recognized this input. Different inputs would activate different subsets. The sparse activation means each input engages a different combination of learned patterns stored in W_2.</p> <p>Source: <code>slide_computations/deep_dive_transformer_examples.py</code> - <code>demo_ffn_forward()</code></p>"},{"location":"appendices/transformer-architecture/#layer-normalization-per-layer","title":"Layer Normalization (Per Layer)","text":"<p>Each transformer block typically has 2 layer norms:</p> Component Shape Parameters Scale (\u03b3) <code>(d_model)</code> 512 Shift (\u03b2) <code>(d_model)</code> 512 <p>Total per layer norm: 1,024 parameters Total per transformer block: 2,048 parameters (2 layer norms)</p> <p>Why layer norm matters: Without it, activations grow/shrink exponentially through layers\u2014causing gradient explosion/vanishing. Layer norm keeps activations stable regardless of depth. The learned \u03b3 and \u03b2 parameters let the model recover useful mean/variance. Appears twice per block (before attention, before FFN) because both operations can distort statistics.</p> <p>Numerical Example: Layer Normalization</p> <pre><code>import torch\n\nx = torch.tensor([[10.0, 20.0, 30.0, 40.0]])  # Varying magnitudes\nprint(f\"Input: {x[0].numpy()}, mean={x.mean():.1f}, std={x.std():.1f}\")\n\n# Normalize to mean=0, std=1\nmean = x.mean(dim=-1, keepdim=True)\nstd = torch.sqrt(x.var(dim=-1, unbiased=False, keepdim=True) + 1e-6)\nx_norm = (x - mean) / std\nprint(f\"Normalized: {x_norm[0].numpy().round(4)}\")\nprint(f\"New mean={x_norm.mean():.6f}, std={x_norm.std():.4f}\")\n\n# Apply learned scale (\u03b3) and shift (\u03b2)\ngamma = torch.tensor([1.0, 2.0, 0.5, 1.5])\nbeta = torch.tensor([0.0, 1.0, -0.5, 0.0])\noutput = gamma * x_norm + beta\nprint(f\"After \u03b3, \u03b2: {output[0].numpy().round(4)}\")\n</code></pre> <p>Output: <pre><code>Input: [10. 20. 30. 40.], mean=25.0, std=11.2\nNormalized: [-1.3416 -0.4472  0.4472  1.3416]\nNew mean=0.000000, std=1.0000\nAfter \u03b3, \u03b2: [-1.3416  0.1056 -0.2764  2.0125]\n</code></pre></p> <p>Interpretation: Input with mean 25 and std 11 gets normalized to mean 0 and std 1. The learned \u03b3 and \u03b2 then rescale\u2014dimension 1 gets doubled (\u03b3=2) and shifted up (\u03b2=1), dimension 2 gets halved (\u03b3=0.5) and shifted down (\u03b2=-0.5). This lets the model learn which dimensions need larger/smaller variance.</p> <p>Source: <code>slide_computations/deep_dive_transformer_examples.py</code> - <code>demo_layer_norm()</code></p>"},{"location":"appendices/transformer-architecture/#parameter-count-summary","title":"Parameter Count Summary","text":"Component Count Parameters Each Total Token Embedding 1 15,360,000 15,360,000 Positional Embedding 1 262,144 262,144 Attention Layers 6 ~1,050,624 ~6,303,744 FFN Layers 6 ~2,099,712 ~12,598,272 Layer Norms 12 1,024 12,288 Output Head 1 15,360,000 15,360,000* <p>Total: ~50 million parameters (with tied embeddings: ~35 million)</p> <p>*Often tied with token embedding</p>"},{"location":"appendices/transformer-architecture/#what-each-component-does-the-why","title":"What Each Component Does (The \"Why\")","text":""},{"location":"appendices/transformer-architecture/#why-positional-encoding","title":"Why Positional Encoding?","text":"<p>The Problem: Self-attention is permutation-invariant</p> <p>Without position information, these sentences would be identical to the model: - \"Dog bites man\" - \"Man bites dog\" - \"Bites man dog\"</p> <p>The attention mechanism only cares about what tokens are present and their relationships, not where they appear.</p> <p>Sinusoidal Positional Encoding</p> \\[PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\\] \\[PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\\] <p>Why sine and cosine?</p> <ol> <li>Bounded range: Values stay in [-1, 1], preventing position from dominating</li> <li>Unique per position: Each position gets a unique encoding</li> <li>Relative positions via linear transformation: For any fixed offset k, \\(PE_{pos+k}\\) can be represented as a linear function of \\(PE_{pos}\\)</li> <li>Generalizes to longer sequences: Works for sequences longer than seen during training</li> </ol> <p>The \"clock hands\" intuition: Each dimension pair (sin, cos) is like a clock hand rotating at a different speed. Dimension 0-1 rotates quickly (completes a full cycle in ~6 positions), while dimension 510-511 rotates extremely slowly (cycle length ~10,000 positions). Position 0 has all clock hands at the same starting angle. As position increases, fast hands spin rapidly while slow hands barely move. Any position creates a unique combination of hand angles\u2014like reading a clock with 256 hands of different speeds. This multi-frequency encoding lets the model learn both local patterns (via fast-changing dimensions) and global structure (via slow-changing dimensions).</p> <p>Numerical Example: Positional Encoding Values</p> <pre><code>import numpy as np\n\ndef get_pe(pos, d_model=8):\n    pe = np.zeros(d_model)\n    for i in range(d_model):\n        if i % 2 == 0:\n            pe[i] = np.sin(pos / (10000 ** (i / d_model)))\n        else:\n            pe[i] = np.cos(pos / (10000 ** ((i-1) / d_model)))\n    return pe\n\nfor pos in [0, 1, 10, 100]:\n    print(f\"Position {pos:3d}: {get_pe(pos).round(4)}\")\n</code></pre> <p>Output: <pre><code>Position   0: [ 0.      1.      0.      1.      0.      1.      0.      1.    ]\nPosition   1: [ 0.8415  0.5403  0.0998  0.995   0.01    1.      0.001   1.    ]\nPosition  10: [-0.544  -0.8391  0.8415  0.5403  0.0998  0.995   0.01    1.    ]\nPosition 100: [-0.5064  0.8623 -0.544  -0.8391  0.8415  0.5403  0.0998  0.995 ]\n</code></pre></p> <p>Interpretation: Position 0 has a distinctive [0,1,0,1,...] pattern. Each position creates a unique encoding. Low dimensions (left) change rapidly\u2014notice positions 1 and 10 have very different dim0 values. High dimensions (right) change slowly\u2014dim6-7 are nearly identical for positions 0, 1, and 10. All values stay bounded in [-1, 1].</p> <p>Source: <code>slide_computations/deep_dive_transformer_examples.py</code> - <code>demo_positional_encoding()</code></p> <p>Why learned embeddings over sinusoidal? Sinusoidal generalizes to arbitrary lengths, but fixed context windows make this rarely matter in practice. Learned embeddings perform slightly better and can capture task-specific patterns (code structure, conversation turns). Neither extrapolates well beyond training lengths. Modern architectures use relative positional encodings (RoPE, ALiBi) that generalize better via relative distances.</p>"},{"location":"appendices/transformer-architecture/#why-self-attention","title":"Why Self-Attention?","text":"<p>Advantage 1: O(1) Path Length</p> <p>To connect position 1 to position 100: - RNN: Information must pass through 99 sequential steps - Attention: Direct connection in one step</p> <p>This solves the long-range dependency problem.</p> <p>Advantage 2: Parallelization</p> <ul> <li>RNN: Must process sequentially (h\u2081 \u2192 h\u2082 \u2192 h\u2083 \u2192 ...)</li> <li>Attention: All positions computed simultaneously</li> </ul> <p>This enables massive speedups on GPUs.</p> <p>Advantage 3: Dynamic, Content-Dependent Connections</p> <p>RNNs have fixed connections (previous \u2192 current). Attention weights are computed based on the content of the sequence:</p> <pre><code>\"The cat sat on the mat because it was soft\"\n                                    \u2191\n                              \"it\" attends strongly to \"mat\"\n\n\"The cat sat on the mat because it was tired\"\n                                    \u2191\n                              \"it\" attends strongly to \"cat\"\n</code></pre> <p>Same architecture, different attention patterns based on meaning.</p> <p>The \"library search\" analogy made concrete: Imagine processing \"The capital of France is ___\": - The blank position generates a Query: \"I need information about capitals and France\" - \"France\" offers a Key: \"I have information about a European country\" - \"France\" also has a Value: the actual semantic content (geography, culture, language facts) - \"capital\" offers a Key: \"I relate to cities and governance\" - The Query-Key comparison finds high similarity between the blank's query and \"France\"/\"capital\" keys - The attention weights then retrieve Values from those high-similarity positions</p> <p>This is why Q, K, V are separate: the question you ask (Q) may differ from what you advertise (K), which may differ from what you actually contribute (V). A pronoun like \"it\" asks \"what noun am I referring to?\" (Q), advertises \"I'm a pronoun needing resolution\" (K), and contributes \"third-person singular reference\" (V).</p> <p>Learning contextual attention: Entirely learned during training via backpropagation. When the model predicts incorrectly (attended to \"mat\" instead of \"cat\"), gradients adjust W_Q, W_K, W_V so \"it\" generates queries with higher similarity to \"cat\" when context suggests animacy. Different heads learn different aspects (proximity, syntax, semantics), enabling sophisticated disambiguation.</p>"},{"location":"appendices/transformer-architecture/#why-the-ffn-mlp","title":"Why the FFN (MLP)?","text":"<p>The attention mechanism is powerful but has limitations:</p> <ol> <li>Attention is linear (after softmax): Just weighted sums of values</li> <li>Attention is the same for all positions: The W_Q, W_K, W_V matrices don't change per position</li> </ol> <p>What FFN provides:</p> <p>Non-linearity: The ReLU (or GELU) activation adds non-linear transformations that attention alone cannot provide.</p> <p>Position-wise processing: Each position gets the same transformation, but independently.</p> <p>Memory/Knowledge storage: Research suggests FFN layers store factual knowledge. When you ask \"The capital of France is ___\", the FFN layers help retrieve \"Paris.\"</p> <p>Why the expansion to 4\u00d7?</p> <p>The expansion (512 \u2192 2048 \u2192 512) creates a \"bottleneck\" architecture: - Expansion: Project to higher dimension, allowing richer intermediate representations - Non-linearity: Apply ReLU/GELU - Contraction: Compress back to model dimension</p> <p>FFN as key-value memory: W_1 rows are \"keys\"\u2014patterns to match. Input compared via matrix multiplication; GELU sparsifies activations. W_2 stores \"values\" retrieved when keys match. Computing FFN(x) = GELU(x W_1) W_2 is: find matching keys, weight by match strength, retrieve values. Ablating specific W_2 rows removes specific facts\u2014strong evidence for this interpretation.</p>"},{"location":"appendices/transformer-architecture/#self-attention-step-by-step-with-matrix-dimensions","title":"Self-Attention Step-by-Step with Matrix Dimensions","text":"<p>Let's trace through self-attention with concrete numbers:</p> <p>Setup: - <code>batch_size (B)</code> = 2 - <code>seq_len (T)</code> = 4 - <code>d_model</code> = 8 - <code>n_heads</code> = 2 - <code>d_k = d_v</code> = 4 (= d_model / n_heads)</p>"},{"location":"appendices/transformer-architecture/#step-1-input","title":"Step 1: Input","text":"<p>X shape: <code>(B, T, d_model)</code> = <code>(2, 4, 8)</code></p> <p>This is 2 sequences, each with 4 tokens, each token represented by 8 dimensions.</p>"},{"location":"appendices/transformer-architecture/#step-2-linear-projections","title":"Step 2: Linear Projections","text":"<p>Weight matrices (learnable parameters): - W_Q: <code>(8, 8)</code> - W_K: <code>(8, 8)</code> - W_V: <code>(8, 8)</code></p> <p>Compute Q, K, V: <pre><code>Q = X @ W_Q: (2, 4, 8) @ (8, 8) \u2192 (2, 4, 8)\nK = X @ W_K: (2, 4, 8) @ (8, 8) \u2192 (2, 4, 8)\nV = X @ W_V: (2, 4, 8) @ (8, 8) \u2192 (2, 4, 8)\n</code></pre></p>"},{"location":"appendices/transformer-architecture/#step-3-reshape-for-multi-head-attention","title":"Step 3: Reshape for Multi-Head Attention","text":"<p>We split d_model=8 into n_heads=2 heads, each with d_k=4 dimensions.</p> <p>Reshape: <code>(B, T, d_model)</code> \u2192 <code>(B, T, n_heads, d_k)</code> \u2192 <code>(B, n_heads, T, d_k)</code></p> <pre><code>Q: (2, 4, 8) \u2192 (2, 4, 2, 4) \u2192 (2, 2, 4, 4)\nK: (2, 4, 8) \u2192 (2, 4, 2, 4) \u2192 (2, 2, 4, 4)\nV: (2, 4, 8) \u2192 (2, 4, 2, 4) \u2192 (2, 2, 4, 4)\n</code></pre> <p>Now we have: - 2 batches - 2 heads per batch - 4 tokens per head - 4 dimensions per token</p> <p>Why multiple heads?\u2014The \"committee of experts\" analogy: Each head operates on a different d_k-dimensional subspace of the embedding. With d_model=512 and 8 heads, each head sees only 64 dimensions\u2014a different \"view\" of the data. One head might specialize in syntactic relationships (subject-verb agreement), another in semantic similarity (synonyms, related concepts), another in positional patterns (attending to adjacent tokens). This emerges naturally from training: different random initializations + gradient descent = different specializations. The output projection W_O then combines these diverse perspectives. Single-head attention with d_k=512 could theoretically learn the same patterns, but multi-head makes it easier\u2014each head has a simpler job.</p> <p>Numerical Example: Multi-Head Reshape</p> <pre><code>import torch\n\n# Create input: 1 batch, 4 tokens, 8 dimensions\nx = torch.zeros(1, 4, 8)\nfor t in range(4):\n    for d in range(8):\n        x[0, t, d] = t + d * 0.1  # Recognizable pattern\n\nprint(f\"Input shape: {tuple(x.shape)} (batch, seq, d_model)\")\nprint(f\"Token 0: {x[0, 0].numpy().round(1)}\")\n\n# Reshape for 2 heads\nx_reshaped = x.view(1, 4, 2, 4).transpose(1, 2)\nprint(f\"\\nAfter reshape: {tuple(x_reshaped.shape)} (batch, heads, seq, d_head)\")\nprint(f\"Head 0, Token 0: {x_reshaped[0, 0, 0].numpy().round(1)}\")\nprint(f\"Head 1, Token 0: {x_reshaped[0, 1, 0].numpy().round(1)}\")\n</code></pre> <p>Output: <pre><code>Input shape: (1, 4, 8) (batch, seq, d_model)\nToken 0: [0.  0.1 0.2 0.3 0.4 0.5 0.6 0.7]\n\nAfter reshape: (1, 2, 4, 4) (batch, heads, seq, d_head)\nHead 0, Token 0: [0.  0.1 0.2 0.3]\nHead 1, Token 0: [0.4 0.5 0.6 0.7]\n</code></pre></p> <p>Interpretation: The 8-dimensional embedding gets split: Head 0 sees dims 0-3, Head 1 sees dims 4-7. Each head processes a different \"view\" of each token. No new parameters are created\u2014it's pure reshaping. The heads then compute attention independently on their respective subspaces.</p> <p>Source: <code>slide_computations/deep_dive_transformer_examples.py</code> - <code>demo_multihead_reshape()</code></p>"},{"location":"appendices/transformer-architecture/#step-4-compute-attention-scores","title":"Step 4: Compute Attention Scores","text":"<p>Formula: scores = Q @ K^T / \u221ad_k</p> <pre><code>Q @ K^T: (2, 2, 4, 4) @ (2, 2, 4, 4)^T \u2192 (2, 2, 4, 4)\n</code></pre> <p>The result <code>(2, 2, 4, 4)</code> means: for each batch, for each head, we have a 4\u00d74 matrix where entry (i,j) is how much token i attends to token j.</p> <p>Scale by \u221ad_k = \u221a4 = 2: <pre><code>scores = scores / 2\n</code></pre></p> <p>This prevents the dot products from growing too large (which would make softmax saturate).</p> <p>Without \u221ad_k scaling: Dot products grow with d_k, pushing softmax into saturated regions (nearly one-hot). Problems: gradients vanish (softmax derivative approaches zero), attention becomes too \"sharp\" (loses weighted combination), model becomes brittle (small changes flip attention). Scaling maintains consistent softmax behavior regardless of dimensionality.</p> <p>What saturation looks like: Consider attention scores [8, 4, 2, 1] (typical for d_k=64). Softmax gives [0.98, 0.02, 0.00, 0.00]\u2014the model attends almost entirely to the first token and ignores the rest. Now scale by \u221a64=8: scores become [1, 0.5, 0.25, 0.125]. Softmax now gives [0.40, 0.24, 0.19, 0.17]\u2014a much smoother distribution where all tokens contribute. The smooth version allows nuanced weighted combinations and provides meaningful gradients for all positions. The saturated version essentially makes attention a hard selection, losing the benefits of soft attention.</p> <p>Numerical Example: Scaling Effect on Softmax</p> <pre><code>import numpy as np\n\ndef softmax(x):\n    exp_x = np.exp(x - np.max(x))\n    return exp_x / exp_x.sum()\n\n# Typical dot product magnitudes for d_k=64 and d_k=512\nscores_64 = np.array([8.0, 4.0, 2.0, 1.0])\nscores_512 = scores_64 * np.sqrt(512/64)  # Scale up for larger d_k\n\nprint(\"WITHOUT scaling:\")\nprint(f\"  d_k=64:  {np.round(softmax(scores_64), 4)}\")\nprint(f\"  d_k=512: {np.round(softmax(scores_512), 4)}\")\n\nprint(\"\\nWITH scaling by sqrt(d_k):\")\nprint(f\"  d_k=64:  {np.round(softmax(scores_64 / np.sqrt(64)), 4)}\")\nprint(f\"  d_k=512: {np.round(softmax(scores_512 / np.sqrt(512)), 4)}\")\n</code></pre> <p>Output: <pre><code>WITHOUT scaling:\n  d_k=64:  [0.9788 0.0179 0.0024 0.0009]\n  d_k=512: [1. 0. 0. 0.]\n\nWITH scaling by sqrt(d_k):\n  d_k=64:  [0.4007 0.243  0.1893 0.167 ]\n  d_k=512: [0.4007 0.243  0.1893 0.167 ]\n</code></pre></p> <p>Interpretation: Without scaling, d_k=512 produces a nearly one-hot distribution\u2014the model attends only to token 0. With scaling, both d_k values produce identical smooth distributions. This consistency across embedding dimensions is why scaling by \u221ad_k is critical.</p> <p>Source: <code>slide_computations/deep_dive_transformer_examples.py</code> - <code>demo_scaling_effect()</code></p>"},{"location":"appendices/transformer-architecture/#step-5-apply-softmax","title":"Step 5: Apply Softmax","text":"<pre><code>attn_weights = softmax(scores, dim=-1)\nShape: (2, 2, 4, 4) \u2192 (2, 2, 4, 4)\n</code></pre> <p>Each row now sums to 1:</p> <pre><code>Example attention matrix for one head:\n        Token0  Token1  Token2  Token3\nToken0:  0.4     0.3     0.2     0.1    = 1.0\nToken1:  0.1     0.5     0.3     0.1    = 1.0\nToken2:  0.2     0.2     0.4     0.2    = 1.0\nToken3:  0.1     0.1     0.2     0.6    = 1.0\n</code></pre> <p>Numerical Example: Attention Scores Step by Step</p> <pre><code>import numpy as np\n\n# 4 tokens, d_k=4\nQ = np.array([[1,0,1,0], [0.5,0.5,0,1], [0,1,0.5,0.5], [1,1,0,0]], dtype=float)\nK = np.array([[1,0,0.5,0.5], [0,1,0,1], [0.5,0.5,1,0], [0,0,1,1]], dtype=float)\n\n# Raw scores: Q @ K^T\nscores = Q @ K.T\nprint(\"Raw scores (Q @ K^T):\")\nprint(scores.round(2))\n\n# Scale by sqrt(d_k)\nscaled = scores / np.sqrt(4)\n\n# Softmax each row\ndef softmax_rows(x):\n    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n    return exp_x / exp_x.sum(axis=1, keepdims=True)\n\nattn = softmax_rows(scaled)\nprint(\"\\nAttention weights (softmax of scaled scores):\")\nprint(attn.round(3))\n</code></pre> <p>Output: <pre><code>Raw scores (Q @ K^T):\n[[1.5  0.   1.5  1.  ]\n [1.   1.5  0.5  1.  ]\n [0.5  1.5  1.   1.  ]\n [1.   1.   1.   0.  ]]\n\nAttention weights (softmax of scaled scores):\n[[0.308 0.145 0.308 0.24 ]\n [0.246 0.316 0.192 0.246]\n [0.192 0.316 0.246 0.246]\n [0.277 0.277 0.277 0.168]]\n</code></pre></p> <p>Interpretation: Row i shows how much token i attends to each token. Token 0 attends equally (0.308) to tokens 0 and 2. Token 1 attends most (0.316) to token 1. Each row sums to 1\u2014it's a probability distribution over which tokens to attend to.</p> <p>Source: <code>slide_computations/deep_dive_transformer_examples.py</code> - <code>demo_attention_scores()</code></p>"},{"location":"appendices/transformer-architecture/#step-6-apply-attention-to-values","title":"Step 6: Apply Attention to Values","text":"<pre><code>attn_weights @ V: (2, 2, 4, 4) @ (2, 2, 4, 4) \u2192 (2, 2, 4, 4)\n</code></pre> <p>Each token's output is now a weighted sum of all value vectors.</p>"},{"location":"appendices/transformer-architecture/#step-7-concatenate-heads","title":"Step 7: Concatenate Heads","text":"<p>Reshape: <code>(B, n_heads, T, d_k)</code> \u2192 <code>(B, T, n_heads, d_k)</code> \u2192 <code>(B, T, d_model)</code></p> <pre><code>context: (2, 2, 4, 4) \u2192 (2, 4, 2, 4) \u2192 (2, 4, 8)\n</code></pre>"},{"location":"appendices/transformer-architecture/#step-8-output-projection","title":"Step 8: Output Projection","text":"<pre><code>output = context @ W_O: (2, 4, 8) @ (8, 8) \u2192 (2, 4, 8)\n</code></pre> <p>Final output has the same shape as input: <code>(2, 4, 8)</code>.</p> <p>Stacking limits: Shape preservation enables many layers (GPT-3 has 96), but practical limits exist: compute/memory scale linearly, training stability degrades, performance shows diminishing returns (12\u219224 helps more than 96\u2192192), and latency matters for real-time applications. For fixed compute budgets, there's an optimal balance between depth, width, and data.</p>"},{"location":"appendices/transformer-architecture/#dimension-flow-diagram","title":"Dimension Flow Diagram","text":"<p>Reading the diagram: This flowchart shows the first half of attention\u2014from input to Q/K/V projections. Blue boxes represent data tensors that flow through the network: embeddings arrive with shape (seq \u00d7 d_model), get positional encoding added, then become X. The key insight is the three-way split: X passes through three separate learnable projection matrices (red hatched boxes labeled W_Q, W_K, W_V), each transforming the same input into a different representation. The \"Learnable!\" annotation emphasizes these are the trained parameters\u2014the attention weights themselves are computed dynamically. Purple boxes show the outputs: Q (queries\u2014what each token is looking for), K (keys\u2014what each token offers to match against), and V (values\u2014the actual information to pass along). All three maintain the same (seq \u00d7 d_model) shape.</p> <p></p> <p>Reading the diagram: This flowchart continues from Q/K/V to the final output. Purple boxes (Q, K, V) are the inputs from the previous diagram. The orange boxes show the computation steps: first, Q and K interact via matrix multiplication (Q @ K^T) and scaling by \u221ad to produce attention scores with shape (seq \u00d7 seq)\u2014a matrix where entry (i,j) indicates how much token i should attend to token j. Softmax normalizes each row to sum to 1, creating a probability distribution. Notice V \"waits\" on the side\u2014it doesn't participate until after softmax. Then the attention weights multiply V (weights @ V), creating a weighted combination of value vectors for each position. The green output box has the same shape as the input (seq \u00d7 d_model), showing how attention transforms representations while preserving dimensions. This shape preservation enables stacking many transformer layers.</p>"},{"location":"appendices/transformer-architecture/#from-scratch-pytorch-implementation","title":"From-Scratch PyTorch Implementation","text":""},{"location":"appendices/transformer-architecture/#self-attention-module","title":"Self-Attention Module","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\n\nclass MultiHeadSelfAttention(nn.Module):\n    \"\"\"Multi-head self-attention from scratch.\"\"\"\n\n    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):\n        super().__init__()\n\n        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.d_k = d_model // n_heads\n\n        # Learnable projection matrices\n        self.W_Q = nn.Linear(d_model, d_model)\n        self.W_K = nn.Linear(d_model, d_model)\n        self.W_V = nn.Linear(d_model, d_model)\n        self.W_O = nn.Linear(d_model, d_model)\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -&gt; torch.Tensor:\n        batch_size, seq_len, _ = x.shape\n\n        # Step 1: Linear projections\n        Q = self.W_Q(x)\n        K = self.W_K(x)\n        V = self.W_V(x)\n\n        # Step 2: Reshape for multi-head attention\n        Q = Q.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n        K = K.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n        V = V.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n\n        # Step 3: Compute attention scores\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n\n        # Step 4: Apply mask (optional)\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, float('-inf'))\n\n        # Step 5: Softmax\n        attn_weights = F.softmax(scores, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n\n        # Step 6: Apply attention to values\n        context = torch.matmul(attn_weights, V)\n\n        # Step 7: Concatenate heads\n        context = context.transpose(1, 2).contiguous()\n        context = context.view(batch_size, seq_len, self.d_model)\n\n        # Step 8: Output projection\n        output = self.W_O(context)\n\n        return output\n</code></pre>"},{"location":"appendices/transformer-architecture/#feed-forward-network","title":"Feed-Forward Network","text":"<pre><code>class FeedForward(nn.Module):\n    \"\"\"Position-wise feed-forward network.\"\"\"\n\n    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n        super().__init__()\n        self.fc1 = nn.Linear(d_model, d_ff)\n        self.fc2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = self.fc1(x)           # Expand\n        x = F.gelu(x)             # Non-linearity\n        x = self.dropout(x)\n        x = self.fc2(x)           # Contract\n        return x\n</code></pre>"},{"location":"appendices/transformer-architecture/#transformer-block","title":"Transformer Block","text":"<pre><code>class TransformerBlock(nn.Module):\n    \"\"\"A single transformer block.\"\"\"\n\n    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):\n        super().__init__()\n\n        self.attention = MultiHeadSelfAttention(d_model, n_heads, dropout)\n        self.ffn = FeedForward(d_model, d_ff, dropout)\n        self.ln1 = nn.LayerNorm(d_model)\n        self.ln2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -&gt; torch.Tensor:\n        # Attention with residual connection\n        normed = self.ln1(x)\n        attn_out = self.attention(normed, mask=mask)\n        x = x + self.dropout(attn_out)\n\n        # FFN with residual connection\n        normed = self.ln2(x)\n        ffn_out = self.ffn(normed)\n        x = x + self.dropout(ffn_out)\n\n        return x\n</code></pre>"},{"location":"appendices/transformer-architecture/#complete-decoder-only-transformer-gpt-style","title":"Complete Decoder-Only Transformer (GPT-style)","text":"<pre><code>class TransformerDecoder(nn.Module):\n    \"\"\"A complete decoder-only transformer (GPT-style).\"\"\"\n\n    def __init__(\n        self,\n        vocab_size: int,\n        d_model: int,\n        n_heads: int,\n        n_layers: int,\n        d_ff: int,\n        max_seq_len: int,\n        dropout: float = 0.1,\n        tie_weights: bool = True\n    ):\n        super().__init__()\n\n        self.token_embedding = nn.Embedding(vocab_size, d_model)\n        self.pos_embedding = nn.Embedding(max_seq_len, d_model)\n\n        self.layers = nn.ModuleList([\n            TransformerBlock(d_model, n_heads, d_ff, dropout)\n            for _ in range(n_layers)\n        ])\n\n        self.ln_final = nn.LayerNorm(d_model)\n        self.output_head = nn.Linear(d_model, vocab_size, bias=False)\n\n        if tie_weights:\n            self.output_head.weight = self.token_embedding.weight\n\n    def forward(self, input_ids: torch.Tensor) -&gt; torch.Tensor:\n        batch_size, seq_len = input_ids.shape\n\n        # Create causal mask\n        mask = torch.tril(torch.ones(seq_len, seq_len, device=input_ids.device))\n\n        # Why this mask: The lower triangular matrix (tril) has 1s below and on\n        # the diagonal, 0s above. Position i can only attend to positions \u2264 i.\n        # During training on \"The cat sat\", when predicting \"sat\", the model\n        # sees [\"The\", \"cat\"] but not \"sat\" itself or anything after.\n        # This matches generation: when producing token 3, you only have tokens 0-2.\n        # Without masking, the model would \"cheat\" during training by looking ahead,\n        # then fail at generation when future tokens don't exist.\n\n        # Embeddings\n        positions = torch.arange(seq_len, device=input_ids.device)\n        x = self.token_embedding(input_ids) + self.pos_embedding(positions)\n\n        # Transformer blocks\n        for layer in self.layers:\n            x = layer(x, mask=mask)\n\n        # Output\n        x = self.ln_final(x)\n        logits = self.output_head(x)\n\n        return logits\n</code></pre> <p>Weight tying: Input embedding maps tokens to semantic space; output projection maps back. Sharing weights (saves ~15M parameters) makes sense since they're conceptually inverse operations. Empirically, tying usually helps or is neutral\u2014the shared weights get more training signal, providing regularization. Some very large models untie for more expressivity, but tying is a sensible default.</p> <p>Numerical Example: Causal Masking</p> <pre><code>import numpy as np\n\n# Attention scores before masking (4 tokens)\nnp.random.seed(42)\nscores = np.random.randn(4, 4).round(2)\nprint(\"Raw attention scores:\")\nprint(scores)\n\n# Create causal mask (upper triangle = -inf)\nmask = np.triu(np.ones((4, 4)) * float('-inf'), k=1)\nmasked = scores + mask\n\n# Softmax each row\ndef softmax_rows(x):\n    exp_x = np.exp(np.where(x == float('-inf'), -1e9, x))\n    return exp_x / exp_x.sum(axis=1, keepdims=True)\n\nattn = softmax_rows(masked)\nprint(\"\\nAttention weights after masking:\")\nprint(attn.round(3))\n</code></pre> <p>Output: <pre><code>Raw attention scores:\n[[ 0.5  -0.14  0.65  1.52]\n [-0.23 -0.23  1.58  0.77]\n [-0.47  0.54 -0.46 -0.47]\n [ 0.24 -1.91 -1.72 -0.56]]\n\nAttention weights after masking:\n[[1.    0.    0.    0.   ]\n [0.5   0.5   0.    0.   ]\n [0.21  0.577 0.212 0.   ]\n [0.586 0.068 0.083 0.263]]\n</code></pre></p> <p>Interpretation: Token 0 can only attend to itself (weight 1.0). Token 1 splits attention between tokens 0-1. Token 2 distributes attention across 0-2. Token 3 sees everything. The upper triangle zeros (from -inf \u2192 exp(-inf) \u2248 0) enforce left-to-right causality\u2014no peeking at future tokens during generation.</p> <p>Source: <code>slide_computations/deep_dive_transformer_examples.py</code> - <code>demo_causal_masking()</code></p>"},{"location":"appendices/transformer-architecture/#common-misconceptions","title":"Common Misconceptions","text":"Misconception Reality \"The attention weights are the main learnable parameters\" W_Q, W_K, W_V, W_O are learned. Attention weights are computed dynamically. \"More attention heads is always better\" Each head gets smaller d_k. Diminishing returns exist. \"Self-attention is expensive because of parameters\" It's expensive because of O(n\u00b2) computation in the attention matrix. \"Transformers understand language\" Transformers learn statistical patterns, not \"understanding.\" \"Attention visualizations show what the model 'thinks'\" Attention weights don't always correlate with importance."},{"location":"appendices/transformer-architecture/#reflection-questions","title":"Reflection Questions","text":"<ol> <li> <p>If you increase the number of attention heads but keep d_model fixed, what happens to d_k? What's the trade-off?</p> </li> <li> <p>Why does the FFN typically expand to 4\u00d7 the model dimension?</p> </li> <li> <p>Where would you expect most of the \"knowledge\" to be stored\u2014attention weights or FFN weights?</p> </li> <li> <p>Why do we scale by \u221ad_k in the attention formula?</p> </li> <li> <p>Why do we need the output projection W_O after concatenating heads?</p> </li> <li> <p>What happens if we remove the residual connections?</p> </li> </ol>"},{"location":"appendices/transformer-architecture/#practice-problems","title":"Practice Problems","text":"<ol> <li> <p>Calculate the total parameter count for GPT-2 (d_model=768, n_heads=12, n_layers=12, vocab=50257)</p> </li> <li> <p>Trace the dimensions through cross-attention (encoder-decoder attention)</p> </li> <li> <p>Implement masked self-attention for causal language modeling</p> </li> </ol>"},{"location":"appendices/transformer-architecture/#summary","title":"Summary","text":"<p>Key takeaways:</p> <ol> <li> <p>Token embeddings contain the most parameters in small transformers (~15M for 30K vocab)</p> </li> <li> <p>Attention parameters: W_Q, W_K, W_V, W_O project inputs to queries, keys, values, and combine head outputs</p> </li> <li> <p>FFN parameters often dominate in larger transformers (2\u00d7 the attention parameters per layer)</p> </li> <li> <p>Multi-head attention doesn't add parameters\u2014it reshapes existing projections</p> </li> <li> <p>The dimension flow: (B, T, d_model) \u2192 project \u2192 reshape for heads \u2192 attention \u2192 concatenate \u2192 project \u2192 (B, T, d_model)</p> </li> <li> <p>Why it works: O(1) path length, parallelization, content-dependent connections, and non-linearity from FFN</p> </li> <li> <p>Understanding the architecture helps you reason about capacity, compute requirements, and what the model might be learning</p> </li> </ol>"},{"location":"appendices/universal-approximators/","title":"Deep Dive: Neural Networks as Universal Approximators","text":"<p>Extends Module 6: Neural Networks</p>"},{"location":"appendices/universal-approximators/#introduction","title":"Introduction","text":"<p>In Module 6, we learned how to build and train neural networks. But there's a deeper question: why can neural networks learn such a wide variety of functions?</p> <p>The answer lies in the Universal Approximation Theorem\u2014one of the most important theoretical results in deep learning. This theorem tells us that neural networks can, in principle, learn any reasonable function.</p> <p>But here's the surprising part: many classical ML models you already know are actually special cases of neural networks. Linear regression, logistic regression, even aspects of decision trees\u2014they're all points on a spectrum of neural network complexity.</p> <p>This deep dive explores these connections and helps you understand when simple models suffice and when you need the full power of neural networks.</p> <p>Why research continues: The Universal Approximation Theorem is an existence result\u2014it guarantees a solution exists but says nothing about finding it. \"Enough neurons\" might mean exponentially many. The theorem provides no guarantees about training difficulty, data requirements, or generalization. Depth enables compositional learning (f(g(h(x))) is more parameter-efficient than one massive layer). The gap between \"theoretically possible\" and \"practically achievable\" drives ongoing research.</p>"},{"location":"appendices/universal-approximators/#the-universal-approximation-theorem","title":"The Universal Approximation Theorem","text":""},{"location":"appendices/universal-approximators/#statement","title":"Statement","text":"<p>Theorem (Cybenko, 1989; Hornik, 1991):</p> <p>A feedforward neural network with a single hidden layer containing a finite number of neurons can approximate any continuous function on a compact subset of \\(\\mathbb{R}^n\\) to arbitrary accuracy.</p> <p>In simpler terms: Given enough hidden neurons, a one-hidden-layer neural network can learn any reasonable function.</p>"},{"location":"appendices/universal-approximators/#intuition-neurons-as-bump-functions","title":"Intuition: Neurons as \"Bump\" Functions","text":"<p>Each hidden neuron with a sigmoid activation creates an S-shaped curve:</p> <p></p> <p>Reading the diagram: The left panel shows a standard sigmoid with weight=1: the output changes gradually from 0 to 1 over a wide x range (roughly -4 to +4). The right panel shows a sigmoid with weight=5: the transition is compressed into a narrow band around x=0, creating an almost step-like function. The dashed horizontal line at y=0.5 marks the decision threshold. Higher weights \u2192 sharper transitions \u2192 more step-like behavior. This is how sigmoid neurons can approximate hard decision boundaries when needed.</p> <p>Numerical Example: Sigmoid Weight Effects</p> <pre><code>import numpy as np\n\ndef sigmoid(x, weight):\n    return 1 / (1 + np.exp(-weight * x))\n\n# Transition width: x range where output goes from 0.1 to 0.9\n# Width = 2*ln(9)/weight \u2248 4.39/weight\nfor w in [1, 2, 5, 10]:\n    width = 2 * np.log(9) / w\n    print(f\"Weight={w}: transition width = {width:.3f}\")\n</code></pre> <p>Output: <pre><code>Weight=1: transition width = 4.394\nWeight=2: transition width = 2.197\nWeight=5: transition width = 0.879\nWeight=10: transition width = 0.439\n</code></pre></p> <p>Interpretation: Weight=1 spreads the transition across ~4.4 units of x. Weight=10 compresses it to ~0.4 units\u2014nearly a step function. This is how sigmoids can approximate hard boundaries.</p> <p>Source: <code>slide_computations/deep_dive_universal_approx_examples.py</code> - <code>demo_sigmoid_weight_effects()</code></p> <p>By combining multiple neurons: - Two sigmoids can create a \"bump\" (rise then fall) - Many bumps can approximate any shape - The more neurons, the smoother the approximation</p> <p>The \"building blocks\" analogy: Think of each neuron as a LEGO brick. Sigmoid neurons make smooth, rounded bumps; ReLU neurons make sharp corners. The Universal Approximation Theorem says: with enough bricks of the right type, you can build any structure. The catch? It doesn't tell you how many bricks you need or how to arrange them\u2014that's what training figures out.</p>"},{"location":"appendices/universal-approximators/#visual-example-approximating-a-square-wave","title":"Visual Example: Approximating a Square Wave","text":"<p>Reading the diagram: Three panels showing the Universal Approximation Theorem in action. Left panel: the target function\u2014a shape with two distinct bumps that we want to learn. Middle panel: a 2-neuron network produces a rough approximation\u2014it can only create about 2 bumps, so it misses the details. Right panel: a 10-neuron network closely matches the target shape. The key insight: more neurons = finer control over the function's shape. Each additional neuron lets the network add another \"building block\" to construct complex shapes.</p>"},{"location":"appendices/universal-approximators/#important-caveats","title":"Important Caveats","text":"<p>The theorem tells us approximation is possible, but NOT: 1. How many neurons are needed (could be astronomically large) 2. How to find the right weights (training might fail) 3. How much data is required (might need infinite samples) 4. Whether it will generalize (might just memorize)</p> <p>The \"blueprint vs building\" analogy: The Universal Approximation Theorem is like knowing that a blueprint for your dream house exists somewhere\u2014it doesn't hand you the blueprint, tell you where to find it, or help you build the house. It's an existence proof (\"a solution exists\") not a construction method (\"here's how to find it\"). This gap between \"theoretically possible\" and \"practically achievable\" is why neural network research continues despite this 35-year-old theorem.</p>"},{"location":"appendices/universal-approximators/#example-1-linear-regression","title":"Example 1: Linear Regression","text":"<p>The simplest case: Neural networks can perform linear regression exactly.</p>"},{"location":"appendices/universal-approximators/#linear-regression-model","title":"Linear Regression Model","text":"\\[\\hat{y} = w_1 x_1 + w_2 x_2 + b\\] <p>For example: \\(\\hat{y} = 2x_1 + 3x_2 + 1\\)</p>"},{"location":"appendices/universal-approximators/#neural-network-equivalent","title":"Neural Network Equivalent","text":"<p>A neural network with: - No hidden layers - Linear activation (or no activation function) - Single output neuron</p> <pre><code>Architecture:\n    x\u2081 \u2500\u2500(w\u2081)\u2500\u2500\u2510\n               \u251c\u2500\u2500 \u03a3 + b \u2500\u2500 \u0177\n    x\u2082 \u2500\u2500(w\u2082)\u2500\u2500\u2518\n</code></pre>"},{"location":"appendices/universal-approximators/#code-demonstration","title":"Code Demonstration","text":"<p>Numerical Example: sklearn vs PyTorch Linear Regression</p> <pre><code>import numpy as np\nimport torch\nimport torch.nn as nn\nfrom sklearn.linear_model import LinearRegression\n\n# Generate data: y = 2*x1 + 3*x2 + 1 + noise\nnp.random.seed(42)\nX = np.random.randn(1000, 2).astype(np.float32)\ny = 2 * X[:, 0] + 3 * X[:, 1] + 1 + 0.1 * np.random.randn(1000).astype(np.float32)\n\n# SKLEARN\nsklearn_model = LinearRegression()\nsklearn_model.fit(X, y)\n\n# PYTORCH (train for 2000 epochs)\nnn_model = nn.Linear(2, 1)\noptimizer = torch.optim.Adam(nn_model.parameters(), lr=0.01)\nX_t, y_t = torch.from_numpy(X), torch.from_numpy(y).reshape(-1, 1)\nfor _ in range(2000):\n    loss = nn.MSELoss()(nn_model(X_t), y_t)\n    optimizer.zero_grad(); loss.backward(); optimizer.step()\n</code></pre> <p>Output: <pre><code>                      True    sklearn    PyTorch\n--------------------------------------------------\n x1 coefficient     2.0000     2.0028     2.0028\n x2 coefficient     3.0000     3.0017     3.0017\n      intercept     1.0000     1.0004     1.0004\n\nMax coefficient difference: 0.000005\n</code></pre></p> <p>Interpretation: Both methods converge to essentially identical coefficients. sklearn uses a closed-form matrix inverse; PyTorch uses gradient descent. Same answer, different paths.</p> <p>Source: <code>slide_computations/deep_dive_universal_approx_examples.py</code> - <code>demo_linear_regression_equiv()</code></p>"},{"location":"appendices/universal-approximators/#key-insight","title":"Key Insight","text":"<p>Linear regression IS a neural network with: - 0 hidden layers - Linear (identity) activation - MSE loss</p> <p>The sklearn <code>LinearRegression</code> uses a closed-form solution (matrix inverse), while the neural network uses gradient descent, but they converge to the same answer.</p> <p>Why use gradient descent for linear regression? The closed-form solution requires O(n\u00b3) matrix inversion\u2014prohibitive for millions of samples or thousands of features. Stochastic gradient descent scales via mini-batches. Also, closed-form only works for squared error with linear models. Gradient descent handles L1 regularization, non-standard losses, and extends to non-linear models. Use closed-form when small enough; gradient descent otherwise.</p>"},{"location":"appendices/universal-approximators/#example-2-logistic-regression","title":"Example 2: Logistic Regression","text":"<p>Binary classification: Neural networks can implement logistic regression exactly.</p>"},{"location":"appendices/universal-approximators/#logistic-regression-model","title":"Logistic Regression Model","text":"\\[P(y=1|x) = \\sigma(w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}\\]"},{"location":"appendices/universal-approximators/#neural-network-equivalent_1","title":"Neural Network Equivalent","text":"<p>A neural network with: - No hidden layers - Sigmoid activation on the output - Binary cross-entropy loss</p> <pre><code>Architecture:\n    x\u2081 \u2500\u2500(w\u2081)\u2500\u2500\u2510\n               \u251c\u2500\u2500 \u03a3 + b \u2500\u2500 \u03c3 \u2500\u2500 P(y=1)\n    x\u2082 \u2500\u2500(w\u2082)\u2500\u2500\u2518\n</code></pre>"},{"location":"appendices/universal-approximators/#code-demonstration_1","title":"Code Demonstration","text":"<p>Numerical Example: sklearn vs PyTorch Logistic Regression</p> <pre><code>import numpy as np\nimport torch\nimport torch.nn as nn\nfrom sklearn.linear_model import LogisticRegression\n\n# Binary classification: decision boundary x1 + 2*x2 = 0.5\nnp.random.seed(42)\nX = np.random.randn(1000, 2).astype(np.float32)\ny = (X[:, 0] + 2 * X[:, 1] &gt; 0.5).astype(np.float32)\n\n# SKLEARN\nsklearn_model = LogisticRegression(C=1e10, max_iter=1000)  # C=1e10 \u2248 no regularization\nsklearn_model.fit(X, y)\n\n# PYTORCH\nnn_model = nn.Sequential(nn.Linear(2, 1), nn.Sigmoid())\noptimizer = torch.optim.Adam(nn_model.parameters(), lr=0.05)\nX_t, y_t = torch.from_numpy(X), torch.from_numpy(y).reshape(-1, 1)\nfor _ in range(2000):\n    loss = nn.BCELoss()(nn_model(X_t), y_t)\n    optimizer.zero_grad(); loss.backward(); optimizer.step()\n</code></pre> <p>Output: <pre><code>                     sklearn      PyTorch\n------------------------------------------\n x1 coefficient     108.4445       9.4888\n x2 coefficient     216.0017      18.5272\n      intercept     -53.9592      -4.4948\n\nAccuracy:  sklearn=1.0000, PyTorch=0.9990\nCoefficient ratio (x2/x1): sklearn=1.99, PyTorch=1.95\n</code></pre></p> <p>Interpretation: Coefficient magnitudes differ (logistic regression coefficients are only unique up to scale), but the ratio is the same (~2:1), so both learn the same decision boundary. Both achieve near-perfect accuracy.</p> <p>Source: <code>slide_computations/deep_dive_universal_approx_examples.py</code> - <code>demo_logistic_regression_equiv()</code></p>"},{"location":"appendices/universal-approximators/#key-insight_1","title":"Key Insight","text":"<p>Logistic regression IS a neural network with: - 0 hidden layers - Sigmoid activation - Binary cross-entropy loss</p> <p>The decision boundary is identical: a linear hyperplane.</p>"},{"location":"appendices/universal-approximators/#example-3-approximating-step-functions","title":"Example 3: Approximating Step Functions","text":"<p>The challenge: Decision trees create axis-aligned step functions. Can neural networks do this?</p>"},{"location":"appendices/universal-approximators/#decision-tree-behavior","title":"Decision Tree Behavior","text":"<pre><code>Decision tree output:\n     1 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502           \u2502\n       \u2502           \u2502\n     0 \u2518           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n              x = 0.5\n       (hard step at threshold)\n</code></pre>"},{"location":"appendices/universal-approximators/#neural-network-approximation","title":"Neural Network Approximation","text":"<p>A ReLU neuron creates a \"bent line\":</p> \\[\\text{ReLU}(x) = \\max(0, x)\\] <p></p> <p>Reading the diagram: The ReLU (Rectified Linear Unit) function outputs 0 for all negative inputs (the flat horizontal line on the left) and passes positive inputs unchanged (the diagonal line on the right). The \"kink\" at x=0 is the key feature\u2014this sharp corner is what makes ReLU networks piecewise linear. Each hidden neuron contributes one such kink to the overall function. The weight and bias of each neuron control where its kink appears and how steep the ramp is.</p> <p>Combining ReLUs can approximate steps:</p> <pre><code>def train_step_approximator(n_hidden):\n    \"\"\"Train a network to approximate a step function.\"\"\"\n    model = nn.Sequential(\n        nn.Linear(1, n_hidden),\n        nn.ReLU(),\n        nn.Linear(n_hidden, 1),\n        nn.Sigmoid()\n    )\n    # ... training code ...\n    return model\n\n# Results:\n# 2 neurons: rough approximation\n# 5 neurons: better\n# 50 neurons: nearly exact step\n</code></pre> <p>Numerical Example: Step Function Approximation</p> <pre><code># Train networks with different widths on step function at x=0\n# Target: output 0 for x&lt;0, output 1 for x&gt;0\nneuron_counts = [2, 5, 10, 20, 50]\n# ... training code ...\n</code></pre> <p>Output: <pre><code>   Neurons          MSE              Quality\n---------------------------------------------\n         2       0.0017         Nearly exact\n         5       0.0020         Nearly exact\n        10       0.0015         Nearly exact\n        20       0.0007         Nearly exact\n        50       0.0006         Nearly exact\n</code></pre></p> <p>Interpretation: Even 2 neurons can approximate a step function reasonably well (MSE ~0.002). The sigmoid output naturally creates a smooth approximation. More neurons reduce error further, but diminishing returns set in quickly for this simple target.</p> <p>Source: <code>slide_computations/deep_dive_universal_approx_examples.py</code> - <code>demo_step_approximation()</code></p> <p>Observation: More neurons \u2192 sharper approximation of the step.</p>"},{"location":"appendices/universal-approximators/#why-relu-combinations-work","title":"Why ReLU Combinations Work","text":"<p>A single ReLU is a ramp: \\(\\max(0, x)\\)</p> <p>Two ReLUs can make a bump:</p> \\[f(x) = \\text{ReLU}(x) - \\text{ReLU}(x - 1)\\] <p>The \"kink budget\" intuition: Each hidden neuron in a ReLU network adds one potential \"kink\" (corner) to your function. A network with 10 hidden neurons can create a function with at most 10 corners. How many kinks do you need? A degree-n polynomial requires roughly n kinks to approximate well. A step function needs just a few. But something wiggly like sin(x) over multiple periods might need 50+ kinks for a good approximation. The Universal Approximation Theorem says \"enough kinks exist\"\u2014but you discover how many through experimentation.</p> <p></p> <p>Reading the diagram: Three panels showing how to build a \"bump\" from two ReLUs. Left panel: ReLU(x) starts ramping up at x=0. Middle panel: ReLU(x-1) starts ramping up at x=1 (shifted by the bias). Right panel: subtracting them (ReLU(x) - ReLU(x-1)) creates a function that rises from 0 to 1, then plateaus. The shaded orange area shows the \"bump\"\u2014a building block for approximating any shape. By positioning many such bumps at different locations and scaling them by different amounts, you can construct arbitrarily complex functions.</p> <p>Numerical Example: ReLU Bump Construction</p> <pre><code>import numpy as np\n\ndef relu(x):\n    return np.maximum(0, x)\n\n# bump(x) = ReLU(x) - ReLU(x-1)\nfor x in [-1, 0, 0.5, 1, 1.5, 2, 3]:\n    r1 = relu(x)\n    r2 = relu(x - 1)\n    bump = r1 - r2\n    print(f\"x={x:4.1f}: ReLU(x)={r1:.2f}, ReLU(x-1)={r2:.2f}, bump={bump:.2f}\")\n</code></pre> <p>Output: <pre><code>     x    ReLU(x)    ReLU(x-1)   Difference   Shape\n------------------------------------------------------------\n  -1.0       0.00         0.00         0.00   Before ramp\n   0.0       0.00         0.00         0.00   Rising\n   0.5       0.50         0.00         0.50   Rising\n   1.0       1.00         0.00         1.00   Plateau at 1\n   1.5       1.50         0.50         1.00   Plateau at 1\n   2.0       2.00         1.00         1.00   Plateau at 1\n</code></pre></p> <p>Interpretation: The bump rises from 0 to 1 as x goes from 0 to 1, then stays at 1 forever. The second ReLU \"catches up\" and cancels further growth. This plateau behavior is the key to building arbitrary shapes.</p> <p>Source: <code>slide_computations/deep_dive_universal_approx_examples.py</code> - <code>demo_relu_bump_construction()</code></p> <p>Many bumps at different positions \u2192 approximate any shape.</p>"},{"location":"appendices/universal-approximators/#key-insight_2","title":"Key Insight","text":"<p>Neural networks can approximate decision tree boundaries, but: - Trees create exact axis-aligned steps - NNs create smooth approximations that approach steps - With enough neurons, the approximation becomes arbitrarily close</p>"},{"location":"appendices/universal-approximators/#example-4-polynomial-regression","title":"Example 4: Polynomial Regression","text":"<p>Challenge: Learn \\(y = x^2\\) using only linear layers and ReLU.</p>"},{"location":"appendices/universal-approximators/#the-problem","title":"The Problem","text":"<p>A single linear layer can only learn: \\(y = wx + b\\)</p> <p>How can we learn \\(y = x^2\\) without explicitly computing \\(x^2\\)?</p>"},{"location":"appendices/universal-approximators/#the-solution-hidden-layer-creates-basis-functions","title":"The Solution: Hidden Layer Creates Basis Functions","text":"<p>With ReLU activations, the network learns piecewise linear approximations:</p> <pre><code># Train networks with different widths to learn y = x\u00b2\n# Results:\n# 2 neurons: Very rough (2 line segments)\n# 5 neurons: Better (5 segments)\n# 50 neurons: Nearly perfect curve\n# 100 neurons: Indistinguishable from x\u00b2\n</code></pre> <p>Numerical Example: Learning x\u00b2 with ReLU Networks</p> <pre><code># Train networks with different widths on y = x\u00b2\n# Input: x in [-2, 2], output: x\u00b2\nneuron_counts = [2, 5, 10, 20, 50]\n# ... training code ...\n</code></pre> <p>Output: <pre><code>   Neurons          MSE    Max Kinks        Approximation\n------------------------------------------------------------\n         2     0.095136            2     Coarse piecewise\n         5     0.001931            5       Fine piecewise\n        10     0.000923           10        Nearly smooth\n        20     0.000436           20        Nearly smooth\n        50     0.000028           50        Nearly smooth\n</code></pre></p> <p>Interpretation: With just 2 neurons, MSE is ~0.1 (visible error). By 50 neurons, MSE drops to 0.00003\u2014visually indistinguishable from the true parabola. Each neuron adds one potential \"kink\" where the piecewise linear function can change slope.</p> <p>Source: <code>slide_computations/deep_dive_universal_approx_examples.py</code> - <code>demo_polynomial_approximation()</code></p>"},{"location":"appendices/universal-approximators/#whats-happening-inside","title":"What's Happening Inside","text":"<p>Each ReLU neuron contributes a \"kink\" in the function:</p> <pre><code>Hidden neuron 1: ReLU(w\u2081x + b\u2081) - kink at x = -b\u2081/w\u2081\nHidden neuron 2: ReLU(w\u2082x + b\u2082) - kink at x = -b\u2082/w\u2082\n...\n\nOutput = sum of scaled, shifted kinks = piecewise linear approximation\n</code></pre> <p>The network learns WHERE to put kinks and HOW MUCH each contributes.</p>"},{"location":"appendices/universal-approximators/#key-insight_3","title":"Key Insight","text":"<p>Neural networks don't explicitly compute polynomials\u2014they approximate them with piecewise linear functions. More neurons = more pieces = smoother approximation.</p> <p>Piecewise linear vs truly smooth: It's worth emphasizing what ReLU networks actually learn. A 10-neuron network creates a function with at most 10 \"corner points.\" Between any two corners, the function is perfectly straight\u2014a line segment. This is fundamentally different from polynomial regression, which creates genuinely smooth curves. The ReLU network approximates x\u00b2 with a sequence of connected line segments. From far away it looks smooth, but zoom in and you'll see the corners. This isn't a flaw\u2014it's a feature that makes training tractable.</p>"},{"location":"appendices/universal-approximators/#the-unifying-framework","title":"The Unifying Framework","text":"<p>All these models are points on a spectrum:</p> <p></p> <p>Reading the diagram: An arrow from \"Simple\" to \"Complex\" shows the spectrum of model flexibility. Blue dots (Linear Regression, Logistic Regression) are neural network special cases with 0 hidden layers\u2014they can only learn linear decision boundaries. Purple dots (Polynomial Regression, Decision Tree) add non-linearity but in constrained ways. The red dot (Deep Neural Net) represents maximum flexibility\u2014can approximate any function. Position on this spectrum reflects model capacity, not model quality. The right model depends on your data size, interpretability needs, and the true complexity of the underlying relationship.</p>"},{"location":"appendices/universal-approximators/#flexibility-trade-offs","title":"Flexibility Trade-offs","text":"Model Flexibility Data Needed Interpretability Overfitting Risk Linear Regression Low Low High Low Logistic Regression Low Low High Low Decision Tree Medium Medium High Medium Shallow NN High Medium Low Medium Deep NN Very High High Very Low High"},{"location":"appendices/universal-approximators/#when-to-use-simpler-models","title":"When to Use Simpler Models","text":"<p>Even though NNs can do everything, simpler models are often better:</p> <ol> <li>Interpretability required: Linear/logistic regression coefficients are meaningful</li> <li>Small data: Simple models generalize better with few samples</li> <li>Fast inference: Linear prediction is O(d), deep NN is O(millions)</li> <li>Debugging: Easier to understand what went wrong</li> <li>Baseline: Always try simple models first</li> </ol> <p>Occam's Razor for model selection: Start with the simplest model that might work. If linear regression achieves R\u00b2=0.95, you're probably done\u2014a neural network won't meaningfully improve and adds complexity, training time, and interpretability costs. Only add complexity when simpler models demonstrably fail. The burden of proof is on complexity: a neural network must earn its place by outperforming the simple baseline by enough to justify its costs.</p> <p>Numerical Example: Width vs Accuracy Trade-off</p> <p>How many neurons do you need for different target functions?</p> <pre><code># Train networks on three different targets, find minimum neurons for MSE thresholds\nfunctions = [\"Step at 0\", \"x\u00b2\", \"sin(2*pi*x)\"]\n# ... training code for various neuron counts ...\n</code></pre> <p>Output: <pre><code>       Function      MSE &lt; 0.01     MSE &lt; 0.001\n--------------------------------------------------\n      Step at 0               5               5\n            x\u00b2              10              20\n    sin(2*pi*x)              75             100\n</code></pre></p> <p>Interpretation: Step functions are \"easy\"\u20145 neurons suffice. Polynomials need moderate width. Oscillating functions like sin(x) are \"hard\"\u2014they need many neurons to capture all the wiggles. Rule of thumb: more \"wiggly\" = more neurons needed.</p> <p>Source: <code>slide_computations/deep_dive_universal_approx_examples.py</code> - <code>demo_width_vs_accuracy()</code></p>"},{"location":"appendices/universal-approximators/#example-decision","title":"Example Decision","text":"<p>You have 100 samples and want to predict a continuous outcome. Neural network or linear regression?</p> <p>Start with linear regression. With 100 samples, a neural network will likely overfit unless heavily regularized. Linear regression provides a strong baseline and is interpretable.</p> <p>Samples per parameter: Traditional rules (10-20 samples per parameter) don't apply cleanly to neural networks. Modern NNs often work in the overparameterized regime (more parameters than samples) due to implicit regularization from SGD, early stopping, and batch normalization. The honest answer: monitor validation loss. If it diverges from training loss, you're overfitting\u2014apply more regularization, get more data, or use a smaller model.</p>"},{"location":"appendices/universal-approximators/#when-neural-networks-shine","title":"When Neural Networks Shine","text":"<ol> <li>Large data: More samples \u2192 can fit more complex patterns</li> <li>Raw inputs: Images, audio, text need feature learning</li> <li>Complex relationships: Highly non-linear, interacting features</li> <li>Transfer learning: Pre-trained models for your task</li> </ol>"},{"location":"appendices/universal-approximators/#example-decision_1","title":"Example Decision","text":"<p>You have 1 million images and want to classify them. Neural network or logistic regression?</p> <p>Neural network, specifically a CNN. Logistic regression would require hand-engineered features and couldn't capture the spatial patterns that CNNs learn automatically.</p>"},{"location":"appendices/universal-approximators/#common-misconceptions","title":"Common Misconceptions","text":"Misconception Reality \"Neural networks understand data better than simpler models\" NNs fit patterns statistically; simpler models may capture the true underlying structure better \"Universal approximation means NNs are always best\" The theorem says nothing about training difficulty, data requirements, or generalization \"More neurons is always better\" More neurons = more capacity to overfit; regularization and data size matter \"Deep networks are always better than shallow\" For some functions, shallow networks are more efficient; depth helps for compositional structure \"If sklearn and PyTorch give same results, there's no benefit to NNs\" True for simple models, but NNs allow extending to more complex architectures"},{"location":"appendices/universal-approximators/#reflection-questions","title":"Reflection Questions","text":"<ol> <li> <p>If neural networks can approximate any function, why use simpler models?</p> </li> <li> <p>What does a neural network learn that logistic regression doesn't?</p> </li> <li> <p>How many hidden neurons are needed to approximate a degree-n polynomial?</p> </li> <li> <p>The Universal Approximation Theorem says one hidden layer is enough. Why do we use deep networks?</p> </li> <li> <p>Your neural network achieves 100% training accuracy but 60% test accuracy. What happened?</p> </li> <li> <p>A colleague says \"just use a deep neural network for everything.\" What's your response?</p> </li> </ol>"},{"location":"appendices/universal-approximators/#practice-problems","title":"Practice Problems","text":"<ol> <li> <p>Train a neural network to learn \\(\\sin(x)\\) - how many neurons needed for MSE &lt; 0.01?</p> </li> <li> <p>Implement logistic regression as a neural network and verify it matches sklearn</p> </li> <li> <p>Find the minimum network (fewest neurons) that achieves 95% accuracy on XOR</p> </li> <li> <p>Compare training time: sklearn vs PyTorch for logistic regression on a large dataset</p> </li> </ol>"},{"location":"appendices/universal-approximators/#summary","title":"Summary","text":"<p>Key takeaways:</p> <ol> <li> <p>The Universal Approximation Theorem guarantees that neural networks can learn any reasonable function (given enough neurons)</p> </li> <li> <p>Linear regression is a neural network with 0 hidden layers and no activation</p> </li> <li> <p>Logistic regression is a neural network with 0 hidden layers and sigmoid activation</p> </li> <li> <p>Neural networks approximate step functions and polynomials through piecewise linear combinations of ReLU units</p> </li> <li> <p>The theorem guarantees existence but says nothing about training difficulty, data requirements, or generalization</p> </li> <li> <p>Simpler models are often better: interpretability, fewer data requirements, faster training, less overfitting</p> </li> <li> <p>Match model complexity to problem complexity and data size</p> </li> </ol>"},{"location":"modules/01-foundations/","title":"Module 1: Foundations of Machine Learning","text":""},{"location":"modules/01-foundations/#introduction","title":"Introduction","text":"<p>This foundational module establishes everything you'll use throughout the course. The vocabulary, the concepts, the data preparation skills, the evaluation methodology\u2014all of it forms the bedrock of your machine learning practice.</p> <p>By the end of this module, you'll have a conceptual framework for understanding what machine learning actually is, how to prepare data for it, and critically, how to know whether your models are actually working. That last part\u2014evaluation\u2014is where most business ML projects go wrong, so we'll spend significant time there.</p> <p>Evaluation failures typically stem from three categories: data leakage (test set information inadvertently influences training), metric mismatch (optimizing for the wrong measure), and insufficient validation rigor (relying on a single train/test split). Teams focus on building models without equal rigor on proving they work\u2014evaluation methodology should receive as much scrutiny as model architecture.</p>"},{"location":"modules/01-foundations/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, you should be able to:</p> <ol> <li>Differentiate between AI, Machine Learning, Data Science, and related fields</li> <li>Classify business problems into appropriate ML task categories (supervised, unsupervised, reinforcement)</li> <li>Construct data preparation pipelines that prevent data leakage</li> <li>Select appropriate evaluation metrics based on business context</li> <li>Diagnose overfitting and underfitting using the bias-variance framework</li> </ol>"},{"location":"modules/01-foundations/#11-introduction-historical-context","title":"1.1 Introduction &amp; Historical Context","text":""},{"location":"modules/01-foundations/#the-aimldata-science-landscape","title":"The AI/ML/Data Science Landscape","text":"<p>Understanding how these concepts relate to each other matters because these terms are thrown around loosely in industry, and you need to cut through the hype.</p> <p></p> <p>Reading the diagram: The nested rectangles show that each inner field is a subset of the outer one\u2014not a separate domain. Deep Learning sits inside Machine Learning, which sits inside Artificial Intelligence, which sits inside Computer Science. This means every deep learning system is also a machine learning system, but not every machine learning system uses deep learning. Similarly, every ML system is AI, but rule-based expert systems are AI without being ML. Keep this hierarchy in mind when you encounter these terms\u2014they're often used interchangeably in marketing, but they have distinct technical meanings.</p> <p>At the outermost level, we have Computer Science\u2014the study of computation, information, and automation. More precisely, computer science is concerned with the theory, design, and application of algorithms: step-by-step procedures for solving problems and processing information.</p> <p>Within that sits Artificial Intelligence\u2014machines that exhibit intelligent behavior. The term was coined in 1956, but the foundations go back further\u2014Alan Turing's 1950 paper \"Computing Machinery and Intelligence\" proposed the Turing test: can a machine's responses be indistinguishable from a human's? AI is a broad umbrella that includes rule-based systems, expert systems, and machine learning.</p> <p>Inside AI, we have Machine Learning\u2014systems that learn from data without being explicitly programmed. This is our focus for the course. The key distinction is learning from data. Instead of a human writing rules, the system discovers patterns from examples.</p> <p>And inside ML, we have Deep Learning\u2014machine learning using neural networks with many layers. Deep learning has driven most of the recent AI breakthroughs, but it's just one approach within ML.</p> <p>Data Science sits alongside and overlaps with all of these. Data Science is about extracting insights from data\u2014it combines statistics, domain expertise, and programming. A data scientist might use ML, or might use traditional statistical methods, or might just create visualizations. It's about the goal (insights from data), not the method.</p> <p>In practice, these boundaries are fuzzy and a single project often spans multiple domains. A customer churn project might involve data science (exploratory analysis), machine learning (predictive model), and software engineering (deployment). The skills transfer across domains: wrangling data, building models, evaluating rigorously, and communicating effectively.</p>"},{"location":"modules/01-foundations/#key-definitions","title":"Key Definitions","text":"Term Definition Artificial Intelligence Machines that exhibit intelligence (broad umbrella) Machine Learning Systems that learn from data without being explicitly programmed Deep Learning ML using neural networks with many layers Data Science Extracting insights from data (may or may not use ML) <p>Important nuance: These terms are used loosely in industry. Job postings for \"AI Engineer\" and \"ML Engineer\" and \"Data Scientist\" often describe the same role. Help yourself by understanding what people actually mean, not just what they say. When evaluating roles, look at specific tools (SQL, Python, TensorFlow), deliverables (reports, dashboards, deployed models), and team structure rather than job titles.</p>"},{"location":"modules/01-foundations/#historical-timeline","title":"Historical Timeline","text":"<p>Understanding where ML came from helps you understand why it works the way it does\u2014and why we've seen cycles of hype and disappointment.</p>"},{"location":"modules/01-foundations/#early-years","title":"Early Years","text":"Year Milestone Significance 1847 Gradient descent published (Cauchy) The optimization algorithm that powers nearly all modern ML 1950 Turing Test proposed Defined the question \"Can machines think?\" 1957 Perceptron invented First neural network, sparked initial optimism 1969 Minsky &amp; Papert's \"Perceptrons\" Showed limitations, contributed to AI Winter 1980s Expert Systems boom Rule-based AI, eventually hit scalability limits 1984 CART published Foundation for all tree-based methods 1986 Backpropagation popularized Enabled training multi-layer networks 1997 Deep Blue beats Kasparov Specialized AI success, but not learning"},{"location":"modules/01-foundations/#modern-era","title":"Modern Era","text":"Year Milestone Significance 2001 Random Forests published Go-to algorithm for tabular data 2006 Deep learning revival; CUDA released New training techniques and GPU computing 2012 AlexNet wins ImageNet Deep learning breakthrough, GPU training 2014 XGBoost released Dominates Kaggle, still state-of-the-art for tabular data 2016 AlphaGo beats Lee Sedol Reinforcement learning milestone 2017 \"Attention Is All You Need\" Transformer architecture, foundation for GPT/BERT 2018 BERT released Transfer learning comes to NLP 2020 GPT-3 Scaling produces emergent capabilities 2022 ChatGPT released Large language models go mainstream <p>AI Winters were caused by overpromising followed by underdelivering\u2014researchers making bold claims to secure funding, then hitting fundamental limitations. The lesson is to be realistic about what current technology can and cannot do.</p> <p>The key insight for business: Most AI projects fail due to poor problem definition, not technical limitations. Getting the problem right matters more than getting the algorithm right. Poor problem definition manifests as vague objectives (\"use AI to improve customer experience\"), wrong target variables (predicting email responses when the business needs conversions), or misaligned success metrics (optimizing call duration when customer satisfaction is the goal). Before writing any code, get crystal clear on: What exactly are we predicting? How will predictions be used? What decisions will change?</p>"},{"location":"modules/01-foundations/#ml-task-categories","title":"ML Task Categories","text":"<p>Reading the diagram: This tree shows how ML problems are categorized based on what kind of feedback the algorithm receives. Start at the top (ML) and ask: \"Do I have labeled examples showing the right answer?\" If yes, go left to Supervised (blue)\u2014the algorithm learns from correct answers. If no labels exist, go to Unsupervised (purple)\u2014the algorithm finds structure on its own. The third branch, Reinforcement (orange), is different: the algorithm learns through trial-and-error feedback (rewards and penalties) rather than from a static dataset.</p> <p>Within supervised learning, the next question is: \"Am I predicting a number or a category?\" Numbers lead to Regression; categories lead to Classification. Within unsupervised, ask: \"Am I grouping similar items (Clustering) or compressing features (Dimensionality Reduction)?\"</p> <p>Machine Learning branches into three main categories:</p> <p>Supervised Learning: You have labeled data, and you want to predict labels for new data. The \"supervision\" comes from the labels\u2014they tell the algorithm what the right answer is. - Regression: Predicting continuous values (numbers)   - Examples: Sales forecasting, price prediction, demand estimation - Classification: Predicting categories (discrete labels)   - Examples: Spam detection, customer churn, fraud detection</p> <p>Unsupervised Learning: No labels. You're trying to find hidden structure in the data. - Clustering: Grouping similar items together   - Examples: Customer segmentation, document grouping, anomaly detection - Dimensionality Reduction: Compressing many features into fewer features   - Examples: Visualization, noise reduction, feature extraction</p> <p>Reinforcement Learning: The algorithm learns optimal actions through trial and error, receiving rewards or penalties for its choices. - Examples: Game playing, robotics, recommendation systems - Brief overview only\u2014not a focus of this course</p> <p>The choice between regression and classification depends on what decision the prediction enables. If the business needs a specific number (\"How many units will we sell?\"), that's regression. If it needs a category (\"Will this customer churn?\"), that's classification. Many problems could be framed either way\u2014a common pattern is to build a regression model (predict probability) and threshold it for classification decisions, giving you both continuous scores for prioritization and discrete classes for action.</p>"},{"location":"modules/01-foundations/#business-applications-by-industry","title":"Business Applications by Industry","text":"Industry Application ML Type Retail Demand forecasting Regression Finance Fraud detection Classification Marketing Customer segmentation Clustering Healthcare Disease diagnosis Classification Manufacturing Predictive maintenance Classification E-commerce Product recommendations Various <p>Classification appears frequently in business because we often need to make decisions: approve or deny, flag or pass, target or ignore. Pedagogically, we learn regression first because it's simpler\u2014you can visualize a line through points, and the loss function (MSE) is intuitive. Many core concepts (features, coefficients, overfitting, regularization) work identically in both settings, so learning them in the simpler regression context means you can focus on concepts rather than classification-specific complications.</p>"},{"location":"modules/01-foundations/#common-misconceptions","title":"Common Misconceptions","text":"Misconception Reality \"AI and ML are the same thing\" ML is a subset of AI. AI includes rule-based systems that don't learn from data. \"ML will replace all human decision-making\" ML augments human decisions. Many problems require human judgment, ethics, and contextual understanding. \"Deep Learning is always better than traditional ML\" Deep learning requires lots of data and compute. For tabular business data, traditional ML (XGBoost, Random Forest) often wins. \"More data always leads to better models\" Data quality matters more than quantity. Biased or noisy data leads to biased or noisy models. <p>How much data is \"enough\"? For classical ML, a common rule of thumb is 10-30 samples per feature for linear models. For deep learning, you typically need thousands to millions of samples, though transfer learning reduces this. The practical test: plot learning curves. If validation performance is still improving as you add data, you need more. If it's plateaued, more data won't help\u2014you need better features or a different model.</p>"},{"location":"modules/01-foundations/#12-data-preparation-feature-engineering","title":"1.2 Data Preparation &amp; Feature Engineering","text":""},{"location":"modules/01-foundations/#the-golden-rule-garbage-in-garbage-out","title":"The Golden Rule: Garbage In, Garbage Out","text":"<p>Data preparation often takes 80% of project time but determines success or failure. This isn't glamorous work, but it's where projects succeed or fail.</p> <p>\"The algorithm is not the hard part. Getting the data right is the hard part.\"</p> <p>Your job as a business analytics professional is often more about data preparation than algorithm selection. The algorithm is almost a commodity at this point\u2014scikit-learn gives you excellent implementations of everything. What matters is what you feed it.</p> <p>Since algorithms are implemented for us, focus on higher-leverage skills: problem framing (translating vague business requests into well-defined ML problems), data intuition (recognizing quality issues and predictive features), evaluation rigor (proper validation setup), communication (explaining results to stakeholders), and debugging (diagnosing whether issues are data quality, feature engineering, or methodology). Libraries implement algorithms; they don't tell you which algorithm to use or whether your features make sense.</p>"},{"location":"modules/01-foundations/#common-data-quality-issues","title":"Common Data Quality Issues","text":"<ol> <li>Missing values: NaN, null, empty strings, placeholder values (-999, \"N/A\")</li> <li>Duplicates: Exact duplicates or near-duplicates</li> <li>Outliers: Extreme values (errors vs. legitimate rare events)</li> <li>Inconsistent formatting: \"USA\" vs \"United States\" vs \"US\"</li> <li>Data entry errors: Typos, wrong units, swapped fields</li> </ol>"},{"location":"modules/01-foundations/#initial-data-exploration","title":"Initial Data Exploration","text":"<p>Before you do anything else, explore your data:</p> <pre><code>import polars as pl\n\ndf.shape              # How big is this? (rows, columns)\ndf.schema             # What are the data types?\ndf.null_count()       # Where are the missing values?\ndf.describe()         # Basic statistics\ndf.is_duplicated().sum()  # Any duplicate rows?\n</code></pre> <p>Always explore before modeling. Don't jump straight into building models without checking basic things like \"are there missing values?\"</p> <p>A reasonable heuristic is to spend 10-20% of your total project time on EDA before modeling. You've explored \"enough\" when you can answer: What are the data types and ranges? Where are missing values and what causes them? Are there obvious outliers? What is the target distribution? Which features correlate with the target? The goal is to catch major issues\u2014I've seen projects waste weeks on sophisticated modeling only to discover the target was incorrectly defined. A few hours of EDA would have saved that time.</p>"},{"location":"modules/01-foundations/#handling-missing-data","title":"Handling Missing Data","text":"<p>Strategy 1: Deletion - Drop rows with any missing values - Simple, but you lose information and might introduce bias - When to use: Missing completely at random, small percentage missing</p> <p>Strategy 2: Imputation - Mean/Median/Mode: Simple, but ignores relationships between variables - Forward/Backward fill: For time series\u2014use the previous or next value - Model-based (k-NN): Predict the missing value from other features</p> <pre><code>from sklearn.impute import SimpleImputer, KNNImputer\n\n# Simple imputation\nimputer = SimpleImputer(strategy='median')\nX_imputed = imputer.fit_transform(X)\n\n# K-NN imputation (considers relationships)\nknn_imputer = KNNImputer(n_neighbors=5)\nX_imputed = knn_imputer.fit_transform(X)\n</code></pre>"},{"location":"modules/01-foundations/#why-data-is-missing-matters","title":"Why Data is Missing Matters","text":"<p>The reason data is missing determines what you should do about it:</p> Type Description Implication MCAR (Missing Completely at Random) Missingness has nothing to do with any values Safe to delete MAR (Missing at Random) Missingness is related to other observed variables Imputation can work MNAR (Missing Not at Random) Missingness is related to the missing value itself Problematic\u2014missingness is informative <p>Concrete example using the same dataset: Imagine an employee survey asking about salary. Here's how each mechanism might cause missing salary data:</p> <ul> <li> <p>MCAR: The survey software randomly crashed for 5% of respondents before they reached the salary question. Missingness has nothing to do with salary, department, or any other variable\u2014purely random technical failure. You can safely delete these rows or impute without bias.</p> </li> <li> <p>MAR: The survey was optional, and employees in the Sales department (who tend to have higher salaries due to commissions) were more likely to skip the survey entirely because they were busy. Salary is missing, but the missingness is explained by department (which you observe). If you control for department, the missing salaries aren't systematically different from observed ones. Imputation works because you can use department to guide it.</p> </li> <li> <p>MNAR: Employees with very high salaries (executives) and very low salaries (embarrassed about compensation) both skip the salary question. The missingness is directly related to the missing value itself\u2014you can't predict who's missing based on other variables because the salary value itself determines the skip. Imputation will underestimate variance, pulling toward the middle. The missingness is informative\u2014itself a signal.</p> </li> </ul> <p>The key diagnostic question: \"If I knew the missing value, would that help me predict why it's missing?\" If yes, you likely have MNAR.</p> <p>Diagnosing missingness type requires evidence and domain knowledge. For MCAR, compare distributions of other variables between rows with and without missing values\u2014they should be similar if missingness is random. For MAR vs MNAR, build a model predicting whether a value is missing using other features; if it has predictive power, missingness is at least partially MAR. Domain knowledge is essential: ask why data might be missing and whether that reason relates to the missing value itself.</p>"},{"location":"modules/01-foundations/#feature-scaling","title":"Feature Scaling","text":"<p>Many algorithms are sensitive to the scale of features. If one feature ranges from 0-1 and another from 0-1,000,000, the larger feature will dominate.</p> <p>The distance problem: Imagine finding the nearest neighbor for a customer using age (20-70 years) and income (\\(30,000-\\)200,000). Two customers might differ by 5 years in age and $1,000 in income. Without scaling, the distance calculation treats these as: - Age difference: 5 units - Income difference: 1,000 units</p> <p>The income difference completely dominates\u2014the algorithm essentially ignores age. A 50-year age difference (5 units) matters less than a $50 income difference (50 units). But intuitively, age and income should both influence \"similarity.\"</p> <p>After standardization (converting to z-scores), both features are measured in \"standard deviations from the mean.\" A 1-standard-deviation difference in age has the same weight as a 1-standard-deviation difference in income. Now the algorithm considers both features fairly.</p> <p>Numerical Example: Feature Scaling Impact on k-NN</p> <pre><code>import numpy as np\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n# Create synthetic data: Age (20-70) and Income ($30k-$200k)\nnp.random.seed(42)\nn_samples = 200\nage = np.random.uniform(low=20, high=70, size=n_samples)\nincome = np.random.uniform(low=30000, high=200000, size=n_samples)\n\n# Target: high income AND middle age (35-55) -&gt; class 1\ntarget = ((income &gt; 80000) &amp; (age &gt; 35) &amp; (age &lt; 55)).astype(int)\nX = np.column_stack([age, income])\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y=target, test_size=0.3, random_state=42\n)\n\n# Without scaling\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\nprint(f\"Without scaling: {knn.score(X_test, y_test):.1%}\")\n\n# With scaling\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nknn_scaled = KNeighborsClassifier(n_neighbors=5)\nknn_scaled.fit(X_train_scaled, y_train)\nprint(f\"With scaling: {knn_scaled.score(X_test_scaled, y_test):.1%}\")\n</code></pre> <p>Output: <pre><code>Without scaling: 61.7%\nWith scaling: 91.7%\n</code></pre></p> <p>Interpretation: A 30 percentage point improvement from scaling alone. Without scaling, income differences (range: $170,000) completely dominate age differences (range: 50 years). The k-NN algorithm essentially ignores age when finding nearest neighbors, missing the actual pattern in the data.</p> <p>Source: <code>slide_computations/module1_examples.py</code> - <code>demo_feature_scaling_impact()</code></p> <p>Algorithms that need scaling: - Linear regression, SVM, k-NN, neural networks - Regularized methods (regularization only penalizes fairly when features are scaled)</p> <p>Standardization (Z-score):</p> \\[x_{scaled} = \\frac{x - \\mu}{\\sigma}\\] <ul> <li>Centers at 0, standard deviation of 1</li> <li>Preserves outliers</li> <li>Use when: Algorithm assumes normally distributed data</li> </ul> <p>Min-Max Normalization:</p> \\[x_{scaled} = \\frac{x - x_{min}}{x_{max} - x_{min}}\\] <ul> <li>Scales to [0, 1] range</li> <li>Sensitive to outliers</li> <li>Use when: Need bounded range (neural networks)</li> </ul> <pre><code>from sklearn.preprocessing import StandardScaler, MinMaxScaler\n\n# Standardization\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)  # Use training parameters!\n\n# Min-Max\nminmax = MinMaxScaler()\nX_train_scaled = minmax.fit_transform(X_train)\n</code></pre> <p>When NOT to scale: Tree-based methods (Decision Trees, Random Forest, XGBoost) are scale-invariant! They make splits based on thresholds, not distances. Don't waste time scaling for tree-based models.</p> <p>If you're unsure which algorithm you'll use, wait until you've chosen one, or incorporate scaling into your pipeline so it's applied conditionally. Different algorithms prefer different scaling (neural networks work better with MinMax [0,1], while SVMs use standardization). The practical solution is sklearn Pipelines\u2014create a pipeline where scaling is a step before the model, which also prevents data leakage during cross-validation.</p>"},{"location":"modules/01-foundations/#outlier-detection","title":"Outlier Detection","text":"<p>Methods for different data distributions:</p> <p>For normally distributed data: - Z-score method: Points more than 3 standard deviations from the mean are outliers</p> <p>For non-normal or unknown distributions: - IQR method: Outliers are below Q1 - 1.5\u00d7IQR or above Q3 + 1.5\u00d7IQR (what box plots use) - Modified Z-score: Uses median and MAD instead of mean and std\u2014very robust - Isolation Forest: Tree-based method that works well in high dimensions</p> <pre><code>from sklearn.ensemble import IsolationForest\nclf = IsolationForest(contamination=0.05)  # Expect ~5% outliers\noutliers = clf.fit_predict(X)  # Returns -1 for outliers\n</code></pre> <p>What to do with outliers: Investigate first! Are they data errors? Legitimate extreme values? Different populations? Options include: remove, cap/winsorize, transform (log), or use robust methods.</p> <p>Critical distinction: Outliers in features vs. outliers as targets are completely different problems. If you're detecting fraud, fraudulent transactions ARE what you're trying to predict\u2014they're your positive class, not outliers to remove. Never remove outliers blindly based on statistics alone. Investigate: are they errors, rare-but-legitimate events, or the signal you're looking for? For anomaly detection tasks, use algorithms designed to find outliers (Isolation Forest, One-Class SVM), don't remove them.</p>"},{"location":"modules/01-foundations/#encoding-categorical-variables","title":"Encoding Categorical Variables","text":"<p>ML algorithms need numbers, not strings. When you have categorical variables, you need to convert them.</p> <p>One-Hot Encoding: - Creates binary column for each category - Use for: Nominal categories (no order), small number of categories - Watch out: High cardinality (many categories) creates many columns</p> <p>Label Encoding: - Assigns integer to each category - Use for: Ordinal categories (low/medium/high) - Watch out: Implies ordering where none exists</p> <p>Target Encoding: - Replace category with mean of target variable for that category - Use for: High cardinality categories (ZIP codes, product SKUs) - Watch out: Data leakage! Must use cross-validation</p> <p>Target encoding causes leakage because when you calculate the mean target value for a category, you're using information from rows you'll later predict\u2014each row's outcome influences its own encoded feature value. The severity scales with category size (worse for rare categories). The solution is cross-validation-style encoding: for each row, calculate the category mean using only OTHER rows. Libraries like <code>category_encoders</code> implement this properly.</p> <pre><code>from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n\n# One-hot encoding\nencoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\nX_encoded = encoder.fit_transform(X[['category_column']])\n\n# With polars (simpler for exploration)\nX_encoded = df.to_dummies(columns=['category_column'])\n</code></pre>"},{"location":"modules/01-foundations/#the-cardinal-rule-preventing-data-leakage","title":"The Cardinal Rule: Preventing Data Leakage","text":"<p>Never let information from the test set influence training!</p> <p>This is called data leakage, and it will give you overly optimistic results that don't hold up in production.</p> <p>Why leakage is so dangerous: Imagine you scale your entire dataset before splitting. The scaler computes mean=50 and std=10 from all 1000 rows\u2014including the 200 test rows. Now when you standardize the test data, each test value is positioned relative to statistics that include itself. The model indirectly \"knows\" something about test examples because they influenced preprocessing. In production, new data won't have this privilege\u2014it gets scaled using only training statistics\u2014so your test performance is artificially inflated.</p> <p>The information flow problem: <pre><code>WRONG: Data \u2192 Scale ALL \u2192 Split \u2192 Train \u2192 Evaluate\n       \u2191_______\u2193\n       Test data statistics leak into training\n\nRIGHT: Data \u2192 Split \u2192 Scale TRAIN \u2192 Train\n                    \u2192 Scale TEST (using train params) \u2192 Evaluate\n       No leakage: test data never influences anything before evaluation\n</code></pre></p> <p>Even small leaks compound. If your pipeline has scaling, then feature selection, then imputation\u2014and each step uses all data\u2014you have three sources of leakage. The resulting performance estimate can be wildly optimistic.</p> <p>Common leakage examples: 1. Scaling before splitting: Scaler sees test data statistics 2. Feature selection on all data: Test data influences feature choice 3. Target encoding without proper CV: Test data target values leak 4. Time series: future predicts past: Future information used for past predictions</p> <p>Numerical Example: Data Leakage Effect</p> <pre><code>import numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\n# Small dataset where leakage effect is visible\nwrong_accuracies, right_accuracies = [], []\n\nfor trial in range(20):\n    X, y = make_classification(\n        n_samples=50, n_features=15, n_informative=5,\n        n_redundant=3, random_state=trial\n    )\n\n    # WRONG: Scale ALL data, then split\n    scaler_wrong = StandardScaler()\n    X_scaled = scaler_wrong.fit_transform(X)  # Leakage!\n    X_tr, X_te, y_tr, y_te = train_test_split(\n        X_scaled, y, test_size=0.3, random_state=trial\n    )\n    model = LogisticRegression(random_state=42, max_iter=1000)\n    model.fit(X_tr, y_tr)\n    wrong_accuracies.append(model.score(X_te, y_te))\n\n    # RIGHT: Split first, then scale\n    X_tr, X_te, y_tr, y_te = train_test_split(\n        X, y, test_size=0.3, random_state=trial\n    )\n    scaler_right = StandardScaler()\n    X_tr_scaled = scaler_right.fit_transform(X_tr)\n    X_te_scaled = scaler_right.transform(X_te)\n    model = LogisticRegression(random_state=42, max_iter=1000)\n    model.fit(X_tr_scaled, y_tr)\n    right_accuracies.append(model.score(X_te_scaled, y_te))\n\nprint(f\"WRONG (scale all, then split): {np.mean(wrong_accuracies):.1%}\")\nprint(f\"RIGHT (split, then scale): {np.mean(right_accuracies):.1%}\")\n</code></pre> <p>Output: <pre><code>WRONG (scale all, then split): 75.7%\nRIGHT (split, then scale): 74.7%\n</code></pre></p> <p>Interpretation: On small datasets (n=50), scaling before splitting inflates accuracy by ~1%. This gap widens with more preprocessing steps, smaller data, or time series. The wrong approach reports better results than you'll see in production\u2014a subtle but dangerous form of overfitting.</p> <p>Source: <code>slide_computations/module1_examples.py</code> - <code>demo_data_leakage()</code></p> <p>The correct workflow:</p> <pre><code>1. Split data FIRST\n   \u2514\u2500\u2192 Training set | Test set\n\n2. Fit preprocessing on TRAINING only\n   \u2514\u2500\u2192 scaler.fit_transform(X_train)\n\n3. Transform test using training parameters\n   \u2514\u2500\u2192 scaler.transform(X_test)\n</code></pre> <p>Handling extrapolation (test data with values outside training range): For standardization, values get z-scores beyond the training range\u2014most models handle this gracefully. For MinMax scaling, values might exceed [0,1], consider clipping. For one-hot encoding, new categories are problematic\u2014set <code>handle_unknown='ignore'</code> to assign zeros. Check whether training data covers the expected production range and monitor for out-of-distribution inputs.</p> <pre><code>from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Step 1: Split FIRST\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Step 2: Fit on training ONLY\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\n\n# Step 3: Transform test using training parameters\nX_test_scaled = scaler.transform(X_test)  # NOT fit_transform!\n</code></pre> <p>Understanding fit, transform, and fit_transform:</p> <p>The packing strategy analogy: Think of a scaler like packing a suitcase. <code>fit()</code> is figuring out your packing strategy\u2014measuring your clothes, deciding how to fold them. <code>transform()</code> is actually packing using that strategy. <code>fit_transform()</code> does both at once.</p> <p>For training data, you figure out the strategy and pack (<code>fit_transform</code>). For test data, you use the same strategy you already figured out\u2014you don't re-measure (<code>transform</code> only). If you called <code>fit_transform</code> on test data, you'd be creating a new packing strategy based on test clothes, which defeats the purpose of consistent preprocessing.</p> Method What it does When to use <code>fit()</code> Learns parameters from data (e.g., mean, std) When you only need to learn, not transform <code>transform()</code> Applies learned parameters to transform data On test data (using parameters learned from train) <code>fit_transform()</code> Does both in one step On training data (learn + transform together)"},{"location":"modules/01-foundations/#common-misconceptions_1","title":"Common Misconceptions","text":"Misconception Reality \"More features always improve models\" Too many features can cause overfitting. Feature selection is often necessary. \"Just drop all rows with missing values\" This can introduce bias and waste data. Imputation is often better. \"Always standardize your features\" Tree-based models don't need scaling. Know your algorithm! \"One-hot encoding is always best\" High-cardinality features may need target encoding or embeddings. <p>Embeddings (covered in detail in Module 6) are learned dense vector representations for high-cardinality categories. Instead of one-hot encoding (millions of columns for product IDs) or target encoding (loses information), each category gets a small vector of continuous values learned during training. The model figures out which categories are \"similar\" based on their relationship to the target.</p>"},{"location":"modules/01-foundations/#13-model-evaluation-validation","title":"1.3 Model Evaluation &amp; Validation","text":""},{"location":"modules/01-foundations/#why-evaluation-matters","title":"Why Evaluation Matters","text":"<p>How do we know if a model is actually good? This seems straightforward, but it's actually subtle: - Good on what metric? Different metrics capture different aspects of performance. - Good compared to what baseline? 85% accuracy might be great or terrible depending on context. - Will it work on new, unseen data? Performance on training data is meaningless if it doesn't generalize.</p> <p>Always compare to a meaningful baseline: for regression, predict the mean (any positive R\u00b2 beats this); for classification, predict the majority class (90% accuracy from always predicting the majority in a 90/10 dataset). In time series, \"predict yesterday's value\" is a common baseline. If your model doesn't substantially beat these simple baselines, either the problem is harder than expected or something is wrong with your setup.</p>"},{"location":"modules/01-foundations/#regression-metrics","title":"Regression Metrics","text":"<p>Mean Squared Error (MSE):</p> \\[MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2\\] <ul> <li>Penalizes large errors heavily (squared)</li> <li>Units are squared (hard to interpret)</li> </ul> <p>Root Mean Squared Error (RMSE):</p> \\[RMSE = \\sqrt{MSE}\\] <ul> <li>Same units as target variable (interpretable)</li> <li>Most common regression metric</li> <li>\"On average, our predictions are off by $X\"</li> </ul> <p>Mean Absolute Error (MAE):</p> \\[MAE = \\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y}_i|\\] <ul> <li>Less sensitive to outliers than RMSE</li> <li>Linear penalty</li> </ul> <p>R\u00b2 (Coefficient of Determination):</p> \\[R^2 = 1 - \\frac{\\sum(y_i - \\hat{y}_i)^2}{\\sum(y_i - \\bar{y})^2}\\] <ul> <li>Proportion of variance explained</li> <li>1 = perfect, 0 = no better than mean prediction</li> <li>Can be negative if model is worse than mean!</li> </ul> <p>Mean Absolute Percentage Error (MAPE):</p> \\[MAPE = \\frac{100\\%}{n}\\sum_{i=1}^{n}\\left|\\frac{y_i - \\hat{y}_i}{y_i}\\right|\\] <ul> <li>Scale-independent (percentage)</li> <li>Easy for stakeholders: \"predictions are off by 5% on average\"</li> <li>Undefined when y = 0</li> </ul> <p>Which to use?</p> Metric Use When RMSE Default choice, care about large errors MAE Outliers in target, want robustness R\u00b2 Comparing models, explaining to stakeholders MAPE Need percentage interpretation, values not near zero <p>For stakeholders, translate metrics to business language: RMSE becomes \"predictions are off by $15,000 on average\"; MAPE becomes \"predictions are typically within 5%\"; R\u00b2 becomes \"our model captures 75% of the predictive signal.\" Best practice: connect to business outcomes\u2014\"we'll avoid $200K in overstock costs annually.\" Stakeholders care about business impact, not statistical properties.</p>"},{"location":"modules/01-foundations/#classification-metrics","title":"Classification Metrics","text":"<p>Everything starts with the confusion matrix:</p> <p></p> <p>Reading the diagram: This matrix shows results from a fraud detection model evaluated on 200 transactions. The rows represent actual outcomes (was it really fraud?), and the columns represent what the model predicted. Reading each cell:</p> Cell Count Meaning (Fraud Example) TP = 85 Top-left (dark) Model said \"fraud\" and it was fraud. Caught it! FN = 15 Top-right (dark) Model said \"not fraud\" but it was fraud. Missed it\u2014costly. FP = 10 Bottom-left (white) Model said \"fraud\" but it wasn't. False alarm\u2014investigation wasted. TN = 90 Bottom-right (dark) Model said \"not fraud\" and it wasn't. Correctly ignored. <p>From these numbers, we can calculate key metrics: - Total actual fraud cases: TP + FN = 85 + 15 = 100 (the top row) - Total actual legitimate: FP + TN = 10 + 90 = 100 (the bottom row) - Precision: TP / (TP + FP) = 85 / 95 = 89.5% \u2014 \"When we flag fraud, we're right 89.5% of the time\" - Recall: TP / (TP + FN) = 85 / 100 = 85% \u2014 \"We catch 85% of actual fraud\" - Accuracy: (TP + TN) / Total = 175 / 200 = 87.5% \u2014 \"Overall, 87.5% of predictions are correct\"</p> <p>Notice the tradeoff: this model has high precision (few false alarms) but misses 15% of fraud cases. Whether that's acceptable depends on business costs\u2014how much does missed fraud cost versus investigation costs?</p> <p>Numerical Example: Computing Metrics from Confusion Matrix Values</p> <pre><code># Given confusion matrix values\nTP, FN = 85, 15  # Top row: actual positives\nFP, TN = 10, 90  # Bottom row: actual negatives\n\n# Calculate metrics\nprecision = TP / (TP + FP)\nrecall = TP / (TP + FN)\naccuracy = (TP + TN) / (TP + FN + FP + TN)\nf1 = 2 * (precision * recall) / (precision + recall)\n\nprint(f\"Precision: {precision:.1%}\")\nprint(f\"Recall:    {recall:.1%}\")\nprint(f\"Accuracy:  {accuracy:.1%}\")\nprint(f\"F1 Score:  {f1:.1%}\")\n</code></pre> <p>Output: <pre><code>Precision: 89.5%\nRecall:    85.0%\nAccuracy:  87.5%\nF1 Score:  87.2%\n</code></pre></p> <p>Interpretation: These calculations match our manual work above. In practice, use <code>sklearn.metrics.classification_report(y_test, y_pred)</code> to compute all metrics at once.</p> <p>Source: <code>slide_computations/module1_examples.py</code> - <code>demo_confusion_matrix_metrics()</code></p> <p>The four cells have standard names:</p> <ul> <li>TP (True Positive): Predicted positive, actually positive. Correct.</li> <li>TN (True Negative): Predicted negative, actually negative. Correct.</li> <li>FP (False Positive): Predicted positive, actually negative. False alarm. Type I error.</li> <li>FN (False Negative): Predicted negative, actually positive. Missed it. Type II error.</li> </ul> <p>Accuracy:</p> \\[Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}\\] <ul> <li>Proportion of correct predictions</li> <li>Misleading with imbalanced classes! A model predicting \"not fraud\" for everything gets 99% accuracy if 99% of transactions are legitimate\u2014but it catches zero fraud.</li> </ul> <p>Precision:</p> \\[Precision = \\frac{TP}{TP + FP}\\] <ul> <li>\"Of those we predicted positive, how many were actually positive?\"</li> <li>High precision = few false alarms</li> </ul> <p>Recall (Sensitivity):</p> \\[Recall = \\frac{TP}{TP + FN}\\] <ul> <li>\"Of actual positives, how many did we catch?\"</li> <li>High recall = few missed positives</li> </ul> <p>The fishing net analogy: Imagine you're fishing for a specific species. Precision is about your net's selectivity\u2014of the fish you catch, what fraction are the species you want? A very fine mesh might catch everything (low precision, lots of unwanted fish), while a specialized trap catches only the target species (high precision). Recall is about your net's coverage\u2014of all target fish in the lake, what fraction do you catch? A small net in one spot has low recall (misses most fish), while a massive net across the whole lake has high recall.</p> <p>The tradeoff: a tight, selective net (high precision) might miss target fish that don't fit perfectly (lower recall). A huge, loose net (high recall) catches everything but includes lots of bycatch (lower precision). You can't maximize both simultaneously\u2014improving one typically hurts the other. Your business context determines which matters more.</p> <p>F1 Score:</p> \\[F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}\\] <ul> <li>Harmonic mean of precision and recall</li> <li>Use when you need to balance both</li> </ul> <p>The harmonic mean penalizes extreme imbalances more than the arithmetic mean. With precision=0.99 and recall=0.01, the arithmetic mean is 0.50 (suggesting \"medium\" performance), but F1 is just 0.02\u2014correctly reflecting the model is nearly useless. A model can't compensate for terrible recall with great precision; F1 remains low. If you care more about one metric (e.g., recall for medical diagnosis), optimize that directly.</p> <p>AUC-ROC: - Area Under the ROC Curve - Measures ranking ability across all thresholds - 0.5 = random, 1.0 = perfect</p> <p>The ROC curve plots True Positive Rate vs. False Positive Rate at different classification thresholds. The diagonal line represents random guessing; a perfect model hugs the top-left corner.</p> <p>The probability interpretation: AUC has a clean probabilistic meaning\u2014it's the probability that the model ranks a random positive example higher than a random negative example. If AUC=0.8, imagine pulling one fraud case and one legitimate transaction from your dataset. 80% of the time, the model assigns a higher fraud probability to the actual fraud case.</p> <p>This interpretation explains the benchmarks: - AUC = 0.5: The model ranks randomly. A coin flip would do equally well at separating positives from negatives. - AUC = 0.7: Decent discrimination. The model is learning something useful. - AUC = 0.8+: Good discrimination. For most business problems, this is solid. - AUC = 0.9+: Excellent. Either the problem is easy or you have very predictive features. - AUC = 1.0: Perfect separation. Every positive ranks above every negative. Often indicates data leakage\u2014check your pipeline.</p> <p>AUC is threshold-independent, making it useful for comparing models before you've decided on a classification threshold. Once you choose a threshold, precision/recall become more directly interpretable for that operating point.</p> <pre><code>from sklearn.metrics import (\n    accuracy_score, precision_score, recall_score,\n    f1_score, roc_auc_score, confusion_matrix,\n    classification_report\n)\n\n# All-in-one report\nprint(classification_report(y_test, y_pred))\n</code></pre>"},{"location":"modules/01-foundations/#cross-validation","title":"Cross-Validation","text":"<p>A single train/test split can be lucky or unlucky. Cross-validation gives us: - More reliable performance estimate - Confidence interval (mean \u00b1 standard deviation) - Uses all data for both training and validation</p> <p>K-Fold Cross-Validation:</p> <p></p> <p>Reading the diagram: Each row represents one \"fold\" or iteration. The orange block is the test set; blue blocks are training data. Notice how the orange block moves across columns\u2014in Fold 1, the first 20% of data is held out for testing; in Fold 2, the second 20% is held out, and so on.</p> <p>Why does rotating matter? A single train/test split might be lucky (easy test examples) or unlucky (hard ones). By rotating through 5 different test sets, you evaluate your model on every data point exactly once. If your model scores 85%, 82%, 88%, 84%, 86% across the five folds, you report 85% \u00b1 2.2%\u2014both the average performance and how much it varies. High variance across folds suggests your model is sensitive to which data it sees, which is a warning sign.</p> <p>The procedure:</p> <ol> <li>Split data into K folds (K=5 shown)</li> <li>Train on K-1 folds, validate on remaining fold</li> <li>Repeat K times, each fold as validation once</li> <li>Average the results</li> </ol> <p>Numerical Example: Why Cross-Validation Beats Single Splits</p> <pre><code>import numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\n\nX, y = make_classification(\n    n_samples=200, n_features=10, n_informative=5, random_state=42\n)\n\n# Run 50 different random single train/test splits\nsingle_split_scores = []\nfor seed in range(50):\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=seed\n    )\n    model = LogisticRegression(random_state=42, max_iter=1000)\n    model.fit(X_train, y_train)\n    single_split_scores.append(model.score(X_test, y_test))\n\n# Compare to 5-fold CV\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nmodel = LogisticRegression(random_state=42, max_iter=1000)\ncv_scores = cross_val_score(model, X, y, cv=cv)\n\nprint(f\"50 random single splits:\")\nprint(f\"  Range: {min(single_split_scores):.1%} to {max(single_split_scores):.1%}\")\nprint(f\"  Mean: {np.mean(single_split_scores):.1%}\")\nprint(f\"\\n5-fold CV: {cv_scores.mean():.1%} \u00b1 {cv_scores.std():.1%}\")\n</code></pre> <p>Output: <pre><code>50 random single splits:\n  Range: 57.5% to 87.5%\n  Mean: 73.1%\n\n5-fold CV: 75.0% \u00b1 6.3%\n</code></pre></p> <p>Interpretation: A single random split could report anywhere from 57.5% to 87.5%\u2014a 30 percentage point range depending on luck. Cross-validation reports 75.0% \u00b1 6.3%, giving both an estimate and a confidence interval. The CV mean is close to the true average across all possible splits.</p> <p>Source: <code>slide_computations/module1_examples.py</code> - <code>demo_cv_variance_reduction()</code></p> <p>Stratified K-Fold: Essential for imbalanced data\u2014maintains class distribution in each fold. Without stratification, regular K-Fold can create folds with very different class distributions, causing unreliable estimates (high variance CV scores) and training on unrealistic distributions. Use <code>StratifiedKFold</code> by default for classification\u2014it never hurts.</p> <p>Time Series Split: Respects temporal ordering\u2014train on past, validate on future. Shuffling time series creates temporal leakage\u2014using future information to predict the past, which is impossible in production. Models can report 90% accuracy with shuffled validation and 55% with proper temporal validation. Use <code>TimeSeriesSplit</code> to mimic production conditions.</p> <pre><code>from sklearn.model_selection import (\n    cross_val_score, KFold, StratifiedKFold, TimeSeriesSplit\n)\n\n# Basic K-Fold\nscores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\nprint(f\"Mean: {scores.mean():.3f} (+/- {scores.std():.3f})\")\n\n# Stratified for classification\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nscores = cross_val_score(model, X, y, cv=cv)\n\n# Time series\ntscv = TimeSeriesSplit(n_splits=5)\nscores = cross_val_score(model, X, y, cv=tscv)\n</code></pre>"},{"location":"modules/01-foundations/#overfitting-vs-underfitting","title":"Overfitting vs Underfitting","text":"<p>Reading the diagram: All three panels show the same data points (blue dots) with an obvious curved pattern. The difference is how each model attempts to fit that pattern:</p> <ul> <li> <p>Left panel (red line): The underfitting model uses a straight line for curved data. It systematically misses the pattern\u2014points at the peaks are far below the line, points at the valleys are far above it. The model is too simple to capture what's actually happening. This is high bias: the model has a built-in assumption (linearity) that doesn't match reality.</p> </li> <li> <p>Middle panel (green curve): The good fit captures the overall curved trend without chasing every individual point. Some points are above the curve, some below\u2014that's fine, because those deviations are likely noise. This model will generalize well to new data.</p> </li> <li> <p>Right panel (orange jagged line): The overfitting model passes through (or very close to) every single training point. It treats noise as signal, contorting itself to explain random variation. On new data, those contortions will hurt\u2014the model learned the training set's quirks, not the underlying pattern. This is high variance: the model would look completely different if trained on a different sample.</p> </li> </ul> <p>Underfitting (High Bias): - Training error HIGH, Test error HIGH - Model too simple to capture patterns - Solutions: More features, more complex model, less regularization</p> <p>Overfitting (High Variance): - Training error LOW, Test error HIGH - Model memorizes training data including noise - Solutions: More data, simpler model, regularization, early stopping</p> <p>Diagnostic pattern: Look at the gap between training and test error. Both high = underfitting. Training low but test high = overfitting.</p> <p>If both training and test error are low, that's the goal\u2014but it doesn't guarantee perfection. Still check for: data leakage (too good to be true?), non-representative test sets, wrong metrics (high accuracy but terrible minority class performance), or overfitting to the test set from repeated model selection. Monitor performance after deployment for concept drift.</p>"},{"location":"modules/01-foundations/#the-bias-variance-tradeoff","title":"The Bias-Variance Tradeoff","text":"<p>Total Error = Bias\u00b2 + Variance + Irreducible Noise</p> <p>Bias is systematic error\u2014the model's tendency to miss patterns. High bias means underfitting. A linear model trying to fit a curved relationship has high bias.</p> <p>Variance is sensitivity to training data\u2014how much the model changes with different training samples. High variance means overfitting. A very deep decision tree has high variance.</p> <p>The tradeoff: Reducing bias usually increases variance (more complex model). Reducing variance usually increases bias (simpler model). The goal is to find the sweet spot.</p> <p>There are ways to reduce both simultaneously: more data lets you fit complex models without overfitting; ensemble methods like Random Forests reduce variance while boosting reduces bias; better features make patterns easier to learn. Irreducible noise sets a floor on total error, and at any given data size there is still a tradeoff\u2014these techniques shift the curve inward but don't eliminate it.</p> <p>The Dart-Throwing Analogy:</p> Scenario Bias Variance Pattern High bias, low variance Off-center Clustered together Consistent but wrong Low bias, high variance Centered on average Scattered everywhere Right on average but inconsistent Ideal Centered Clustered Accurate and precise"},{"location":"modules/01-foundations/#business-specific-evaluation","title":"Business-Specific Evaluation","text":"<p>Not all errors cost the same!</p> Domain False Positive Cost False Negative Cost Spam filter Important email missed Spam in inbox Medical diagnosis Unnecessary treatment Missed disease Fraud detection Investigation cost Fraud loss Manufacturing QC Discarding good product Shipping defect <p>For spam, a false positive (real email marked as spam) is much worse than a false negative. Optimize for precision.</p> <p>For medical diagnosis, a false negative (missed disease) is much worse. Optimize for recall.</p> <p>Choose metrics that reflect business costs. Accuracy is misleading because it treats all errors as equal when they rarely are.</p> <p>To quantify costs, work backwards from business outcomes: (1) identify the action taken for each prediction (predicted churn \u2192 retention offer), (2) quantify costs for each outcome (false positive blocks a $200 customer, false negative lets $500 fraud through), (3) build a cost matrix multiplying confusion matrix cells by costs, (4) optimize threshold for minimum total cost. Start with rough estimates, get stakeholder buy-in, and refine over time\u2014approximate cost quantification beats implicitly assuming all errors cost the same.</p>"},{"location":"modules/01-foundations/#common-misconceptions_2","title":"Common Misconceptions","text":"Misconception Reality \"Higher R\u00b2 always means better model\" Can overfit to get high R\u00b2. Test set R\u00b2 is what matters. R\u00b2 can be negative! \"Accuracy is the best metric for classification\" Misleading for imbalanced classes. Use precision/recall/F1/AUC instead. \"More complex models are always better\" Complexity increases variance. Simpler models often generalize better. \"Cross-validation eliminates the need for a test set\" CV estimates performance but you should still have a final holdout test set."},{"location":"modules/01-foundations/#14-python-ecosystem-setup","title":"1.4 Python Ecosystem Setup","text":""},{"location":"modules/01-foundations/#jupyter-notebooks-and-google-colab","title":"Jupyter Notebooks and Google Colab","text":"<p>Jupyter Notebooks: - Interactive computing environment - Mix code, output, and documentation - Great for exploration and teaching</p> <p>Google Colab advantages: - No setup required - Free GPU access - Easy sharing - Pre-installed packages</p> <p>Best practices: - Use markdown cells for documentation - Keep cells focused (one logical step per cell) - Restart and run all before sharing - Use consistent naming conventions</p> <p>\"Restart and Run All\" prevents hidden state issues: cells run out of order, deleted cells leaving ghost variables, or imports removed but modules still loaded. If it fails, your notebook has hidden dependencies. If it succeeds, anyone can reproduce your results. Do this before every commit or share.</p>"},{"location":"modules/01-foundations/#marimo-notebooks","title":"marimo Notebooks","text":"<p>marimo is a next-generation Python notebook that solves many of Jupyter's pain points. Unlike Jupyter's manual cell execution, marimo notebooks are reactive\u2014when you change a variable, all cells that depend on it automatically re-execute. This eliminates the hidden state problems that plague Jupyter notebooks.</p> <p>Key differences from Jupyter:</p> Feature Jupyter marimo Execution Manual cell runs Reactive (auto-updates) File format JSON (.ipynb) Pure Python (.py) Reproducibility State can diverge from code Always reproducible Version control Difficult diffs Clean git diffs Cell ordering Can run out of order Execution order enforced <p>When to use each: - Jupyter/Colab: Quick exploration, collaboration with non-technical stakeholders, free GPU access (Colab) - marimo: Production notebooks, version control, reproducibility, teaching environments where you want guaranteed consistency</p> <p>In this course, we use marimo for most notebooks because the reactive execution model prevents the \"run cells out of order\" bugs that commonly confuse students. The pure Python format also means you can use standard development tools like linters and formatters.</p>"},{"location":"modules/01-foundations/#pixi-package-manager","title":"pixi Package Manager","text":"<p>pixi is a modern package manager that handles Python environments with speed and reproducibility. It uses the same package repository as conda but with faster dependency resolution and a cleaner project structure.</p> <p>Why pixi for this course? - Fast: Resolves dependencies in seconds, not minutes - Reproducible: Lock files ensure everyone has identical environments - Cross-platform: Same configuration works on Windows, Mac, and Linux - Simple: One configuration file (<code>pixi.toml</code>) defines your entire project</p> <p>Installation:</p> <pre><code># macOS/Linux\ncurl -fsSL https://pixi.sh/install.sh | bash\n\n# Windows (PowerShell)\niwr -useb https://pixi.sh/install.ps1 | iex\n</code></pre> <p>After installation, restart your terminal and verify with <code>pixi --version</code>.</p> <p>Project structure:</p> <p>A pixi project is defined by a <code>pixi.toml</code> file:</p> <pre><code>[project]\nname = \"ban501-module1\"\nversion = \"0.1.0\"\nchannels = [\"conda-forge\"]\nplatforms = [\"linux-64\", \"osx-arm64\", \"win-64\"]\n\n[dependencies]\npython = \"&gt;=3.11\"\npolars = \"&gt;=1.0\"\nscikit-learn = \"&gt;=1.5\"\nmatplotlib = \"&gt;=3.8\"\nseaborn = \"&gt;=0.13\"\nmarimo = \"&gt;=0.9\"\n</code></pre> <p>Common commands:</p> Command Description <code>pixi install</code> Install all dependencies from pixi.toml <code>pixi add polars</code> Add a new dependency <code>pixi run python script.py</code> Run Python in the project environment <code>pixi run marimo edit notebook.py</code> Open marimo notebook in project environment <code>pixi shell</code> Activate the environment in your shell <p>Typical workflow:</p> <pre><code># Clone or create a project\ncd my-project\n\n# Install dependencies (creates/updates lock file)\npixi install\n\n# Run your code\npixi run python analysis.py\n\n# Or activate the environment for interactive work\npixi shell\npython\n</code></pre>"},{"location":"modules/01-foundations/#polars-essentials","title":"polars Essentials","text":"<p>Core operations you need to know:</p> <pre><code>import polars as pl\nimport numpy as np\n\n# Reading data\ndf = pl.read_csv('data.csv')\n\n# Basic exploration\ndf.head()           # First 5 rows\ndf.schema           # Data types (column name -&gt; dtype mapping)\ndf.describe()       # Statistical summary\ndf.shape            # (rows, columns)\n\n# Selection\ndf.select('column')              # Single column\ndf.select(['col1', 'col2'])      # Multiple columns\ndf.row(0)                        # Single row by index\ndf[0:5]                          # Slice rows\n\n# Filtering\ndf.filter(pl.col('age') &gt; 30)\ndf.filter((pl.col('age') &gt; 30) &amp; (pl.col('income') &gt; 50000))\n\n# Aggregation\ndf.group_by('category').agg(pl.all().mean())\ndf.group_by('category').agg([\n    pl.col('sales').sum(),\n    pl.col('customers').count()\n])\n\n# Handling missing values\ndf.null_count()                    # Count nulls per column\ndf.drop_nulls()                    # Drop rows with any nulls\ndf.fill_null(0)                    # Fill with constant\ndf.fill_null(strategy='mean')      # Fill with mean\n</code></pre>"},{"location":"modules/01-foundations/#basic-eda-workflow","title":"Basic EDA Workflow","text":"<p>Standard exploration sequence:</p> <ol> <li>Load and inspect: Shape, dtypes, head/tail. Check for obvious issues.</li> <li>Missing values: Count per column, visualize missingness patterns.</li> <li>Univariate analysis: Distributions of each feature, identify outliers.</li> <li>Bivariate analysis: Correlations, feature vs. target relationships.</li> <li>Document findings: Key insights, data quality issues, feature engineering ideas.</li> </ol> <p>Write EDA documentation for your future self six months from now\u2014you'll have forgotten everything. Minimum viable documentation: what questions were you answering? What did you find? What decisions did you make based on findings? What concerns remain? Document conclusions and decisions, not every chart. For formal projects, a separate EDA report for stakeholders should tell a story; the working notebook is for reproducibility.</p>"},{"location":"modules/01-foundations/#visualization-with-matplotlibseaborn","title":"Visualization with matplotlib/seaborn","text":"<pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Distribution\nplt.figure(figsize=(10, 6))\nsns.histplot(df['column'], kde=True)\nplt.title('Distribution of Column')\nplt.show()\n\n# Correlation heatmap\nplt.figure(figsize=(12, 8))\nsns.heatmap(df.corr(), annot=True, cmap='coolwarm', center=0)\nplt.title('Correlation Matrix')\nplt.show()\n\n# Scatter plot with hue\nsns.scatterplot(data=df, x='feature1', y='feature2', hue='target')\n\n# Box plot for outliers\nsns.boxplot(data=df, x='category', y='value')\n\n# Pair plot (for small number of features)\nsns.pairplot(df, hue='target')\n</code></pre> <p>Visualization principles: - Always label axes - Use appropriate chart types - Consider colorblind-friendly palettes - Don't clutter\u2014one message per chart</p>"},{"location":"modules/01-foundations/#gpu-computing-and-cuda","title":"GPU Computing and CUDA","text":"<p>In Section 1.1, we mentioned that NVIDIA released CUDA in 2006 and that AlexNet's 2012 ImageNet victory was enabled by GPU training. Understanding why GPUs matter for machine learning is essential context for the deep learning modules later in this course.</p> <p>Why GPUs matter for ML:</p> <p>CPUs (Central Processing Units) are designed for sequential tasks\u2014they have a few powerful cores that excel at complex operations one at a time. GPUs (Graphics Processing Units) take the opposite approach: thousands of simpler cores that perform many calculations simultaneously.</p> <p>Neural network training involves the same operation (multiply-accumulate) applied to millions of numbers. A CPU processes these one at a time; a GPU processes thousands in parallel. This is why training that takes weeks on a CPU can finish in hours on a GPU.</p> <p>CUDA and the deep learning revolution:</p> <p>CUDA (Compute Unified Device Architecture) is NVIDIA's programming interface that lets general software\u2014not just graphics\u2014run on GPUs. Before CUDA, using GPUs for non-graphics tasks required awkward workarounds. CUDA made GPU computing accessible to researchers, and deep learning took off.</p> <p>The 2012 AlexNet breakthrough wasn't just about a better algorithm\u2014the same architecture trained on CPUs would have taken months. GPU training made rapid experimentation possible, accelerating the entire field.</p> <p>Practical considerations:</p> Task Hardware Recommendation Tabular data (small-medium datasets) CPU sufficient Traditional ML (Random Forest, XGBoost) CPU sufficient Deep learning training GPU recommended Large-scale inference GPU beneficial Image/video processing GPU recommended <p>For the early modules of this course (regression, classification, ensemble methods), you won't need GPU access\u2014these algorithms run well on CPUs. When we reach the neural network modules (6-8), GPU access becomes valuable.</p> <p>Accessing GPUs:</p> <ul> <li>Google Colab: Free tier includes limited GPU access\u2014select \"Runtime &gt; Change runtime type &gt; GPU\"</li> <li>Local GPU: Requires an NVIDIA GPU and CUDA toolkit installation</li> <li>Cloud platforms: AWS, GCP, and Azure offer GPU instances for rent</li> </ul> <p>For this course, Colab's free tier is sufficient for the deep learning exercises. If you have a local NVIDIA GPU, you can configure pixi to use CUDA-enabled packages, but this is optional.</p>"},{"location":"modules/01-foundations/#reflection-questions","title":"Reflection Questions","text":"<ol> <li> <p>A company says they're using \"AI\" for their chatbot. Is this AI? Machine Learning? Both? Neither? How would you find out?</p> </li> <li> <p>You have a dataset where 30% of income values are missing. The missingness correlates with age (older people less likely to report). What imputation strategy would you use?</p> </li> <li> <p>A colleague scales the entire dataset before splitting into train/test. Why is this a problem? How would it affect your model evaluation?</p> </li> <li> <p>Your manager asks: \"Is 85% accuracy good?\" How would you respond?</p> </li> <li> <p>A model has R\u00b2 = 0.95 on training data and R\u00b2 = 0.60 on test data. What's happening? What would you do?</p> </li> <li> <p>Which metric would you optimize for a medical diagnosis model where missing a disease (false negative) is much worse than a false alarm?</p> </li> </ol>"},{"location":"modules/01-foundations/#practice-problems","title":"Practice Problems","text":"<ol> <li> <p>Given a dataset description, identify whether each column needs scaling, encoding, or neither.</p> </li> <li> <p>Calculate precision, recall, and F1 from a confusion matrix.</p> </li> <li> <p>Interpret R\u00b2 = 0.75 in business terms.</p> </li> <li> <p>Identify potential data leakage in a described ML pipeline.</p> </li> <li> <p>For each business problem, recommend an appropriate evaluation metric and justify:</p> </li> <li>Predicting next month's revenue</li> <li>Identifying customers likely to cancel</li> <li>Grouping products that are often bought together</li> </ol>"},{"location":"modules/01-foundations/#chapter-summary","title":"Chapter Summary","text":"<p>Six key takeaways from Module 1:</p> <ol> <li> <p>ML learns patterns from data\u2014it's a subset of AI, and it overlaps with Data Science. Don't conflate these terms.</p> </li> <li> <p>Match problem to task type\u2014regression for continuous values, classification for categories, clustering for grouping.</p> </li> <li> <p>Data prep is critical\u2014handle missing values, scale features (when appropriate), encode categoricals. This is 80% of the work.</p> </li> <li> <p>Prevent data leakage\u2014split first, fit on train, transform test. This is the cardinal rule.</p> </li> <li> <p>Evaluate appropriately\u2014choose metrics that reflect business costs. Accuracy is often the wrong choice.</p> </li> <li> <p>Diagnose model issues\u2014use the bias-variance framework. High test error + low train error = overfitting.</p> </li> </ol>"},{"location":"modules/01-foundations/#whats-next","title":"What's Next","text":"<p>In Module 2, we'll dive into Regression Methods: - Linear regression fundamentals - Regularization (Lasso, Ridge) - Polynomial features - Model interpretation</p> <p>You'll apply everything from Module 1: - Data preparation pipelines - Train-test splits - RMSE, MAE, R\u00b2 evaluation - Cross-validation</p> <p>The concepts we've established in this module are the foundation. Module 2 starts building on that foundation.</p>"},{"location":"modules/02-regression/","title":"Module 2: Classical Machine Learning - Regression","text":""},{"location":"modules/02-regression/#introduction","title":"Introduction","text":"<p>Module 1 established the foundation: what ML is, how to prepare data, and how to evaluate models. Now we put that foundation to work.</p> <p>Regression is the workhorse of predictive analytics. When a business wants to predict sales, estimate prices, or forecast demand, regression is often the first tool they reach for. But we're not just going to use regression as a black box\u2014we're going to understand how it works, including implementing gradient descent from scratch.</p> <p>Why learn gradient descent? Because gradient descent is the foundation for training neural networks. Every deep learning model you've heard of\u2014GPT, image classifiers, everything\u2014learns through gradient descent. Understanding it for linear regression means understanding it for neural networks.</p> <p>The closed-form solution for linear regression requires matrix inversion\u2014O(n\u00b3) complexity that's impossible for neural networks with millions of parameters. Neural networks also have non-convex loss surfaces with many local minima; there's no mathematical formula to jump to the optimal weights. Gradient descent works for any differentiable function and scales to billions of parameters\u2014the linear regression closed-form is a special case where we can skip the search.</p>"},{"location":"modules/02-regression/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, you should be able to:</p> <ol> <li>Explain the mechanics of linear regression including the least squares method</li> <li>Implement gradient descent from scratch and understand its trade-offs</li> <li>Interpret regression coefficients for business insights</li> <li>Diagnose model issues through residual analysis</li> <li>Apply regularization techniques (L1, L2, Elastic Net) to prevent overfitting</li> <li>Communicate regression findings to non-technical stakeholders</li> </ol>"},{"location":"modules/02-regression/#21-simple-linear-regression","title":"2.1 Simple Linear Regression","text":""},{"location":"modules/02-regression/#the-three-components-of-every-ml-model","title":"The Three Components of Every ML Model","text":"<p>Before diving into linear regression, here's a framework that applies to every supervised learning algorithm:</p> Component Question It Answers Linear Regression Decision Model How do we transform inputs into predictions? \\(\\hat{y} = \\beta_0 + \\beta_1 x\\) Quality Measure How do we evaluate prediction quality? Sum of Squared Errors (SSE) Update Method How do we improve the model? Gradient descent (or closed-form) <p>This same pattern applies to every algorithm: logistic regression, decision trees, random forests, neural networks. The decision model changes, the quality measure may change, but the structure is always the same.</p> <p>Different quality measures encode different assumptions about what \"good\" means. MSE assumes symmetric, quadratic costs\u2014but in demand forecasting, under-predicting (stockouts) might cost more than over-predicting. For classification, cross-entropy penalizes confident wrong predictions. For outlier-heavy data, MAE or Huber loss are more robust. Choose a quality measure that aligns with your actual business costs.</p>"},{"location":"modules/02-regression/#the-goal-of-linear-regression","title":"The Goal of Linear Regression","text":"<p>Given input features, we want to predict a continuous output. We assume the relationship can be approximated by a line:</p> \\[\\hat{y} = \\beta_0 + \\beta_1 x\\] <p>Where: - \\(\\hat{y}\\) (y-hat) is the predicted value - \\(\\beta_0\\) (beta-zero) is the intercept\u2014the baseline prediction when x = 0 - \\(\\beta_1\\) (beta-one) is the slope\u2014how much y changes for a one-unit change in x - \\(x\\) is the input feature</p> <p>The key assumption is that a straight line is a reasonable approximation of the true relationship.</p> <p>Check linearity visually: scatter plots should show points around an imaginary straight line. Use <code>sns.pairplot()</code> for multiple regression. Correlation measures only linear association\u2014a perfect U-shaped relationship has r=0. Always check residual plots after fitting; curved patterns reveal non-linearity. If non-linear, try transforms (log, square root), polynomial terms (x\u00b2, x\u00b3), or inherently non-linear models (trees, neural networks).</p>"},{"location":"modules/02-regression/#the-least-squares-method","title":"The Least Squares Method","text":"<p>How do we find the best line? We find the coefficients that minimize squared prediction errors:</p> \\[\\text{minimize } \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2\\] <p>For each data point, calculate the error (actual minus predicted), square it, and add them all up. The best line is the one that makes this sum as small as possible.</p> <p>Why squared errors?</p> <ol> <li> <p>Penalizes large errors more than small ones. An error of 10 contributes 100; an error of 1 contributes only 1. The algorithm cares about avoiding big mistakes.</p> </li> <li> <p>Mathematically tractable. The function is differentiable and convex, so we can find the minimum using calculus.</p> </li> <li> <p>Nice statistical properties. Under certain assumptions, least squares gives the Best Linear Unbiased Estimator (BLUE).</p> </li> </ol> <p>For asymmetric costs, use weighted least squares (assign higher weights to observations where errors are more costly) or quantile regression (systematically over/under-predicts\u2014useful for safety stock). In deep learning, you can define arbitrary custom loss functions. Start simple; only add complexity when you have clear business justification for asymmetric costs.</p> <p>Closed-form solution:</p> \\[\\beta_1 = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum(x_i - \\bar{x})^2}\\] \\[\\beta_0 = \\bar{y} - \\beta_1\\bar{x}\\]"},{"location":"modules/02-regression/#linear-regression-assumptions","title":"Linear Regression Assumptions","text":"<p>For statistical inference to be valid, certain assumptions must hold:</p> <ol> <li>Linearity: The relationship between X and Y is linear</li> <li>Independence: Observations are independent of each other</li> <li>Homoscedasticity: Constant variance of residuals across all levels of X</li> <li>Normality: Residuals are normally distributed</li> <li>No multicollinearity: (for multiple regression) Predictors aren't too highly correlated</li> </ol> <p>When assumptions are violated:</p> Violation Symptom Solution Non-linearity Curved pattern in residuals Transform variables, add polynomial terms Heteroscedasticity Fan-shaped residual plot Transform Y, use robust standard errors Non-normality Q-Q plot deviates from line Transform Y, use bootstrap Autocorrelation Patterns in time-ordered residuals Time series methods <p>Common transformations:</p> Transform Formula Best for Log \\(\\log(x)\\) Right-skewed data, multiplicative relationships Square root \\(\\sqrt{x}\\) Count data, mild right skew Box-Cox \\((x^\\lambda - 1)/\\lambda\\) Automated selection\u2014finds optimal \u03bb <p>The log transform is the workhorse\u2014it handles right-skewed distributions (common in business data like income, prices, counts) and converts multiplicative relationships to additive ones.</p> <p>With a log-transformed target, coefficients represent percentage changes: a one-unit increase in x multiplies y by \\(e^{\\beta_1}\\). For small \u03b2 (roughly |\u03b2| &lt; 0.2), this approximates \u03b2 \u00d7 100% change. If both x and y are logged, coefficients represent elasticities: a 1% change in x \u2192 \u03b2\u2081% change in y. Always back-transform predictions before evaluating metrics, and document which scale coefficients are interpreted on.</p>"},{"location":"modules/02-regression/#gradient-descent","title":"Gradient Descent","text":"<p>We could use the closed-form solution, but gradient descent is worth learning because it's the foundation for all neural network training.</p> <p>The algorithm: 1. Start with random values for \\(\\beta_0\\) and \\(\\beta_1\\) 2. Calculate the gradient\u2014the direction of steepest error increase 3. Update parameters in the opposite direction\u2014downhill toward lower error 4. Repeat until convergence</p> <p>The landscape intuition: Imagine a hilly terrain where your position is determined by your parameter values (\\(\\beta_0\\), \\(\\beta_1\\)) and the elevation is your error (MSE). You're dropped somewhere on this terrain\u2014probably high up\u2014and want to reach the lowest valley. You can't see the whole landscape, but you can feel which way is steepest right where you're standing. The gradient tells you that direction. Each step, you walk downhill proportional to the steepness. For linear regression, this landscape is bowl-shaped (convex) with exactly one lowest point\u2014you'll always reach it eventually. Neural networks have more complex terrain with multiple valleys; you'll find a valley, but maybe not the deepest one.</p> <p>The gradients:</p> \\[\\frac{\\partial MSE}{\\partial \\beta_0} = -\\frac{2}{n}\\sum(y_i - \\hat{y}_i)\\] \\[\\frac{\\partial MSE}{\\partial \\beta_1} = -\\frac{2}{n}\\sum(y_i - \\hat{y}_i) \\cdot x_i\\] <p>Update rules:</p> \\[\\beta_0 \\leftarrow \\beta_0 - \\alpha \\cdot \\frac{\\partial MSE}{\\partial \\beta_0}\\] \\[\\beta_1 \\leftarrow \\beta_1 - \\alpha \\cdot \\frac{\\partial MSE}{\\partial \\beta_1}\\] <p>Where \\(\\alpha\\) is the learning rate\u2014how big a step we take each iteration.</p> <p>Convergence means parameters have stabilized\u2014further iterations don't meaningfully improve the solution. Common stopping criteria: loss change below threshold (1e-6), small gradient magnitude, or maximum iterations. Use a combination. For linear regression, the loss surface is convex with one global minimum; for neural networks, you'll find a local minimum (usually good enough). Monitor the loss curve\u2014oscillating or increasing loss suggests the learning rate is too high.</p> <p>Implementation:</p> <pre><code>def gradient_descent_linear_regression(\n    X, y,\n    learning_rate=0.01,\n    n_iterations=1000,\n    tolerance=1e-6\n):\n    n = len(X)\n\n    # Initialize parameters randomly\n    beta_0 = np.random.randn()\n    beta_1 = np.random.randn()\n\n    history = []\n\n    for i in range(n_iterations):\n        # Predictions\n        y_pred = beta_0 + beta_1 * X\n\n        # Compute gradients\n        d_beta_0 = -2/n * np.sum(y - y_pred)\n        d_beta_1 = -2/n * np.sum((y - y_pred) * X)\n\n        # Update parameters\n        beta_0 = beta_0 - learning_rate * d_beta_0\n        beta_1 = beta_1 - learning_rate * d_beta_1\n\n        # Track loss\n        mse = np.mean((y - y_pred)**2)\n        history.append(mse)\n\n        # Check convergence\n        if i &gt; 0 and abs(history[-1] - history[-2]) &lt; tolerance:\n            break\n\n    return beta_0, beta_1, history\n</code></pre> <p>Numerical Example: Watching Gradient Descent Converge</p> <pre><code>import numpy as np\n\n# Generate data: y = 3 + 2x + noise\nnp.random.seed(42)\nX = np.random.uniform(low=0, high=10, size=100)\ny = 3.0 + 2.0 * X + np.random.normal(loc=0, scale=1.5, size=100)\n\n# Gradient descent with learning_rate=0.02\nbeta_0, beta_1 = 0.0, 0.0\nfor iteration in range(501):\n    y_pred = beta_0 + beta_1 * X\n    d_beta_0 = -2/100 * np.sum(y - y_pred)\n    d_beta_1 = -2/100 * np.sum((y - y_pred) * X)\n    beta_0 = beta_0 - 0.02 * d_beta_0\n    beta_1 = beta_1 - 0.02 * d_beta_1\n    if iteration in [0, 10, 50, 100, 200, 500]:\n        mse = np.mean((y - y_pred)**2)\n        print(f\"Iter {iteration:3d}: \u03b2\u2080={beta_0:.4f}, \u03b2\u2081={beta_1:.4f}, MSE={mse:.4f}\")\n</code></pre> <p>Output: <pre><code>Iter   0: \u03b2\u2080=0.6787, \u03b2\u2081=2.3374, MSE=188.2950\nIter  10: \u03b2\u2080=0.6787, \u03b2\u2081=2.3374, MSE=3.7994\nIter  50: \u03b2\u2080=1.6304, \u03b2\u2081=2.1911, MSE=2.6278\nIter 100: \u03b2\u2080=2.3539, \u03b2\u2081=2.0799, MSE=2.0813\nIter 200: \u03b2\u2080=3.0051, \u03b2\u2081=1.9798, MSE=1.8434\nIter 500: \u03b2\u2080=3.3115, \u03b2\u2081=1.9328, MSE=1.8149\n</code></pre></p> <p>Interpretation: Starting from zeros, gradient descent iteratively improves toward the true parameters (\u03b2\u2080=3, \u03b2\u2081=2). MSE drops rapidly at first (188\u21924 in just 10 iterations), then refines more slowly. The final estimates (3.31, 1.93) are close to truth\u2014the remaining gap is due to noise in the data, not algorithm failure.</p> <p>Source: <code>slide_computations/module2_examples.py</code> \u2013 <code>demo_gradient_descent_convergence()</code></p>"},{"location":"modules/02-regression/#learning-rate-trade-offs","title":"Learning Rate Trade-offs","text":"<p>The learning rate \\(\\alpha\\) is crucial:</p> Too Small Just Right Too Large Very slow convergence Converges in reasonable time Overshoots the minimum Safe but inefficient Reaches good solution Can diverge (loss increases!) <p>What the loss curve tells you: Plot MSE vs. iteration number to diagnose learning rate issues: - Too small (\u03b1 = 0.0001): The curve creeps downward very slowly\u2014you might need 10,000+ iterations to converge. The slope is shallow but always decreasing. - Just right (\u03b1 = 0.01): The curve drops quickly at first, then flattens as you approach the minimum. Convergence in hundreds, not thousands, of iterations. - Too large (\u03b1 = 0.1): The curve may oscillate wildly (bouncing up and down) or explode upward. If loss increases iteration-over-iteration, your learning rate is too high.</p> <p>If your loss keeps increasing, the learning rate is too high. Reduce it by a factor of 10.</p> <p>Adaptive learning rate methods automatically adjust during training. Learning rate schedules (step decay, exponential decay, cosine annealing) decrease the rate over time\u2014large steps initially, smaller steps later. Adaptive optimizers (AdaGrad, RMSprop, Adam) adjust per parameter based on gradient history. Adam is the default for deep learning\u2014it works well with default learning rate 0.001. For scikit-learn's linear regression, optimization is handled automatically.</p> <p>Numerical Example: Learning Rate Effects</p> <p>Same dataset, three different learning rates:</p> Learning Rate Status Iterations Final MSE 0.0001 Not converged 1000 4.04 0.01 Converged 920 1.81 0.1 DIVERGED 6 \u221e <p>Interpretation: With \u03b1=0.0001 (too small), the algorithm made progress but didn't converge in 1000 iterations\u2014MSE is still far from optimal. With \u03b1=0.01 (just right), it converged in 920 iterations to the best solution (MSE=1.81). With \u03b1=0.1 (too large), the algorithm diverged after just 6 iterations\u2014the loss exploded to infinity. When you see increasing loss, immediately reduce the learning rate by 10x.</p> <p>Source: <code>slide_computations/module2_examples.py</code> \u2013 <code>demo_learning_rate_effects()</code></p>"},{"location":"modules/02-regression/#using-statsmodels-for-regression","title":"Using statsmodels for Regression","text":"<p>In practice, we use libraries. For regression with good statistical output, use statsmodels:</p> <pre><code>import statsmodels.formula.api as smf\n\n# R-style formula interface\nmodel = smf.ols(\n    formula='sales ~ advertising + price',\n    data=df\n)\nresults = model.fit()\nprint(results.summary())\n</code></pre> <p>Key statistics in the output:</p> <p>R\u00b2 (Coefficient of Determination):</p> \\[R^2 = 1 - \\frac{\\sum(y_i - \\hat{y}_i)^2}{\\sum(y_i - \\bar{y})^2}\\] <p>R\u00b2 tells you what proportion of variance your model explains. R\u00b2 = 0.75 means 75% explained, 25% unexplained.</p> <p>p-values for coefficients: - Tests: \"Is this coefficient different from zero?\" - p &lt; 0.05 is conventional threshold for \"statistically significant\" - Caution: p-values don't tell you effect size</p> <p>The 0.05 threshold is a historical convention from R.A. Fisher, not a magic number. Problems: with enough data, trivial effects become \"significant\"; with little data, real effects may not be. Modern practice: report exact p-values, consider effect sizes and confidence intervals, and remember that practical significance matters more than statistical significance in business\u2014a statistically significant 0.1% improvement might not be worth implementing.</p> <p>Confidence intervals: - 95% CI of [1.8, 3.2] means: \"We're 95% confident the true effect is between $1.80 and $3.20\" - If the CI includes zero, the effect is not statistically significant</p> <p>F-statistic: - Tests whether the model as a whole is useful - \"Is this model better than just predicting the mean?\"</p> <p>When to use scikit-learn instead: When building ML pipelines, when prediction is the main goal, when you need cross-validation and hyperparameter tuning.</p>"},{"location":"modules/02-regression/#interpreting-coefficients","title":"Interpreting Coefficients","text":"<p>The standard interpretation: \"A one-unit increase in X is associated with a \\(\\beta_1\\) change in Y, holding all else constant.\"</p> <p>Example: <pre><code>sales = 50,000 + 2.5 \u00d7 advertising + 1,200 \u00d7 sales_staff\n</code></pre></p> <p>Translation: - Baseline sales: $50,000 (when advertising = 0 and sales_staff = 0) - Each $1 in advertising \u2192 $2.50 more in sales (250% ROI) - Each additional sales staff \u2192 $1,200 more in sales</p> <p>Caution: Correlation \u2260 Causation</p> <p>Regression shows association, not causation. When we say \"Each $1 in advertising \u2192 $2.50 more in sales,\" we mean they're associated. We haven't proven advertising causes sales.</p> <p>Classic example: Ice cream sales and drowning deaths are positively correlated. Both are caused by summer heat\u2014a confounding variable.</p> <p>Establishing causation requires experimental design or careful causal inference methods. Randomized experiments (A/B tests) are the gold standard. Natural experiments (policy changes affecting some regions) create quasi-random groups. Instrumental variables find factors that affect treatment but not outcome directly. Causal inference frameworks (propensity score matching, difference-in-differences) try to estimate effects from observational data. With observational regression alone, you have association\u2014to claim causation, you need a convincing argument for why confounders are controlled.</p>"},{"location":"modules/02-regression/#residual-analysis","title":"Residual Analysis","text":"<p>Residuals reveal problems:</p> \\[e_i = y_i - \\hat{y}_i\\] <p>If the model is good and assumptions hold, residuals should look like random noise.</p> <p>Diagnostic plots:</p> <ol> <li>Residuals vs. Fitted Values: Should show random scatter around zero</li> <li>Curve = non-linearity</li> <li> <p>Funnel = heteroscedasticity</p> </li> <li> <p>Q-Q Plot: Residuals vs. theoretical normal quantiles</p> </li> <li>Straight line = normality satisfied</li> <li>Curves at ends = heavy/light tails</li> </ol> <pre><code>import matplotlib.pyplot as plt\nimport scipy.stats as stats\n\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 3, 1)\nplt.scatter(y_pred, residuals)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('Fitted Values')\nplt.ylabel('Residuals')\nplt.title('Residuals vs Fitted')\n\nplt.subplot(1, 3, 2)\nstats.probplot(residuals, dist=\"norm\", plot=plt)\nplt.title('Q-Q Plot')\n\nplt.subplot(1, 3, 3)\nplt.hist(residuals, bins=30, edgecolor='black')\nplt.xlabel('Residuals')\nplt.title('Residual Distribution')\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Reading residual plots\u2014a pattern recognition guide:</p> What You See What It Means Action Random scatter around zero Good! Assumptions satisfied Proceed with model U-shape or inverted-U curve Non-linear relationship missed Add polynomial terms or transform variables Funnel shape (wider on one side) Heteroscedasticity\u2014variance changes Transform Y (log), use robust standard errors Clusters or groups Subgroups with different patterns Add categorical variable, consider separate models Pattern over time (if ordered) Autocorrelation Time series methods, lagged variables <p>The residual plot is your diagnostic dashboard. Even if R\u00b2 looks great, always check residuals\u2014patterns reveal problems that summary statistics hide.</p> <p>Key insight: High R\u00b2 with a patterned residual plot is still a bad model. The pattern means you're missing structure in the data.</p> <p>To fix a curved residual pattern: (1) Identify which feature causes it by plotting residuals against each predictor. (2) Try transforms\u2014log for diminishing returns, square root for counts, polynomial terms (x\u00b2, x\u00b3). (3) Use <code>PolynomialFeatures(degree=2)</code> with regularization. (4) If transforms don't help, consider non-linear models (trees, GAMs, neural networks). (5) Verify the fix\u2014residuals should show random scatter.</p>"},{"location":"modules/02-regression/#common-misconceptions","title":"Common Misconceptions","text":"Misconception Reality \"Correlation implies causation\" Regression shows association only. Causation requires experimental design or causal inference methods. \"High R\u00b2 means the model is good\" R\u00b2 can be high due to overfitting. Must check test set performance and residual plots. \"The intercept is always meaningful\" Often it's not (e.g., salary when experience = 0 years). Focus on slopes for interpretation. \"Larger coefficients mean more important features\" Only true if features are on the same scale. Use standardized coefficients to compare."},{"location":"modules/02-regression/#22-multiple-linear-regression","title":"2.2 Multiple Linear Regression","text":""},{"location":"modules/02-regression/#multiple-predictors","title":"Multiple Predictors","text":"<p>In the real world, we rarely have just one predictor:</p> \\[\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_d x_d\\] <p>Why multiple predictors? 1. Single predictor rarely captures the full story 2. Control for confounding variables 3. Improve prediction accuracy</p> <p>Key insight: Each coefficient represents the effect of that variable while controlling for the others. This is different from running separate simple regressions!</p> <p>Multiple regression coefficients are partial effects\u2014the effect of one variable holding others constant. Simple regression captures both direct and indirect effects through correlated variables. If experience and education are correlated, simple regression conflates their effects; multiple regression \"controls for\" education when estimating experience's effect. Sometimes adding variables can even flip coefficient signs (Simpson's paradox)\u2014the Berkeley admissions example showed apparent disadvantage for women overall that reversed within each department.</p>"},{"location":"modules/02-regression/#confounding-variables","title":"Confounding Variables","text":"<p>Example: - Simple regression: Salary ~ Experience \u2192 \\(\\beta = \\$5,000\\) per year - Multiple regression: Salary ~ Experience + Education \u2192 \\(\\beta_{exp} = \\$3,500\\) per year</p> <p>The coefficient for experience dropped because education was confounded with experience. People with more experience often have more education. The simple regression was attributing some of education's effect to experience.</p> <p>The causal diagram view: Picture arrows showing what causes what:</p> <pre><code>         Education\n         \u2199      \u2198\n    Experience \u2192 Salary\n</code></pre> <p>Education affects both experience (more education often means less early work experience) and salary. When we don't control for education, we're measuring a \"back-door\" path: Experience \u2190 Education \u2192 Salary. The simple regression captures this indirect association alongside the direct effect. Multiple regression \"closes\" this path by holding education constant, isolating experience's direct effect.</p> <p>Three types of variables to distinguish: - Confounder (Education above): Affects both X and Y. Include it to avoid bias. - Mediator (e.g., Skills on the Experience\u2192Salary path): On the causal chain between X and Y. Include only if you want indirect effects removed. - Collider (e.g., \"Got Promoted\" affected by both Experience and Salary): Controlling for it creates spurious associations. Don't include.</p> <p>To identify confounders, ask: \"What could affect both X and Y?\" Draw causal diagrams\u2014confounders have arrows TO both predictor and outcome. Check correlations, but correlation alone isn't sufficient; you need domain reasoning. Don't throw every variable in\u2014mediators (on the causal path) or colliders (affected by both) can introduce bias. Include variables that theory suggests are confounders and were measured before the treatment.</p>"},{"location":"modules/02-regression/#multicollinearity","title":"Multicollinearity","text":"<p>What is multicollinearity? High correlation between predictors.</p> <p>Symptoms: - Coefficients change dramatically when you add/remove features - High R\u00b2 but few significant individual predictors - Signs of coefficients seem wrong</p> <p>The see-saw analogy: Imagine predicting house price from both \"total square feet\" and \"number of rooms.\" These are highly correlated\u2014bigger houses have more rooms. In the model, their coefficients are like two kids on a see-saw that must balance to produce the right prediction. If one kid (coefficient) goes up, the other must go down. The model can balance them many different ways: (sqft=+100, rooms=-50), (sqft=+50, rooms=0), (sqft=+150, rooms=-100)\u2014all giving similar predictions. The total effect is stable, but the individual coefficients are unstable. That's why coefficients jump around with small data changes when multicollinearity is present.</p>"},{"location":"modules/02-regression/#detecting-multicollinearity-vif","title":"Detecting Multicollinearity: VIF","text":"<p>Variance Inflation Factor (VIF):</p> \\[VIF_j = \\frac{1}{1 - R_j^2}\\] <p>Where \\(R_j^2\\) is R\u00b2 from regressing feature j on all other features.</p> VIF Value Interpretation VIF = 1 No correlation with other features VIF &gt; 5 Moderate multicollinearity\u2014investigate VIF &gt; 10 Serious multicollinearity\u2014must address <pre><code>from statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Calculate VIF for each feature\nfor i in range(X.shape[1]):\n    vif = variance_inflation_factor(X.values, i)\n    print(f\"{feature_names[i]}: VIF = {vif:.2f}\")\n</code></pre> <p>Numerical Example: VIF Multicollinearity Detection</p> <p>Scenario 1: Three independent features</p> Feature VIF x1 1.03 x2 1.01 x3 1.02 <p>Scenario 2: Added x4 which is highly correlated with x1 (r = 0.994)</p> Feature VIF x1 88.60 x2 1.02 x3 1.03 x4 (corr with x1) 88.12 <p>Interpretation: Independent features have VIF \u2248 1, indicating no multicollinearity. When x4 (nearly identical to x1) is added, both x1 and x4 jump to VIF \u2248 88\u2014severe multicollinearity. The x2 and x3 VIF values remain unaffected because they're not involved in the collinear relationship. VIF &gt; 10 requires action: remove x4, combine with x1, or use Ridge.</p> <p>Source: <code>slide_computations/module2_examples.py</code> \u2013 <code>demo_vif_multicollinearity()</code></p> <p>Solutions for multicollinearity: 1. Remove one of the correlated features 2. Combine features (average or PCA) 3. Use regularization (Ridge handles this well) 4. Accept it if prediction is your only goal</p> <p>Multicollinearity doesn't affect prediction accuracy\u2014but it creates problems for interpretation. Coefficients become unstable (jumping around with small data changes), standard errors inflate (true effects appear \"not significant\"), and signs may reverse. If your goal is purely prediction, ignore it. If you need interpretation, address it.</p>"},{"location":"modules/02-regression/#why-regularize","title":"Why Regularize?","text":"<p>Regularization prevents overfitting by penalizing large coefficients:</p> \\[\\text{minimize } \\sum(y_i - \\hat{y}_i)^2 + \\lambda \\cdot \\text{penalty}(\\beta)\\] <p>The parameter \\(\\lambda\\) controls how strong the penalty is.</p> <p>Large coefficients can indicate overfitting\u2014the model making sharp adjustments for individual data points (memorizing). Signs of overfitting: large coefficients only with small samples, huge standard errors, or poor test performance. Regularization forces the model to justify large coefficients\u2014if fitting noise, the penalty isn't worth it; if fitting real patterns, the benefit outweighs the penalty. Note: \"large\" depends on feature scale\u2014always standardize before judging.</p>"},{"location":"modules/02-regression/#l1-regularization-lasso","title":"L1 Regularization (Lasso)","text":"<p>Penalty: \\(\\lambda \\sum|\\beta_j|\\)</p> <p>Effect: Can shrink coefficients exactly to zero \u2192 automatic feature selection!</p> <p>The geometry of regularization: Picture a 2D space where each axis is a coefficient (\u03b2\u2081, \u03b2\u2082). The loss function forms elliptical contours\u2014concentric ovals centered on the OLS solution. Regularization adds a constraint: \"stay within this budget region.\"</p> <ul> <li>L1 (Lasso): The budget region is a diamond with corners touching the axes. As you shrink the budget, the loss contours first touch a corner\u2014where one coefficient equals zero. The diamond's sharp corners make this likely.</li> <li>L2 (Ridge): The budget region is a circle. Loss contours touch it tangentially, almost never on an axis. Coefficients shrink smoothly toward zero but rarely reach exactly zero.</li> </ul> <p>Why this matters for feature selection: L1's diamond geometry naturally produces sparse solutions (some coefficients = 0). L2's circle geometry keeps all features, just smaller. Additionally, the L1 gradient is constant (\u00b11) regardless of how small \u03b2 gets\u2014always pulling toward zero\u2014while the L2 gradient (2\u03b2) weakens as \u03b2 approaches zero.</p> <p>When to use: - You suspect many features are irrelevant - You want an interpretable, sparse model - Feature selection is an explicit goal</p> <pre><code>from sklearn.linear_model import Lasso\n\nlasso = Lasso(alpha=0.1, random_state=42)\nlasso.fit(X_train_scaled, y_train)\n\n# See which features were selected (non-zero coefficients)\nselected_features = [f for f, c in zip(feature_names, lasso.coef_) if c != 0]\nprint(f\"Selected {len(selected_features)} features\")\n</code></pre> <p>Numerical Example: Lasso Feature Selection</p> <p>True model: y = 5\u00b7x1 + 3\u00b7x2 \u2212 2\u00b7x3 (features x4\u2013x10 are pure noise)</p> Feature \u03b1=0.01 \u03b1=0.1 \u03b1=0.5 \u03b1=1.0 x1 4.50 4.37 3.89 3.29 x2 3.41 3.32 2.92 2.43 x3 \u22122.07 \u22121.94 \u22121.49 \u22120.93 x4 0.09 0 0 0 x5 0.03 0 0 0 x6 \u22120.07 0 0 0 x7\u2013x10 small 0 0 0 Non-zero 10 3 3 3 <p>Interpretation: At low regularization (\u03b1=0.01), all 10 features have non-zero coefficients\u2014noise features show small spurious effects. As \u03b1 increases, noise features (x4\u2013x10) shrink exactly to zero, leaving only the three true predictors. By \u03b1=0.1, Lasso has automatically identified which features matter. This is automatic feature selection in action.</p> <p>Source: <code>slide_computations/module2_examples.py</code> \u2013 <code>demo_lasso_feature_selection()</code></p>"},{"location":"modules/02-regression/#l2-regularization-ridge","title":"L2 Regularization (Ridge)","text":"<p>Penalty: \\(\\lambda \\sum\\beta_j^2\\)</p> <p>Effect: Shrinks all coefficients toward zero, but never exactly zero</p> <p>When to use: - Multicollinearity is present - All features are potentially relevant - Prediction accuracy is the main goal</p> <pre><code>from sklearn.linear_model import Ridge\n\nridge = Ridge(alpha=1.0, random_state=42)\nridge.fit(X_train_scaled, y_train)\n</code></pre>"},{"location":"modules/02-regression/#elastic-net-combining-l1-and-l2","title":"Elastic Net: Combining L1 and L2","text":"<p>Penalty: \\(\\lambda_1 \\sum|\\beta_j| + \\lambda_2 \\sum\\beta_j^2\\)</p> <p>Benefits: - Feature selection from L1 (can zero out coefficients) - Stability from L2 (handles correlated features better) - More flexible than either alone</p> <pre><code>from sklearn.linear_model import ElasticNet\n\nelastic = ElasticNet(\n    alpha=0.1,       # Overall regularization strength\n    l1_ratio=0.5,    # Balance between L1 and L2\n    random_state=42\n)\nelastic.fit(X_train_scaled, y_train)\n</code></pre> <p>Numerical Example: Ridge vs Lasso Comparison</p> <p>True model: y = 3\u00b7x1 + 2\u00b7x2 + 1.5\u00b7x3 (x4 is noise, x1 and x2 are correlated with r=0.89)</p> Feature OLS Ridge Lasso x1 2.88 2.86 2.84 x2 (corr) 2.05 2.07 1.98 x3 1.49 1.48 1.38 x4 (noise) 0.08 0.08 0.00 <p>Interpretation: All methods recover approximate true coefficients. The key difference is in handling x4 (noise): Ridge keeps a small non-zero coefficient (0.08); Lasso zeros it out completely. Both Ridge and Lasso shrink coefficients toward zero, but only Lasso performs selection. When x1 and x2 are correlated, Ridge distributes weight across both; Lasso might keep one and drop the other (not shown here, but common with stronger regularization).</p> <p>When to choose which: - Ridge: When all features likely contribute, especially with multicollinearity - Lasso: When you want automatic feature selection (sparse model) - Elastic Net: When you want both\u2014feature selection plus stability</p> <p>Source: <code>slide_computations/module2_examples.py</code> \u2013 <code>demo_ridge_vs_lasso()</code></p>"},{"location":"modules/02-regression/#choosing-regularization-strength","title":"Choosing Regularization Strength","text":"<p>Use cross-validation:</p> <pre><code>from sklearn.linear_model import LassoCV, RidgeCV\n\n# Automatic alpha selection via CV\nlasso_cv = LassoCV(\n    alphas=np.logspace(-4, 1, 50),\n    cv=5\n)\nlasso_cv.fit(X_train_scaled, y_train)\nprint(f\"Best alpha: {lasso_cv.alpha_}\")\n</code></pre> <p>Important: Always scale your features before regularization! Regularization penalizes large coefficients, and features on different scales are penalized unfairly.</p> <p>To convert scaled coefficients back to original units: \\(\\beta_{original} = \\frac{\\beta_{scaled}}{\\sigma}\\). In code: <code>original_coefs = model.coef_ / scaler.scale_</code>. Back-transform for business communication (\"each $1000 in spending \u2192 X more sales\"); keep scaled for comparing feature importance. Save your scaler object so you have access to <code>mean_</code> and <code>scale_</code> attributes.</p>"},{"location":"modules/02-regression/#linear-regression-as-a-neural-network","title":"Linear Regression as a Neural Network","text":"<p>Here's something that will pay off in Module 6:</p> <pre><code>Input Layer          Output Layer\n\n   x\u2081 \u2500\u2500\u2500\u2500 w\u2081 \u2500\u2500\u2500\u2500\u2510\n                   \u251c\u2500\u2500\u2192 \u03a3 + b \u2500\u2500\u2192 \u0177\n   x\u2082 \u2500\u2500\u2500\u2500 w\u2082 \u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Linear regression is just a neural network with no hidden layers! - The weights (w) are our coefficients (\\(\\beta\\)) - The bias (b) is our intercept (\\(\\beta_0\\)) - We sum the weighted inputs and add the bias</p> <p>When we add hidden layers and non-linear activations, we get deep learning. But the foundation\u2014weighted sums optimized by gradient descent\u2014is exactly what we learned here.</p>"},{"location":"modules/02-regression/#common-misconceptions_1","title":"Common Misconceptions","text":"Misconception Reality \"Regularization always hurts training performance\" True, but that's the point! We sacrifice training fit for better generalization. \"Lasso always performs feature selection\" Only with sufficient regularization. Very small alpha may keep all features. \"More features always improve the model\" Only if they're informative. Irrelevant features add noise and overfitting risk. \"Ridge is inferior because it doesn't zero out coefficients\" Ridge is often better when all features matter. Lasso is for sparse solutions."},{"location":"modules/02-regression/#23-business-application","title":"2.3 Business Application","text":""},{"location":"modules/02-regression/#end-to-end-regression-workflow","title":"End-to-End Regression Workflow","text":"<ol> <li>Business problem definition - What are we predicting? Why does it matter?</li> <li>Data collection and exploration - EDA, quality assessment</li> <li>Feature engineering - Create informative variables from raw data</li> <li>Model building and evaluation - Compare approaches, cross-validate</li> <li>Interpretation and communication - Translate for stakeholders</li> </ol> <p>Most of your time should go into steps 1-3. The modeling itself is almost mechanical once you have good data and features.</p> <p>When is \"enough\" feature engineering? Track validation performance as you add features\u2014stop when new features don't improve it. The 80/20 rule applies: basic features (raw data, simple transforms) usually dominate; exotic interactions rarely add much. Don't exceed n/10 to n/20 features for n samples without regularization. Start simple, add complexity where residual analysis suggests it, and stop when validation performance plateaus.</p>"},{"location":"modules/02-regression/#feature-engineering-patterns","title":"Feature Engineering Patterns","text":"<p>Time-based features: <pre><code>import polars as pl\n\ndf = df.with_columns([\n    pl.col('date').dt.weekday().alias('day_of_week'),\n    pl.col('date').dt.month().alias('month'),\n    pl.col('date').dt.weekday().is_in([5, 6]).cast(pl.Int32).alias('is_weekend'),\n])\n</code></pre></p> <p>Aggregations: <pre><code>customer_features = df.group_by('customer_id').agg([\n    pl.col('amount').sum().alias('total_spend'),\n    pl.col('amount').mean().alias('avg_order'),\n    pl.col('amount').count().alias('order_count'),\n])\n</code></pre></p> <p>Interactions: <pre><code>df = df.with_columns([\n    (pl.col('price') / pl.col('sqft')).alias('price_per_sqft'),\n])\n</code></pre></p> <p>Polynomial features: <pre><code>from sklearn.preprocessing import PolynomialFeatures\n\npoly = PolynomialFeatures(degree=2, include_bias=False)\nX_poly = poly.fit_transform(X)\n</code></pre></p>"},{"location":"modules/02-regression/#translating-statistics-to-business-language","title":"Translating Statistics to Business Language","text":"Statistical Term Business Translation Coefficient = 2.5 \"Every $1 more in advertising is associated with $2.50 more in sales\" R\u00b2 = 0.75 \"Our model explains about 75% of the variation in sales\" p-value &lt; 0.05 \"We're confident this factor genuinely affects sales, not just by chance\" 95% CI: [1.8, 3.2] \"We estimate the effect is between $1.80 and $3.20 per dollar spent\" <p>Standardized vs. unstandardized coefficients: - Unstandardized (original units): For business interpretation\u2014\"Every $1 in advertising \u2192 $2.50 in sales\" - Standardized (z-scores): For comparing feature importance\u2014\"Which variable has the biggest effect?\"</p>"},{"location":"modules/02-regression/#sensitivity-analysis","title":"Sensitivity Analysis","text":"<p>Stakeholders love \"what-if\" scenarios:</p> <pre><code># Base prediction\nbase = model.predict(X_current)\n\n# Modified scenario\nX_modified = X_current.copy()\nX_modified['advertising'] *= 1.10\nnew = model.predict(X_modified)\n\nprint(f\"10% more advertising \u2192 ${new - base:,.0f} more sales\")\n</code></pre>"},{"location":"modules/02-regression/#the-business-memo","title":"The Business Memo","text":"<p>Structure:</p> <ol> <li>Executive Summary - 2-3 sentences. Main finding plus recommendation.</li> <li>Key Findings - Bullet points with business impact.</li> <li>Recommendations - Specific, actionable steps.</li> <li>Limitations - What the analysis cannot tell us.</li> </ol> <p>Rules: - No code - No jargon without explanation - No p-values without context - Always include limitations</p> <p>The limitations section matters. Every analysis has limitations. If you don't acknowledge them, you're either unaware (bad) or hiding them (worse).</p> <p>Frame limitations as scope definition, not weakness. Instead of \"The model doesn't account for competitor pricing,\" say \"The model predicts based on our historical data. For decisions involving major competitor moves, supplement with competitive intelligence.\" Quantify uncertainty (\"accurate to within \u00b115% 80% of the time\") rather than saying \"predictions might be wrong.\" Lead with capabilities, then contextualize what's left. Provide recommendations within limitations (\"Given \u00b115% uncertainty, keep 20% buffer inventory\"). Models presented as perfect lose credibility when they fail; honest limitations build trust.</p>"},{"location":"modules/02-regression/#reflection-questions","title":"Reflection Questions","text":"<ol> <li> <p>You implement gradient descent and the loss keeps increasing. What's likely wrong? How would you fix it?</p> </li> <li> <p>A regression coefficient for 'ice cream sales' on 'drowning deaths' is positive and statistically significant. Should ice cream vendors be concerned about causing drownings?</p> </li> <li> <p>Your model has R\u00b2 = 0.95 but the residual plot shows a clear curved pattern. Is this a good model?</p> </li> <li> <p>When would you prefer Lasso over Ridge regression? Give a business scenario.</p> </li> <li> <p>You add more features to your model and R\u00b2 on training data increases, but test set performance decreases. What's happening?</p> </li> <li> <p>How would you explain regularization to a business stakeholder without using math?</p> </li> </ol>"},{"location":"modules/02-regression/#practice-problems","title":"Practice Problems","text":"<ol> <li> <p>Given a fitted model equation, interpret each coefficient in business terms.</p> </li> <li> <p>Diagnose issues from residual plots (identify non-linearity, heteroscedasticity).</p> </li> <li> <p>Calculate VIF and decide which features to remove.</p> </li> <li> <p>Choose between Lasso, Ridge, and ElasticNet for different scenarios.</p> </li> <li> <p>Write a business memo interpreting regression results for a non-technical audience.</p> </li> </ol>"},{"location":"modules/02-regression/#chapter-summary","title":"Chapter Summary","text":"<p>Six key takeaways from Module 2:</p> <ol> <li> <p>Linear regression minimizes squared errors to find the best-fit line. The math is elegant, but the intuition is simple: find the line that makes predictions as close as possible to reality.</p> </li> <li> <p>Gradient descent iteratively optimizes parameters. This is the foundation for all neural network training. Learning rate matters: too small is slow, too large is unstable.</p> </li> <li> <p>Coefficients show association, not causation. Be careful how you communicate this. \"Associated with\" is not \"causes.\"</p> </li> <li> <p>Residual analysis reveals assumption violations. Always check your residual plots. High R\u00b2 with a patterned residual plot is a bad model.</p> </li> <li> <p>Regularization prevents overfitting. Lasso selects features, Ridge handles multicollinearity, Elastic Net does both.</p> </li> <li> <p>Communication matters. Translate statistics to business impact. Include limitations. Make it actionable.</p> </li> </ol>"},{"location":"modules/02-regression/#whats-next","title":"What's Next","text":"<p>In Module 3, we tackle Classification Methods: - Logistic regression (extends linear regression to classification) - Decision boundaries and probability estimation - Classification metrics in depth - Handling imbalanced classes</p> <p>You'll apply everything from Module 2: - Same data preparation workflow - Gradient descent concepts - Regularization techniques</p> <p>The difference: we're predicting categories instead of numbers.</p>"},{"location":"modules/03-classification/","title":"Module 3: Classification Methods","text":""},{"location":"modules/03-classification/#introduction","title":"Introduction","text":"<p>We've covered a lot of ground\u2014foundations in Module 1, regression in Module 2. Now we move to classification, which is arguably even more prevalent in business applications.</p> <p>Think about the decisions businesses make every day: Should we approve this loan? Is this transaction fraudulent? Will this customer cancel their subscription? Is this email spam? These are all classification problems\u2014predicting a category, not a number.</p> <p>The concepts extend to multiclass classification: logistic regression uses softmax instead of sigmoid; decision trees handle it naturally; evaluation uses per-class precision/recall and NxN confusion matrices. The fundamentals transfer directly\u2014the mechanics get more complex but the reasoning stays the same.</p> <p>This module covers four major topics: logistic regression (extending regression concepts to classification), decision trees (intuitive classifiers that set us up for ensemble methods), handling imbalanced data (because when 99% of transactions are legitimate, accuracy is meaningless), and hyperparameter optimization.</p>"},{"location":"modules/03-classification/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, you should be able to:</p> <ol> <li>Explain the mechanics and interpretation of logistic regression, including log odds and probability</li> <li>Build and interpret decision tree classifiers, understanding their tendency to overfit</li> <li>Apply appropriate techniques for handling imbalanced classification problems</li> <li>Use hyperparameter optimization techniques to improve model performance</li> <li>Select appropriate evaluation metrics based on business context</li> </ol>"},{"location":"modules/03-classification/#31-logistic-regression","title":"3.1 Logistic Regression","text":""},{"location":"modules/03-classification/#three-components-logistic-regression","title":"Three Components: Logistic Regression","text":"<p>Connecting to the framework from Module 2:</p> Component Logistic Regression Decision Model \\(P(Y=1) = \\sigma(\\beta_0 + \\beta_1 x_1 + ...)\\) \u2014 sigmoid of linear combination Quality Measure Cross-entropy (log loss) \u2014 penalizes confident wrong predictions Update Method Gradient descent on log-likelihood <p>The decision model changes from a line to a sigmoid curve, and the quality measure changes from SSE to cross-entropy\u2014but the overall structure is identical to linear regression.</p>"},{"location":"modules/03-classification/#why-linear-regression-fails-for-classification","title":"Why Linear Regression Fails for Classification","text":"<p>Binary outcomes are coded as 0 or 1. If we fit a line, predictions can be less than 0 or greater than 1. \"There's a -15% chance of churn\" is meaningless.</p> <p>The solution: Transform the output so it's always between 0 and 1.</p> <p>Other functions map to (0,1)\u2014probit, scaled tanh\u2014but sigmoid has unique advantages: its derivative is expressible in terms of the output (efficient gradients), its inverse is the logit (clean coefficient interpretation as log-odds), and it arises from maximum entropy principles. Tools and practices are standardized around it.</p>"},{"location":"modules/03-classification/#the-sigmoid-function","title":"The Sigmoid Function","text":"\\[\\sigma(z) = \\frac{1}{1 + e^{-z}}\\] <p>Where \\(z = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ...\\)</p> <p>Key properties: - Output is always between 0 and 1\u2014valid probability - S-shaped curve\u2014small changes in x have the biggest effect near 0.5 - At z=0, output is exactly 0.5</p> <p>The math: - When z is large and positive: \\(e^{-z} \\to 0\\), so \\(\\sigma(z) \\to 1\\) - When z is large and negative: \\(e^{-z} \\to \\infty\\), so \\(\\sigma(z) \\to 0\\) - When z = 0: \\(e^{0} = 1\\), so \\(\\sigma(0) = 0.5\\)</p> <p>Why this particular S-shape? The sigmoid isn't arbitrary\u2014it's the only function that: (1) maps any real number to (0,1), (2) is symmetric around 0.5, and (3) has a derivative expressible in terms of itself (making gradient descent efficient). Think of z as a \"confidence score\": strongly negative z means the model is confident in class 0, strongly positive means confident in class 1, and z near 0 means the model is uncertain. The sigmoid converts this confidence into a probability. The \"action zone\" where probability changes meaningfully is roughly z \u2208 [-4, 4]\u2014outside this range, the model is essentially certain.</p> <p>Numerical Example: Sigmoid Function in Action</p> <pre><code>import numpy as np\n\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\nz_values = [-6, -4, -2, 0, 2, 4, 6]\nfor z in z_values:\n    prob = sigmoid(z)\n    print(f\"z = {z:&gt;3}: \u03c3(z) = {prob:.4f}\")\n</code></pre> <p>Output: <pre><code>z =  -6: \u03c3(z) = 0.0025\nz =  -4: \u03c3(z) = 0.0180\nz =  -2: \u03c3(z) = 0.1192\nz =   0: \u03c3(z) = 0.5000\nz =   2: \u03c3(z) = 0.8808\nz =   4: \u03c3(z) = 0.9820\nz =   6: \u03c3(z) = 0.9975\n</code></pre></p> <p>Interpretation: At z=0, probability is exactly 0.5 (maximum uncertainty). Moving to z=\u00b14 brings probability within 2% of the extremes (0 or 1). At z=\u00b16, the model is 99.75% confident. The \"interesting range\" where probability changes meaningfully is roughly z \u2208 [-4, 4].</p> <p>Source: <code>slide_computations/module3_examples.py</code> - <code>demo_sigmoid_function()</code></p>"},{"location":"modules/03-classification/#understanding-odds-and-log-odds","title":"Understanding Odds and Log Odds","text":"<p>Step 1: Odds</p> \\[Odds = \\frac{P(Y=1)}{P(Y=0)} = \\frac{p}{1-p}\\] <p>If P(churn) = 0.75, odds = 0.75/0.25 = 3. \"3 to 1 odds of churning.\"</p> <p>Step 2: Log Odds (Logit)</p> \\[\\log\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 x_1 + ...\\] <p>Key insight: The log odds ARE linear in the predictors. This is where the \"linear\" in logistic regression comes from.</p> <p>Example: Model: \\(\\log(odds) = -2 + 0.5 \\times age\\)</p> Age Log Odds Odds Probability 0 -2 \\(e^{-2}\\) \u2248 0.14 12% 20 8 \\(e^8\\) \u2248 2981 99.97% 30 13 \\(e^{13}\\) \u2248 442,413 \u2248100% <p>Log odds change linearly, but probabilities don't. That's the magic of the logit transform.</p> <p>Why log odds? They provide interpretable coefficients (each \u03b2 is \"change in log-odds per unit\"), unbounded range (the linear predictor can take any value while output stays bounded 0-1), and additive effects (effects of multiple variables sum in log-odds space, unlike in probability space). The transformation connects linear models to probability naturally.</p>"},{"location":"modules/03-classification/#coefficient-interpretation","title":"Coefficient Interpretation","text":"<p>The coefficient \\(\\beta_1\\): Change in log odds for a one-unit increase in \\(x_1\\).</p> <p>The odds ratio \\(e^{\\beta_1}\\): Multiplicative change in odds.</p> <p>Example: - If \\(\\beta_1 = 0.5\\), then \\(e^{0.5} \\approx 1.65\\) - \"Each unit increase in X increases the odds by 65%\"</p> <p>Converting to probability: 1. Calculate log-odds: \\(z = \\beta_0 + \\beta_1 x_1 + ...\\) 2. Apply sigmoid: \\(P(Y=1) = \\frac{1}{1 + e^{-z}}\\)</p>"},{"location":"modules/03-classification/#is-it-regression-or-classification","title":"Is It Regression or Classification?","text":"Aspect Answer Name \"Regression\" (historical reasons) What it models Probability (continuous 0-1) What we use it for Classification (discrete classes) How Apply a threshold to the probability <p>Key insight: Logistic regression IS a regression model (predicts continuous probability), but we USE it for classification by thresholding.</p> <p>Probabilities give crucial flexibility over hard class predictions: threshold flexibility (adjust without retraining when costs change), ranking and prioritization (\"which 100 customers are most likely to churn?\"), confidence communication (P=0.95 vs P=0.55 both classify as positive but represent different confidence), and risk quantification (expected value calculations require probabilities). In business, you almost always benefit from probabilities.</p>"},{"location":"modules/03-classification/#decision-thresholds","title":"Decision Thresholds","text":"<p>The default threshold of 0.5 is often NOT optimal!</p> <p>Example - Fraud Detection: - Cost of missing fraud (false negative): $10,000 - Cost of investigating non-fraud (false positive): $100</p> <p>With asymmetric costs, lower the threshold\u2014catch more fraud, accept more false alarms.</p> <p>Cost-based threshold formula: \\(t^* = \\frac{C_{FP}}{C_{FP} + C_{FN}}\\). For fraud costing $10,000 (FN) and investigation costing $100 (FP): threshold \u2248 100/(100+10000) \u2248 0.01\u2014predict fraud for anyone above 1% probability! This assumes well-calibrated probabilities; verify calibration first. Alternative: Youden's J statistic (maximize TPR-FPR) when costs are unknown.</p> <p>Why does this formula work? It comes from minimizing expected cost. At the threshold, the expected cost of a false positive equals the expected cost of a false negative:</p> \\[P(\\text{actually positive}) \\times C_{FN} = P(\\text{actually negative}) \\times C_{FP}\\] <p>Solving for the probability threshold where you're indifferent between predicting positive or negative gives the formula. When \\(C_{FN}\\) is much larger than \\(C_{FP}\\) (missing fraud is expensive), the threshold drops close to zero\u2014you flag almost anything suspicious. When costs are equal, the threshold is 0.5 (the default).</p> <p>Numerical Example: Cost-Based Threshold Selection</p> <pre><code># Fraud detection costs\ncost_fp = 50   # Investigation cost\ncost_fn = 500  # Missed fraud cost\n\noptimal_threshold = cost_fp / (cost_fp + cost_fn)\nprint(f\"Optimal threshold: {optimal_threshold:.3f}\")\n\n# Effect on predictions\nprobabilities = [0.05, 0.10, 0.20, 0.40, 0.60]\nfor p in probabilities:\n    default = \"Flag\" if p &gt;= 0.5 else \"Clear\"\n    optimal = \"Flag\" if p &gt;= optimal_threshold else \"Clear\"\n    print(f\"P={p:.2f}: Default={default}, Optimal={optimal}\")\n</code></pre> <p>Output: <pre><code>Optimal threshold: 0.091\nP=0.05: Default=Clear, Optimal=Clear\nP=0.10: Default=Clear, Optimal=Flag\nP=0.20: Default=Clear, Optimal=Flag\nP=0.40: Default=Clear, Optimal=Flag\nP=0.60: Default=Flag, Optimal=Flag\n</code></pre></p> <p>Interpretation: When fraud costs 10x more than investigation, we flag any transaction with P(fraud) &gt; 9.1%. A transaction with 10% fraud probability gets flagged\u2014it's worth investigating because the expected loss from missing fraud ($500 \u00d7 0.1 = \\(50) equals the investigation cost (\\)50).</p> <p>Source: <code>slide_computations/module3_examples.py</code> - <code>demo_cost_based_threshold()</code></p> <p>Threshold effects:</p> Lower Threshold Higher Threshold More positive predictions Fewer positive predictions Higher recall Higher precision Lower precision Lower recall Fewer false negatives Fewer false positives"},{"location":"modules/03-classification/#roc-curves-and-auc","title":"ROC Curves and AUC","text":"<p>For each possible threshold: 1. Calculate True Positive Rate: \\(TPR = \\frac{TP}{TP + FN}\\) 2. Calculate False Positive Rate: \\(FPR = \\frac{FP}{FP + TN}\\) 3. Plot the point</p> <p>AUC interpretation: - 0.5 = Random guessing - 1.0 = Perfect separation - 0.8 = \"80% chance that a randomly chosen positive ranks higher than a randomly chosen negative\"</p> <p>Note: AUC \u2260 accuracy. AUC measures ranking ability across all thresholds.</p> <p>Ranking ability means correctly ordering examples by likelihood\u2014higher-risk items get higher scores\u2014even if actual probability values are wrong. This matters for resource allocation (\"call top 100 highest-risk customers\"), campaign targeting (top decile by response rate), and prioritization (fraud investigators review by score). A model with AUC=0.9 and poor calibration is often more useful than AUC=0.6 with perfect calibration\u2014you can recalibrate using Platt scaling or isotonic regression; you can't easily fix ranking ability.</p> <p>The \"threshold dial\" intuition: Imagine a dial you can turn from 0 to 1. As you turn it up (higher threshold), you become more selective\u2014fewer positive predictions, but the ones you make are more confident. As you turn it down (lower threshold), you cast a wider net\u2014catching more true positives but also more false alarms. The ROC curve shows you every position of this dial simultaneously. A good model gives you attractive options along the curve; a poor model forces you to choose between bad options (high FPR or low TPR).</p> <p>Numerical Example: Building an ROC Curve Step by Step</p> <pre><code>import numpy as np\n\n# Small dataset: 4 positive, 6 negative\ntrue_labels = np.array([1, 1, 1, 1, 0, 0, 0, 0, 0, 0])\npred_proba  = np.array([0.95, 0.85, 0.70, 0.45, 0.60, 0.40, 0.35, 0.20, 0.10, 0.05])\n\nthresholds = [1.0, 0.70, 0.45, 0.0]\nfor thresh in thresholds:\n    pred = (pred_proba &gt;= thresh).astype(int)\n    tp = np.sum((pred == 1) &amp; (true_labels == 1))\n    fp = np.sum((pred == 1) &amp; (true_labels == 0))\n    tpr = tp / 4  # 4 actual positives\n    fpr = fp / 6  # 6 actual negatives\n    print(f\"Threshold {thresh:.2f}: TPR={tpr:.2f}, FPR={fpr:.2f}\")\n</code></pre> <p>Output: <pre><code>Threshold 1.00: TPR=0.00, FPR=0.00\nThreshold 0.70: TPR=0.75, FPR=0.00\nThreshold 0.45: TPR=1.00, FPR=0.17\nThreshold 0.00: TPR=1.00, FPR=1.00\n</code></pre></p> <p>Interpretation: The ROC curve plots these (FPR, TPR) points as threshold varies. At threshold 0.70, we achieve TPR=75% with zero false positives\u2014an excellent operating point. Lowering to 0.45 catches all positives but introduces one false positive (FPR=17%). The AUC measures the area under this curve; higher is better.</p> <p>Source: <code>slide_computations/module3_examples.py</code> - <code>demo_roc_curve_construction()</code></p> <p>Choosing optimal threshold: - Youden's J statistic: Maximize (TPR - FPR) - Cost-based: Minimize expected cost given FP/FN costs - Precision-Recall trade-off: Use PR curve for imbalanced data</p> <pre><code>from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, auc\n\nlog_reg = LogisticRegression(penalty='l2', C=1.0, random_state=42)\nlog_reg.fit(X_train, y_train)\n\n# Get probabilities\ny_proba = log_reg.predict_proba(X_test)[:, 1]\n\n# ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_proba)\nroc_auc = auc(fpr, tpr)\n\n# Optimal threshold (Youden's J)\noptimal_idx = (tpr - fpr).argmax()\noptimal_threshold = thresholds[optimal_idx]\n\n# Interpret coefficients as odds ratios\nfor feature, coef in zip(feature_names, log_reg.coef_[0]):\n    odds_ratio = np.exp(coef)\n    print(f\"{feature}: odds ratio = {odds_ratio:.3f}\")\n</code></pre>"},{"location":"modules/03-classification/#common-misconceptions","title":"Common Misconceptions","text":"Misconception Reality \"Logistic regression outputs are well-calibrated probabilities\" Outputs may need calibration (Platt scaling, isotonic regression) for reliable probability estimates. \"Higher AUC always means better model\" AUC ignores calibration and threshold choice. A model with lower AUC but better calibration might be preferable. \"Logistic regression requires linear relationships\" It requires linearity in LOG ODDS, not probability. Add polynomial terms for non-linear relationships. \"Logistic regression can't handle multiple classes\" Multinomial logistic regression extends to multiple classes (one-vs-rest or softmax)."},{"location":"modules/03-classification/#32-decision-trees-cart","title":"3.2 Decision Trees (CART)","text":""},{"location":"modules/03-classification/#three-components-decision-trees","title":"Three Components: Decision Trees","text":"Component Decision Trees Decision Model Tree of if-then rules \u2014 follow branches based on feature thresholds Quality Measure Gini impurity or entropy \u2014 measures class mixture in nodes Update Method Greedy recursive splitting \u2014 find best split at each node <p>Key difference: We're not doing gradient descent. Trees use a greedy algorithm that builds one split at a time.</p>"},{"location":"modules/03-classification/#decision-tree-intuition","title":"Decision Tree Intuition","text":"<p>Imagine you're a loan officer: - First: Is income &gt; $50,000?   - Yes \u2192 Check debt-to-income ratio   - No \u2192 Check employment history...</p> <p>Decision trees formalize this intuitive process. They automatically learn which questions to ask, in what order, and what thresholds to use.</p> <p>The tree picks the feature and threshold that best separates classes (maximally reduces impurity). This is a greedy algorithm\u2014locally best splits without looking ahead. The first feature is often important but not always \"most important\": a feature might matter most after controlling for another, or correlated features might be interchanged. Feature importance scores (aggregating across all nodes) are more reliable than just the root split.</p>"},{"location":"modules/03-classification/#splitting-criteria","title":"Splitting Criteria","text":"<p>Gini Impurity (scikit-learn default):</p> \\[Gini = 1 - \\sum_{i=1}^{C} p_i^2\\] <p>Where \\(p_i\\) is the proportion of class \\(i\\) in the node.</p> <ul> <li>Gini = 0: Pure node (all same class)</li> <li>Gini = 0.5: Maximum impurity for binary (50-50)</li> </ul> <p>Gini as \"certainty of a random guess\": If you randomly pick an example from a node and randomly assign it a class based on the node's distribution, Gini measures how often you'd be wrong. For a pure node (100% class A), you'd always guess A and always be right\u2014Gini = 0. For a 50-50 node, you'd be wrong half the time on average\u2014Gini = 0.5 (maximum uncertainty). The tree seeks splits that create nodes where a random guess would more often be correct.</p> <p>Hand-calculating Gini: For a node with 60% class A, 40% class B:</p> \\[Gini = 1 - (0.6^2 + 0.4^2) = 1 - (0.36 + 0.16) = 1 - 0.52 = 0.48\\] <p>This is close to maximum impurity (0.5), indicating the node is nearly evenly split.</p> <p>Numerical Example: Evaluating a Split with Gini</p> <pre><code>def gini(class_proportions):\n    return 1 - sum(p**2 for p in class_proportions)\n\n# Parent node: 100 samples, 50-50 split\nparent_gini = gini([0.5, 0.5])\nprint(f\"Parent Gini: {parent_gini:.4f}\")\n\n# After split: Left (60 samples, 80-20), Right (40 samples, 10-90)\nleft_gini = gini([0.8, 0.2])\nright_gini = gini([0.1, 0.9])\nweighted_gini = (60 * left_gini + 40 * right_gini) / 100\ninfo_gain = parent_gini - weighted_gini\n\nprint(f\"Left child Gini: {left_gini:.4f}\")\nprint(f\"Right child Gini: {right_gini:.4f}\")\nprint(f\"Weighted child Gini: {weighted_gini:.4f}\")\nprint(f\"Information gain: {info_gain:.4f}\")\n</code></pre> <p>Output: <pre><code>Parent Gini: 0.5000\nLeft child Gini: 0.3200\nRight child Gini: 0.1800\nWeighted child Gini: 0.2640\nInformation gain: 0.2360\n</code></pre></p> <p>Interpretation: The parent node has maximum impurity (0.5). After the split, both children are more \"pure\"\u2014the left child is 80% one class (Gini=0.32), the right is 90% the other (Gini=0.18). The weighted average (0.264) is much lower than the parent (0.5), yielding high information gain (0.236). The tree picks the split that maximizes this gain.</p> <p>Source: <code>slide_computations/module3_examples.py</code> - <code>demo_gini_impurity_calculation()</code></p> <p>Entropy:</p> \\[Entropy = -\\sum_{i=1}^{C} p_i \\log_2(p_i)\\] <p>Usually produces similar results. Gini is slightly faster (no logarithms).</p> <p>In practice, Gini vs entropy rarely matters. Entropy penalizes near-equal splits slightly more; with many classes, Gini can favor isolating one class while entropy prefers balanced information gain. Default to Gini (slightly faster); if hyperparameter tuning, include criterion and let cross-validation decide.</p>"},{"location":"modules/03-classification/#the-scikit-learn-api-pattern","title":"The scikit-learn API Pattern","text":"<p>This pattern is consistent across almost ALL scikit-learn models:</p> <pre><code># 1. Instantiate\nmodel = DecisionTreeClassifier(max_depth=5)\n\n# 2. Fit\nmodel.fit(X_train, y_train)\n\n# 3. Predict\npredictions = model.predict(X_test)\nprobabilities = model.predict_proba(X_test)\n</code></pre>"},{"location":"modules/03-classification/#decision-boundaries","title":"Decision Boundaries","text":"<p>Trees create rectangular decision regions: - Each split creates a horizontal or vertical line - Deep trees create many small rectangles - Different from logistic regression's smooth boundary</p>"},{"location":"modules/03-classification/#demonstrating-overfitting","title":"Demonstrating Overfitting","text":"<p>Deep Tree (no limit): - Train accuracy: 100% - Test accuracy: 75% - Hundreds of nodes</p> <p>Shallow Tree (depth=3): - Train accuracy: 85% - Test accuracy: 82% - ~15 nodes</p> <p>Key insight: Deep trees memorize training data including noise. 100% training accuracy almost certainly means overfitting.</p> <p>Numerical Example: Decision Tree Overfitting</p> <pre><code>from sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Data with 10% label noise\nX, y = make_classification(\n    n_samples=300, n_features=10, n_informative=5,\n    flip_y=0.1, random_state=42\n)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nfor depth in [3, 5, 10, None]:\n    tree = DecisionTreeClassifier(max_depth=depth, random_state=42)\n    tree.fit(X_train, y_train)\n    print(f\"Depth {str(depth):&gt;4}: Train={tree.score(X_train, y_train):.1%}, \"\n          f\"Test={tree.score(X_test, y_test):.1%}, Leaves={tree.get_n_leaves()}\")\n</code></pre> <p>Output: <pre><code>Depth    3: Train=81.9%, Test=71.1%, Leaves=7\nDepth    5: Train=92.4%, Test=77.8%, Leaves=16\nDepth   10: Train=100.0%, Test=80.0%, Leaves=29\nDepth None: Train=100.0%, Test=80.0%, Leaves=29\n</code></pre></p> <p>Interpretation: The unlimited tree achieves 100% training accuracy\u2014but the data has 10% label noise, so perfect training accuracy means it memorized the noise! The train-test gap (20%) signals overfitting. Shallow trees (depth 3-5) have lower training accuracy but smaller gaps, indicating better generalization.</p> <p>Source: <code>slide_computations/module3_examples.py</code> - <code>demo_tree_overfitting()</code></p> <p>100% training accuracy is occasionally okay: perfectly separable data (predicting even/odd from last digit), very small clean datasets, or memorization tasks. Verify by checking test accuracy (also very high?), the train-test gap (small vs large?), complexity (10 leaves for 10,000 samples = simple rules; 5,000 leaves = memorized), and cross-validation consistency. The heuristic remains useful: 100% training accuracy should trigger suspicion.</p>"},{"location":"modules/03-classification/#pruning-strategies","title":"Pruning Strategies","text":"<p>Pre-pruning (early stopping): - <code>max_depth</code>: Maximum tree depth - <code>min_samples_split</code>: Minimum samples to split a node - <code>min_samples_leaf</code>: Minimum samples in a leaf</p> <p>Post-pruning: - Grow full tree, then prune back - Use <code>ccp_alpha</code> parameter - Higher alpha = more pruning</p> <p>Recommendation: Start with pre-pruning. Set <code>max_depth=5</code> as starting point, use cross-validation to optimize.</p> <pre><code>from sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.model_selection import cross_val_score\n\ntree = DecisionTreeClassifier(\n    criterion='gini',\n    max_depth=5,\n    min_samples_split=10,\n    min_samples_leaf=5,\n    random_state=42\n)\ntree.fit(X_train, y_train)\n\n# Check for overfitting\ntrain_acc = tree.score(X_train, y_train)\ntest_acc = tree.score(X_test, y_test)\nprint(f\"Train: {train_acc:.3f}, Test: {test_acc:.3f}\")\n\n# Cross-validation for depth selection\nfor depth in range(1, 15):\n    tree_cv = DecisionTreeClassifier(max_depth=depth, random_state=42)\n    scores = cross_val_score(tree_cv, X_train, y_train, cv=5)\n    print(f\"Depth {depth}: {scores.mean():.3f} (+/- {scores.std():.3f})\")\n</code></pre>"},{"location":"modules/03-classification/#feature-importance","title":"Feature Importance","text":"\\[Importance = \\sum_{nodes} (impurity\\ reduction \\times samples)\\] <p>Caveats: - Importance is relative (sums to 1) - Correlated features split importance between them - Doesn't indicate direction of effect or causation</p> <p>To understand importance with correlated features: use domain knowledge (which is more causal?), remove one and retrain (does importance transfer?), or use permutation importance (shuffles independently). For prediction, keeping both adds complexity without benefit. For interpretation, report both but note correlation. Consider reporting \"this cluster of correlated features is important\" rather than attributing to one.</p> <p>The \"weighted vote\" intuition: Think of feature importance as a weighted vote. Every time a feature is used to split a node, it gets \"votes\" equal to (impurity reduction \u00d7 number of samples affected). A feature used at the root affects all samples\u2014many votes. A feature used deep in the tree affects few samples\u2014fewer votes. The final importance is each feature's total votes divided by all votes. This explains why the root feature often has high importance even if it's not the \"most important\" conceptually\u2014it affects the most samples.</p> <p>Numerical Example: Feature Importance</p> <pre><code>import numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\n\nnp.random.seed(42)\nn = 500\nx_strong = np.random.randn(n)  # Strong predictor (coef 2.0)\nx_medium = np.random.randn(n)  # Medium predictor (coef 1.0)\nx_weak = np.random.randn(n)    # Weak predictor (coef 0.3)\nx_noise = np.random.randn(n)   # Pure noise\n\ny = (2.0*x_strong + 1.0*x_medium + 0.3*x_weak + np.random.randn(n)*0.5 &gt; 0).astype(int)\nX = np.column_stack([x_strong, x_medium, x_weak, x_noise])\n\ntree = DecisionTreeClassifier(max_depth=5, random_state=42)\ntree.fit(X, y)\n\nnames = ['strong', 'medium', 'weak', 'noise']\nfor name, imp in zip(names, tree.feature_importances_):\n    print(f\"{name:&gt;8}: {imp:.3f}\")\n</code></pre> <p>Output: <pre><code>  strong: 0.661\n  medium: 0.280\n    weak: 0.046\n   noise: 0.013\n</code></pre></p> <p>Interpretation: The tree correctly identifies the strong predictor as most important (66%), followed by medium (28%). The weak predictor and noise have minimal importance. Note: importance is relative (sums to 1) and doesn't indicate causation\u2014just predictive value in this tree.</p> <p>Source: <code>slide_computations/module3_examples.py</code> - <code>demo_feature_importance()</code></p>"},{"location":"modules/03-classification/#why-decision-trees-are-popular","title":"Why Decision Trees Are Popular","text":"<ol> <li>Explainable: Show decision rules to stakeholders</li> <li>No preprocessing: Handle different scales, categorical variables, missing values</li> <li>Non-linear: Capture complex relationships automatically</li> <li>Visual: Tree diagrams are intuitive for non-technical audiences</li> </ol>"},{"location":"modules/03-classification/#common-misconceptions_1","title":"Common Misconceptions","text":"Misconception Reality \"Deeper trees are always better\" Deeper trees overfit. Find the sweet spot via cross-validation. \"Decision trees require feature scaling\" Trees are scale-invariant! One of their advantages. \"Feature importance = causal importance\" Importance only shows predictive power, not causation. \"Trees can't capture interactions\" Trees naturally capture interactions through hierarchical structure."},{"location":"modules/03-classification/#33-handling-imbalanced-data","title":"3.3 Handling Imbalanced Data","text":""},{"location":"modules/03-classification/#why-accuracy-is-misleading","title":"Why Accuracy is Misleading","text":"<p>Fraud detection: - 99.9% of transactions are legitimate - 0.1% are fraudulent</p> <p>A model that predicts \"legitimate\" for EVERYTHING: - Accuracy: 99.9% - Catches zero fraud!</p> <p>Accuracy is useless for imbalanced classes.</p> <p>When is it \"imbalanced\"? 60/40 is typically fine; 70/30 is mild; 80/20 starts requiring attention; 90/10 likely needs specialized techniques; 95/5 definitely needs SMOTE, class weights, or threshold adjustment. But it's not just about ratio\u2014absolute numbers matter (90/10 with 10,000 minority samples is fine; with 100 is problematic). The practical test: does your model learn anything about the minority class? If accuracy comes from ignoring the minority entirely, you have a problem.</p>"},{"location":"modules/03-classification/#better-metrics","title":"Better Metrics","text":"<p>Precision: Of those we flagged as positive, how many actually were?</p> \\[Precision = \\frac{TP}{TP + FP}\\] <p>Recall: Of actual positives, how many did we catch?</p> \\[Recall = \\frac{TP}{TP + FN}\\] <p>F1 Score: Harmonic mean balancing both</p> \\[F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}\\] <p>Why harmonic mean? It punishes extreme imbalance: - Precision = 100%, Recall = 1% \u2192 F1 = 2% - Precision = 50%, Recall = 50% \u2192 F1 = 50%</p> <p>The harmonic mean (as opposed to arithmetic mean) has a special property: it's dominated by the smaller value. If you have P=100% and R=1%, the arithmetic mean would be 50.5%\u2014making it look decent. But the harmonic mean is 2%\u2014revealing that one metric is terrible. This is what we want! A model that achieves high precision by predicting almost nothing (low recall) shouldn't score well. The harmonic mean enforces balance: both metrics must be reasonable for F1 to be high.</p> <p>The precision-recall \"fishing net\" analogy: Imagine you're fishing for a specific type of fish (positives) in a lake (your data). Precision asks: \"Of the fish in your net, what fraction are the type you wanted?\" Recall asks: \"Of all the target fish in the lake, what fraction did you catch?\" A tight net (high threshold) catches few fish but mostly the right kind\u2014high precision, low recall. A wide net (low threshold) catches more target fish but also lots of other fish\u2014high recall, low precision. You can't optimize both without a better model (or more target fish).</p> <p>Numerical Example: Precision-Recall at Different Thresholds</p> <pre><code>from sklearn.datasets import make_classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\n# Imbalanced dataset: 90% negative, 10% positive\nX, y = make_classification(n_samples=1000, weights=[0.9, 0.1], random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nmodel = LogisticRegression(random_state=42, max_iter=1000)\nmodel.fit(X_train, y_train)\ny_proba = model.predict_proba(X_test)[:, 1]\n\nfor thresh in [0.1, 0.3, 0.5, 0.7]:\n    y_pred = (y_proba &gt;= thresh).astype(int)\n    prec = precision_score(y_test, y_pred, zero_division=0)\n    rec = recall_score(y_test, y_pred, zero_division=0)\n    print(f\"Threshold {thresh}: Precision={prec:.0%}, Recall={rec:.0%}\")\n</code></pre> <p>Output: <pre><code>Threshold 0.1: Precision=29%, Recall=69%\nThreshold 0.3: Precision=70%, Recall=44%\nThreshold 0.5: Precision=86%, Recall=19%\nThreshold 0.7: Precision=100%, Recall=9%\n</code></pre></p> <p>Interpretation: As threshold increases, precision rises (fewer false alarms) but recall falls (more missed positives). At threshold 0.1, we catch 69% of positives but 71% of our \"positive\" predictions are wrong. At 0.7, we're always right when we predict positive, but we miss 91% of actual positives. Choose based on business costs!</p> <p>Source: <code>slide_computations/module3_examples.py</code> - <code>demo_precision_recall_threshold()</code></p>"},{"location":"modules/03-classification/#the-precision-recall-trade-off","title":"The Precision-Recall Trade-off","text":"<p>Usually you can't maximize both: - High precision \u2192 few false alarms but miss some positives - High recall \u2192 catch most positives but more false alarms</p> <p>Business context determines priority: - High precision: Email marketing (don't waste budget) - High recall: Medical screening (don't miss sick patients)</p>"},{"location":"modules/03-classification/#resampling-smote","title":"Resampling: SMOTE","text":"<p>SMOTE (Synthetic Minority Over-sampling Technique): - Creates synthetic minority examples - Interpolates between existing minority points - Better than simple duplication</p> <pre><code>from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n</code></pre> <p>How SMOTE \"fills in the gaps\": SMOTE doesn't just copy existing minority examples (which would teach the model to memorize specific points). Instead, it creates synthetic examples by: (1) picking a minority example, (2) finding its k nearest minority neighbors, (3) drawing a line to one neighbor, and (4) placing a new point somewhere along that line. This \"fills in\" the feature space between existing minority examples, helping the model learn the general region where minority examples live rather than just memorizing specific cases. Think of it as sketching between the dots to reveal the underlying shape.</p> <p>Important: Only apply SMOTE to training data, never test data! The test set must reflect real-world conditions\u2014your deployed model will face the true class distribution. SMOTE is a training trick to help the model learn about the minority class, not a data transformation. The correct workflow: (1) Split data first. (2) Apply SMOTE only to training set. (3) Evaluate on original, imbalanced test set. (4) Use appropriate metrics (F1, precision, recall) that work for imbalanced data.</p>"},{"location":"modules/03-classification/#class-weights","title":"Class Weights","text":"<p>Many algorithms have built-in support:</p> <pre><code>model = LogisticRegression(class_weight='balanced')\nmodel = DecisionTreeClassifier(class_weight='balanced')\n</code></pre> <p>Effect: Increases penalty for misclassifying minority class. Often simpler than resampling.</p> <p>Numerical Example: Effect of Class Weights</p> <pre><code>from sklearn.datasets import make_classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import recall_score, f1_score\n\n# Severely imbalanced: 95% negative, 5% positive\nX, y = make_classification(n_samples=1000, weights=[0.95, 0.05], random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Without class weights\nmodel_uw = LogisticRegression(class_weight=None, random_state=42, max_iter=1000)\nmodel_uw.fit(X_train, y_train)\npred_uw = model_uw.predict(X_test)\n\n# With balanced class weights\nmodel_w = LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000)\nmodel_w.fit(X_train, y_train)\npred_w = model_w.predict(X_test)\n\nprint(f\"No weights:  Recall={recall_score(y_test, pred_uw):.0%}, F1={f1_score(y_test, pred_uw):.0%}\")\nprint(f\"Balanced:    Recall={recall_score(y_test, pred_w):.0%}, F1={f1_score(y_test, pred_w):.0%}\")\n</code></pre> <p>Output: <pre><code>No weights:  Recall=0%, F1=0%\nBalanced:    Recall=78%, F1=19%\n</code></pre></p> <p>Interpretation: Without weights, the model learns to predict \"negative\" for everything\u2014achieving 96% accuracy by ignoring the minority class entirely (0% recall). With balanced weights, the model actually tries to find positives, achieving 78% recall. Accuracy drops because of more false positives, but F1 improves because we're actually solving the problem!</p> <p>Source: <code>slide_computations/module3_examples.py</code> - <code>demo_class_weights()</code></p>"},{"location":"modules/03-classification/#threshold-adjustment","title":"Threshold Adjustment","text":"<pre><code>y_proba = model.predict_proba(X_test)[:, 1]\nthreshold = 0.3  # Instead of 0.5\ny_pred = (y_proba &gt;= threshold).astype(int)\n</code></pre> <p>Lower threshold \u2192 predict positive more often \u2192 higher recall, lower precision.</p>"},{"location":"modules/03-classification/#business-context-examples","title":"Business Context Examples","text":"Domain Priority Reason Fraud Detection High recall Cost of fraud &gt;&gt; investigation cost Medical Diagnosis High recall Don't miss sick patients Churn Prediction Balance Retention cost vs customer value Manufacturing QC Depends Defect severity vs discard cost"},{"location":"modules/03-classification/#common-misconceptions_2","title":"Common Misconceptions","text":"Misconception Reality \"Always balance classes to 50-50\" Optimal ratio depends on the problem. Original distribution may be meaningful. \"SMOTE is always better than oversampling\" SMOTE can create unrealistic synthetic examples. Test both. \"Class weights and resampling do the same thing\" Similar effect but different mechanisms. Results can differ. \"Imbalanced data is always a problem\" If minority class is well-separated, imbalance may not hurt. Always check metrics."},{"location":"modules/03-classification/#34-hyperparameter-optimization","title":"3.4 Hyperparameter Optimization","text":""},{"location":"modules/03-classification/#parameters-vs-hyperparameters","title":"Parameters vs Hyperparameters","text":"Parameters Hyperparameters Learned during training Set before training Model learns via .fit() You choose before .fit() Example: Coefficients Example: Regularization strength Example: Split points Example: Max tree depth <p>Hyperparameters control HOW the model learns.</p> <p>Finding hyperparameters: Use official documentation (search \"sklearn DecisionTreeClassifier\"), in-code exploration (<code>model.get_params()</code>, <code>help(DecisionTreeClassifier)</code>), or IDE autocomplete. Not all hyperparameters matter equally\u2014most algorithms have 3-5 \"important\" ones: for decision trees, focus on <code>max_depth</code>, <code>min_samples_split</code>, <code>min_samples_leaf</code>; for Random Forests add <code>n_estimators</code>, <code>max_features</code>; for XGBoost: <code>learning_rate</code>, <code>max_depth</code>, <code>n_estimators</code>, <code>subsample</code>.</p>"},{"location":"modules/03-classification/#grid-search","title":"Grid Search","text":"<p>Try every combination in a predefined grid:</p> <pre><code>from sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'max_depth': [3, 5, 7, 10],\n    'min_samples_split': [2, 5, 10],\n}\n# Total: 4 \u00d7 3 = 12 combinations\n\ngrid_search = GridSearchCV(\n    estimator=DecisionTreeClassifier(random_state=42),\n    param_grid=param_grid,\n    cv=5,\n    scoring='f1'\n)\ngrid_search.fit(X_train, y_train)\nprint(f\"Best params: {grid_search.best_params_}\")\n</code></pre> <p>Pros: Exhaustive, reproducible Cons: Exponential growth, wastes time on bad regions</p>"},{"location":"modules/03-classification/#random-search","title":"Random Search","text":"<p>Sample random combinations from distributions:</p> <pre><code>from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\nparam_distributions = {\n    'max_depth': randint(2, 20),\n    'min_samples_split': randint(2, 50),\n}\n\nrandom_search = RandomizedSearchCV(\n    estimator=DecisionTreeClassifier(random_state=42),\n    param_distributions=param_distributions,\n    n_iter=50,\n    cv=5,\n    scoring='f1',\n    random_state=42\n)\nrandom_search.fit(X_train, y_train)\n</code></pre>"},{"location":"modules/03-classification/#why-random-often-beats-grid","title":"Why Random Often Beats Grid","text":"<p>Key insight (Bergstra &amp; Bengio, 2012): - Not all hyperparameters are equally important - Grid search wastes trials on unimportant parameters - Random search explores more values of what matters</p> <p>In practice, random search often beats grid search with the same computational budget.</p> <p>Why random beats grid\u2014the geometric intuition: Imagine a 2D hyperparameter space where only one dimension matters (common in practice). Grid search with 9 trials might try 3 values per dimension, giving you only 3 unique values of the important parameter. Random search with 9 trials gives you 9 unique values of the important parameter! When you don't know which parameters matter most (and you usually don't), random search automatically allocates more trials to exploring variation in every dimension. Grid search wastes trials exploring combinations of unimportant parameters.</p> <p>Standard ranges for common hyperparameters: <code>max_depth</code>: 2-20 for trees; <code>n_estimators</code>: 50-500 for forests/boosting; <code>learning_rate</code>: 0.001-0.3 for boosting; <code>min_samples_split</code>: 2-50; <code>C</code> (regularization): 0.001-100 (log scale). If the best value is at the edge of your range, extend that direction. Start with wide, log-spaced ranges, do a coarse search (10 values), then refine in the promising region.</p>"},{"location":"modules/03-classification/#bayesian-optimization-optuna","title":"Bayesian Optimization (optuna)","text":"<p>Use past results to guide future trials:</p> <pre><code>import optuna\n\ndef objective(trial):\n    params = {\n        'max_depth': trial.suggest_int('max_depth', 2, 20),\n        'min_samples_split': trial.suggest_int('min_samples_split', 2, 50),\n    }\n    model = DecisionTreeClassifier(**params, random_state=42)\n    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='f1')\n    return scores.mean()\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=50)\nprint(f\"Best params: {study.best_params}\")\n</code></pre> <p>More efficient than random search\u2014learns from previous trials.</p>"},{"location":"modules/03-classification/#never-use-test-set-for-tuning","title":"Never Use Test Set for Tuning!","text":"<p>Correct workflow: 1. Split into train/test 2. Use cross-validation on training set for tuning 3. Select best hyperparameters via CV score 4. Retrain on full training set 5. Evaluate once on test set</p> <p>If you tune on test set, your estimate is no longer unbiased.</p> <p>After tuning: Retrain on all training data with the best hyperparameters. Cross-validation models were trained on only (K-1)/K of your data. Retraining on 100% gives the model more examples. <code>GridSearchCV</code> does this automatically\u2014<code>grid_search.best_estimator_</code> is already retrained on the full training set.</p>"},{"location":"modules/03-classification/#common-misconceptions_3","title":"Common Misconceptions","text":"Misconception Reality \"More hyperparameter tuning always helps\" Diminishing returns. 50-100 trials often enough. Risk overfitting to validation data. \"Grid search is more thorough\" Grid is exhaustive only for values you specify. Random can find values between grid points. \"Best hyperparameters are universal\" Optimal hyperparameters depend on your specific dataset. \"Use test set to choose hyperparameters\" Never! Use cross-validation on training data."},{"location":"modules/03-classification/#reflection-questions","title":"Reflection Questions","text":"<ol> <li> <p>A model predicts P(churn) = 0.6 for a customer. What does this actually mean? How confident should we be?</p> </li> <li> <p>Why might you choose a threshold other than 0.5? Give scenarios for very low and very high thresholds.</p> </li> <li> <p>A logistic regression coefficient for 'number of support tickets' is 0.3. How would you explain this to a stakeholder?</p> </li> <li> <p>You build a decision tree with 100% training accuracy. Is this good or bad? What would you do next?</p> </li> <li> <p>In fraud detection with 0.1% fraud rate, a model achieves 99.9% accuracy. What's wrong with celebrating this?</p> </li> <li> <p>When would you prefer high precision over high recall? Give a business example.</p> </li> <li> <p>Why might random search find better hyperparameters than grid search with the same budget?</p> </li> </ol>"},{"location":"modules/03-classification/#practice-problems","title":"Practice Problems","text":"<ol> <li> <p>Calculate odds and log-odds for P = 0.8</p> </li> <li> <p>Given coefficients \u03b2\u2080 = -2, \u03b2\u2081 = 0.5, \u03b2\u2082 = -0.3, calculate P(Y=1) when x\u2081 = 4, x\u2082 = 2</p> </li> <li> <p>Draw what a decision tree boundary would look like for 2D data with 2 splits</p> </li> <li> <p>Given a 95% legitimate / 5% fraud dataset: if we predict all legitimate, what's accuracy? Precision for fraud? Recall for fraud?</p> </li> <li> <p>Choose between precision and recall priority for: (a) spam filter, (b) cancer screening, (c) loan approval</p> </li> </ol>"},{"location":"modules/03-classification/#chapter-summary","title":"Chapter Summary","text":"<p>Six key takeaways from Module 3:</p> <ol> <li> <p>Logistic regression outputs probabilities via sigmoid; threshold for classification</p> </li> <li> <p>Odds ratios (exponentiate coefficients) translate to business-friendly interpretation</p> </li> <li> <p>Decision trees are intuitive but overfit easily\u2014use pruning</p> </li> <li> <p>Accuracy is misleading for imbalanced data\u2014use precision/recall/F1</p> </li> <li> <p>Handle imbalance with SMOTE, class weights, or threshold adjustment</p> </li> <li> <p>Hyperparameter tuning via cross-validation, never on test set</p> </li> </ol>"},{"location":"modules/03-classification/#whats-next","title":"What's Next","text":"<p>In Module 4, we tackle Ensemble Methods: - Random Forests (ensembles of decision trees) - Gradient Boosting (XGBoost, LightGBM) - Why combining weak learners creates strong models</p> <p>Understanding decision trees is essential\u2014Random Forests take everything we learned about trees and combine many of them for better performance.</p>"},{"location":"modules/04-ensemble-methods/","title":"Module 4: Ensemble Methods","text":""},{"location":"modules/04-ensemble-methods/#introduction","title":"Introduction","text":"<p>In Module 3, we learned about decision trees\u2014intuitive classifiers that are easy to interpret but prone to overfitting. Deep trees memorize training data; shallow trees underfit.</p> <p>This module answers a natural question: What if we could get the benefits of deep trees without the overfitting?</p> <p>The answer is ensemble methods. Instead of training one model, we train many models and combine their predictions. This simple idea\u2014the wisdom of crowds\u2014turns out to be one of the most powerful techniques in machine learning.</p> <p>By the end of this module, you'll understand two major ensemble paradigms: bagging (where Random Forests come from) and boosting (where XGBoost comes from). These methods dominate tabular data competitions and are workhorses in industry.</p>"},{"location":"modules/04-ensemble-methods/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, you should be able to:</p> <ol> <li>Explain the intuition behind ensemble methods and why combining models outperforms individuals</li> <li>Implement bagging (Random Forests) and boosting (XGBoost)</li> <li>Interpret feature importance for business stakeholders</li> <li>Select appropriate ensemble strategies based on problem characteristics</li> </ol>"},{"location":"modules/04-ensemble-methods/#41-ensemble-learning-concepts","title":"4.1 Ensemble Learning Concepts","text":""},{"location":"modules/04-ensemble-methods/#the-wisdom-of-crowds","title":"The Wisdom of Crowds","text":"<p>Galton's Ox Experiment (1907):</p> <p>At a county fair, 787 people tried to guess the weight of an ox. Individual guesses varied wildly\u2014some way too high, some way too low.</p> <ul> <li>Median of all guesses: 1,207 lbs</li> <li>Actual weight: 1,198 lbs (&lt; 1% error!)</li> </ul> <p>How can a crowd of non-experts outperform individuals?</p> <p>Key insight: Errors cancel out when they're uncorrelated. Some people guessed too high, some too low. The errors went in different directions. When you average, errors cancel and the true signal remains.</p> <p>This is exactly the principle behind ensemble machine learning.</p> <p>Correlation matters: Ensembles work best with uncorrelated errors, but help even with partially correlated errors. If individual models have variance \u03c3\u00b2 and correlation \u03c1 between errors, ensemble variance is \u03c1\u03c3\u00b2 + (1-\u03c1)\u03c3\u00b2/n. With perfect independence (\u03c1=0), variance drops as 1/n. With perfect correlation (\u03c1=1), averaging doesn't help. In practice, even 50% correlation provides substantial benefit.</p> <p>Concrete example: Imagine 5 models, each with 70% accuracy on a binary prediction. If each model makes independent errors: - Probability all 5 are wrong on the same example: 0.3\u2075 = 0.24% - Majority vote is wrong only when 3+ models are wrong - The ensemble achieves ~84% accuracy\u2014significantly better than any individual</p> <p>But if all models make the same mistakes (\u03c1=1), the ensemble is still just 70% accurate. Diversity is the key ingredient.</p> <p>Numerical Example: Ensemble Variance and Correlation</p> <pre><code>import numpy as np\n\n# Parameters: 10 models, each with variance \u03c3\u00b2 = 100\nindividual_variance = 100\nn_models = 10\n\n# Ensemble variance formula: Var = \u03c1\u03c3\u00b2 + (1-\u03c1)\u03c3\u00b2/n\nfor rho in [0.0, 0.25, 0.5, 0.75, 1.0]:\n    ensemble_var = (\n        rho * individual_variance\n        + (1 - rho) * individual_variance / n_models\n    )\n    reduction = (1 - ensemble_var / individual_variance) * 100\n    print(f\"\u03c1={rho:.2f}: Var={ensemble_var:.1f}, Reduction={reduction:.0f}%\")\n</code></pre> <p>Output: <pre><code>\u03c1=0.00: Var=10.0, Reduction=90%\n\u03c1=0.25: Var=32.5, Reduction=68%\n\u03c1=0.50: Var=55.0, Reduction=45%\n\u03c1=0.75: Var=77.5, Reduction=22%\n\u03c1=1.00: Var=100.0, Reduction=0%\n</code></pre></p> <p>Interpretation: With 10 independent models (\u03c1=0), variance drops by 90%. Even with moderate correlation (\u03c1=0.5), you still get 45% reduction. This is why Random Forest's feature sampling matters\u2014it reduces \u03c1 between trees.</p> <p>Source: <code>slide_computations/module4_examples.py</code> - <code>demo_ensemble_variance_correlation()</code></p>"},{"location":"modules/04-ensemble-methods/#how-ensembles-improve-predictions","title":"How Ensembles Improve Predictions","text":"<p>Variance Reduction (Bagging): - Single decision trees are high-variance estimators - Small changes in training data \u2192 very different trees - Averaging multiple trees reduces instability - Mathematically: \\(Var(average) = Var(individual) / n\\) when predictions are uncorrelated</p> <p>Bias Reduction (Boosting): - Each new model focuses on errors of previous models - The ensemble gradually learns patterns individual weak learners missed - Sequential learning reduces systematic error</p>"},{"location":"modules/04-ensemble-methods/#model-diversity-is-critical","title":"Model Diversity is Critical","text":"<p>Ensembles only help if the models are different!</p> <p>If all models make the same mistakes, averaging doesn't help. Think: if you ask 787 people the same leading question and they all guess the same wrong answer, the median is still wrong.</p> <p>How ensemble methods create diversity: - Random Forests: Random sampling of data AND features - Boosting: Sequential focus on different examples - Different algorithms: Different inductive biases (heterogeneous ensembles)</p> <p>Heterogeneous ensembles combine completely different algorithms (neural network + decision tree + logistic regression). Different algorithms have different inductive biases, making them unlikely to make the same mistakes. The Netflix Prize winning solution combined 107 different models.</p>"},{"location":"modules/04-ensemble-methods/#common-misconceptions","title":"Common Misconceptions","text":"Misconception Reality \"More models always means better results\" Diminishing returns kick in quickly. 1 \u2192 10 trees helps a lot; 100 \u2192 1000 helps little. \"Ensembles are always better than single models\" For simple problems or when interpretability is paramount, single models may be preferable. \"You need sophisticated models in your ensemble\" Ensembles of simple models (shallow trees, stumps) can be remarkably effective."},{"location":"modules/04-ensemble-methods/#42-bagging-methods","title":"4.2 Bagging Methods","text":""},{"location":"modules/04-ensemble-methods/#three-components-random-forest","title":"Three Components: Random Forest","text":"Component Random Forest Decision Model Ensemble of decision trees \u2014 each tree votes, majority wins Quality Measure Gini/entropy for individual trees; OOB error for ensemble Update Method Independent parallel training \u2014 no iteration between trees <p>Key insight: Random Forest doesn't \"update\" traditionally. Each tree trains independently on a bootstrap sample. Learning happens through aggregation\u2014the wisdom of crowds.</p>"},{"location":"modules/04-ensemble-methods/#bootstrap-aggregating-bagging","title":"Bootstrap Aggregating (Bagging)","text":"<p>Algorithm:</p> <ol> <li>Create B bootstrap samples (sample with replacement)</li> <li>Each sample same size as original data</li> <li>Some observations appear multiple times, some not at all</li> <li> <p>~63.2% unique observations per sample</p> </li> <li> <p>Train a separate model on each sample</p> </li> <li> <p>Aggregate predictions:</p> </li> <li>Regression: Average</li> <li>Classification: Majority vote</li> </ol> <p>Why ~63.2%? When sampling n observations with replacement from n, the probability any specific row is never selected is:</p> \\[(1 - \\frac{1}{n})^n \\approx e^{-1} \\approx 0.368\\] <p>So ~36.8% are left out (\"out-of-bag\"), meaning ~63.2% are included.</p> <p>Building intuition for this limit: Consider sampling 1000 observations with replacement from 1000: - Each draw, P(row i is NOT picked) = 999/1000 = 0.999 - After 1000 draws, P(row i NEVER picked) = 0.999^1000 \u2248 0.368 - This converges to e\u207b\u00b9 as n grows\u2014a fundamental constant in probability</p> <p>The math is elegant: (1 - 1/n)^n approaches e\u207b\u00b9 because this is how the exponential function is defined!</p> <p>Numerical Example: Bootstrap Sampling in Action</p> <pre><code>import numpy as np\n\nnp.random.seed(42)\nn_samples = 1000\nn_bootstrap_samples = 100\n\nunique_fractions = []\nfor _ in range(n_bootstrap_samples):\n    bootstrap_indices = np.random.choice(\n        n_samples,\n        size=n_samples,\n        replace=True,\n    )\n    unique_count = len(np.unique(bootstrap_indices))\n    unique_fractions.append(unique_count / n_samples)\n\nprint(f\"Mean unique fraction: {np.mean(unique_fractions):.3f}\")\nprint(f\"Theoretical (1 - e\u207b\u00b9): {1 - np.exp(-1):.3f}\")\n</code></pre> <p>Output: <pre><code>Mean unique fraction: 0.632\nTheoretical (1 - e\u207b\u00b9): 0.632\n</code></pre></p> <p>Interpretation: Across 100 bootstrap samples, exactly 63.2% of observations appear on average\u2014matching the theoretical prediction. The remaining 36.8% are \"out-of-bag\" and can be used for free validation.</p> <p>Source: <code>slide_computations/module4_examples.py</code> - <code>demo_bootstrap_sampling()</code></p> <p>Why replacement? Without replacement at the same size, you'd get identical datasets. With replacement: some observations appear multiple times (emphasized), some don't appear (~36.8%, providing OOB validation), and different trees emphasize different observations\u2014creating diversity. Bootstrap sampling approximates drawing fresh samples from the true population.</p>"},{"location":"modules/04-ensemble-methods/#random-forests-double-randomness","title":"Random Forests: Double Randomness","text":"<p>Random Forests extend bagging with two sources of randomness:</p> <ol> <li> <p>Row sampling (from bagging): Each tree gets a bootstrap sample</p> </li> <li> <p>Feature sampling (unique to RF): At each split, consider only a random subset</p> </li> <li>Default: \\(\\sqrt{d}\\) features for classification (where d = total features)</li> </ol> <p>Why feature sampling matters:</p> <p>Imagine one incredibly predictive feature (credit score for loan default). Without feature sampling, every tree uses it as the root split. All trees become highly correlated.</p> <p>With feature sampling, each split considers a random subset. Sometimes credit score isn't available. The tree finds other splits. This creates diversity.</p> <p>The tradeoff: Ignoring the best feature sometimes hurts individual trees (higher bias), but trees become more diverse (lower correlation). The ensemble variance formula shows reducing correlation (\u03c1) often helps more than the slight increase in individual variance (\u03c3\u00b2). Random Forests typically outperform bagged trees precisely because of this tradeoff. The <code>max_features</code> hyperparameter controls this\u2014default \u221ad is a good starting point.</p>"},{"location":"modules/04-ensemble-methods/#why-bagging-reduces-overfitting","title":"Why Bagging Reduces Overfitting","text":"<ul> <li>A single deep tree overfits to specific patterns</li> <li>Each tree in the forest also overfits, but to DIFFERENT patterns</li> <li>When we average, idiosyncratic overfitting cancels out</li> <li>True signal remains (all trees agree on it)</li> </ul> <p>The ensemble variance formula:</p> \\[Var(ensemble) = \\rho\\sigma^2 + \\frac{(1-\\rho)\\sigma^2}{n}\\] <p>Where: - \\(\\sigma^2\\) = variance of individual tree predictions - \\(\\rho\\) = average correlation between trees (0 = independent, 1 = identical) - \\(n\\) = number of trees</p> <p>Reading this formula: - First term (\\(\\rho\\sigma^2\\)): Irreducible variance from correlation - Second term: Shrinks as you add trees</p> <p>Key insight: Lower correlation between trees = better ensemble. Feature sampling specifically reduces \\(\\rho\\).</p> <p>Seeing the formula in action: With 10 trees and \u03c3\u00b2=100:</p> Correlation (\u03c1) Ensemble Variance Reduction 0.0 (independent) 10 90% 0.5 (moderate) 55 45% 1.0 (identical) 100 0% <p>Even with \u03c1=0.5, you still get 45% variance reduction. This explains why Random Forests work well in practice\u2014trees don't need to be perfectly independent, just somewhat different.</p> <p>Numerical Example: Random Forest vs Single Tree</p> <pre><code>from sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\n\nnp.random.seed(42)\nX, y = make_classification(\n    n_samples=500, n_features=20, n_informative=10,\n    n_redundant=5, n_classes=2, random_state=42,\n)\n\n# Run 20 different train/test splits\ntree_scores, rf_scores = [], []\nfor trial in range(20):\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.3, random_state=trial,\n    )\n    tree = DecisionTreeClassifier(max_depth=None, random_state=42)\n    tree.fit(X_train, y_train)\n    tree_scores.append(tree.score(X_test, y_test))\n\n    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n    rf.fit(X_train, y_train)\n    rf_scores.append(rf.score(X_test, y_test))\n\nprint(f\"Single Tree: Mean={np.mean(tree_scores):.3f}, Std={np.std(tree_scores):.3f}\")\nprint(f\"RF (100):    Mean={np.mean(rf_scores):.3f}, Std={np.std(rf_scores):.3f}\")\n</code></pre> <p>Output: <pre><code>Single Tree: Mean=0.809, Std=0.032\nRF (100):    Mean=0.891, Std=0.025\n</code></pre></p> <p>Interpretation: Across 20 different data splits, Random Forest achieves 8 percentage points higher accuracy AND 22% lower variance. The ensemble is both more accurate and more stable than any single tree.</p> <p>Source: <code>slide_computations/module4_examples.py</code> - <code>demo_rf_vs_single_tree()</code></p> <p>Number of trees: 100-500 trees usually sufficient. Plot OOB error vs. n_estimators\u2014it decreases rapidly then flattens. Unlike boosting, more RF trees never hurt performance; they just stop helping. More trees mean more memory and slower inference, so balance accuracy against cost.</p>"},{"location":"modules/04-ensemble-methods/#feature-importance","title":"Feature Importance","text":"<p>Mean Decrease in Impurity (MDI): - Sum of impurity decreases from splits using each feature, averaged across trees - Fast to compute - Can favor high-cardinality features</p> <p>Permutation Importance: - Shuffle each feature and measure accuracy decrease - More reliable, slower - Preferred for stakeholder communication</p> <p>Important caveat: Importance \u2260 direction of effect! Importance tells you which features the model relies on, not HOW they affect predictions. For that, use SHAP values (Module 9).</p> <pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.inspection import permutation_importance\n\nrf = RandomForestClassifier(\n    n_estimators=100,\n    max_features='sqrt',\n    oob_score=True,\n    random_state=42\n)\nrf.fit(X_train, y_train)\n\n# OOB score (free validation!)\nprint(f\"OOB Accuracy: {rf.oob_score_:.3f}\")\n\n# MDI importance (fast)\nimportance_mdi = rf.feature_importances_\n\n# Permutation importance (more reliable)\nperm_imp = permutation_importance(rf, X_test, y_test, n_repeats=10)\n</code></pre>"},{"location":"modules/04-ensemble-methods/#out-of-bag-oob-error","title":"Out-of-Bag (OOB) Error","text":"<p>Each bootstrap sample leaves out ~36.8% of observations. These \"out-of-bag\" samples provide free validation:</p> <ul> <li>For each observation, predict using only trees that didn't train on it</li> <li>OOB error \u2248 cross-validation error, but FREE!</li> </ul> <pre><code>rf = RandomForestClassifier(oob_score=True)\nrf.fit(X_train, y_train)\nprint(f\"OOB Accuracy: {rf.oob_score_}\")\n</code></pre> <p>Why OOB \u2248 cross-validation: For any single observation, about 36.8% of trees never saw it during training. When you predict that observation using only those trees, you get an honest estimate\u2014those trees couldn't have memorized it. Aggregating these honest predictions across all observations gives you an error estimate very close to what k-fold cross-validation would produce, but without the computational cost of retraining k times.</p> <p>Numerical Example: OOB Error vs Cross-Validation</p> <pre><code>from sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\n\nnp.random.seed(42)\nX, y = make_classification(\n    n_samples=1000, n_features=20, n_informative=10,\n    n_redundant=5, n_classes=2, random_state=42,\n)\n\n# OOB scoring\nrf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\nrf.fit(X, y)\n\n# Cross-validation\ncv_scores = cross_val_score(\n    RandomForestClassifier(n_estimators=100, random_state=42),\n    X, y, cv=5,\n)\n\nprint(f\"OOB Accuracy:    {rf.oob_score_:.4f}\")\nprint(f\"5-Fold CV Mean:  {np.mean(cv_scores):.4f}\")\nprint(f\"Difference:      {abs(rf.oob_score_ - np.mean(cv_scores)):.4f}\")\n</code></pre> <p>Output: <pre><code>OOB Accuracy:    0.9210\n5-Fold CV Mean:  0.9330\nDifference:      0.0120\n</code></pre></p> <p>Interpretation: OOB and 5-fold CV produce nearly identical estimates (within 1.2 percentage points), but OOB comes free\u2014no extra model training required. Use OOB for quick hyperparameter feedback during tuning.</p> <p>Source: <code>slide_computations/module4_examples.py</code> - <code>demo_oob_vs_cv()</code></p>"},{"location":"modules/04-ensemble-methods/#common-misconceptions_1","title":"Common Misconceptions","text":"Misconception Reality \"Random Forest can't overfit\" It can! Deep trees with too few estimators still overfit. Tuning on test set causes overfitting to that. \"More trees is always better\" Diminishing returns. 100-500 usually sufficient. \"Random Forest is a black box\" Feature importance and SHAP make it reasonably interpretable. \"Feature importance = feature effect\" Importance shows reliance, not direction of effect."},{"location":"modules/04-ensemble-methods/#43-boosting-methods","title":"4.3 Boosting Methods","text":""},{"location":"modules/04-ensemble-methods/#three-components-gradient-boosting","title":"Three Components: Gradient Boosting","text":"Component Gradient Boosting (XGBoost) Decision Model Sequential ensemble \u2014 sum of many shallow trees Quality Measure Any differentiable loss + regularization Update Method Gradient descent in function space \u2014 each tree corrects previous errors <p>The update method is fascinating: instead of updating parameters, we add new functions (trees). Each tree predicts the negative gradient (residuals).</p>"},{"location":"modules/04-ensemble-methods/#the-boosting-philosophy","title":"The Boosting Philosophy","text":"<p>Build models sequentially, where each new model focuses on mistakes of previous ones.</p> Bagging Boosting Parallel (independent trees) Sequential (dependent trees) Reduces variance Reduces bias (and variance) Deep trees Shallow trees typical <p>Visual metaphors: - Bagging: Committee of experts who work independently and vote - Boosting: Relay team where each runner covers for previous weaknesses</p>"},{"location":"modules/04-ensemble-methods/#adaboost-adaptive-boosting","title":"AdaBoost: Adaptive Boosting","text":"<ol> <li>Start with equal weights for all training examples</li> <li>Train a weak learner (often a \"stump\"\u2014one split)</li> <li>Identify misclassified examples</li> <li>Increase weights on misclassified examples</li> <li>Train next weak learner on reweighted data</li> <li>Repeat</li> </ol> <p>Key insight: Each subsequent learner specializes in hard examples previous learners got wrong.</p> <p>Boosting and outliers: Boosting can obsess over mislabeled or impossible-to-fit examples. Mitigation: (1) <code>subsample</code> (0.8) so outliers don't appear every round, (2) lower learning rate to limit per-iteration damage, (3) regularization (<code>reg_alpha</code>, <code>reg_lambda</code>) to prevent extreme predictions, (4) early stopping before overfitting to noise. Random Forests are more robust because outliers only affect ~63% of trees and no tree specifically focuses on them.</p>"},{"location":"modules/04-ensemble-methods/#gradient-boosting-machines","title":"Gradient Boosting Machines","text":"<p>Core innovation: Fit each new tree to the residuals (errors).</p> <ol> <li>Make initial prediction (often the mean)</li> <li>Calculate residuals: \\(actual - predicted\\)</li> <li>Fit a tree to predict the residuals</li> <li>Add this tree's predictions (with learning rate)</li> <li>Calculate new residuals</li> <li>Repeat</li> </ol> <p>Why \"gradient\"? For MSE loss:</p> \\[\\frac{\\partial L}{\\partial \\hat{y}} = -(y - \\hat{y}) = -\\text{residual}\\] <p>The residual IS the negative gradient of the loss. When we fit trees to residuals, we're following the gradient in function space.</p> <p>Gradient in function space: Normal gradient descent optimizes parameters (adjust \u03b8). Gradient boosting optimizes functions (add a new tree). For squared error, the negative gradient is simply the residual. Fitting a tree to residuals approximates \"what should I add to reduce error?\" The learning rate works like in gradient descent\u2014taking fractional steps (0.1 \u00d7 tree_prediction) prevents overshooting. So: F_new(x) = F_old(x) + learning_rate \u00d7 new_tree(x). Each tree is a step in function space toward lower loss.</p> <p>Watching boosting learn: On a simple regression problem (y = 2x + 3 + noise), here's what happens across boosting rounds:</p> Round Residual Std MSE Init 5.93 35.1 1 4.69 22.0 2 3.83 14.6 3 3.22 10.4 5 2.46 6.1 <p>Each tree chips away at the remaining error. The residual standard deviation drops steadily as boosting \"discovers\" the linear relationship through many small corrections.</p> <p>Numerical Example: Gradient Boosting Step by Step</p> <pre><code>from sklearn.tree import DecisionTreeRegressor\nimport numpy as np\n\nnp.random.seed(42)\nn_samples = 100\nX = np.random.uniform(low=0, high=10, size=(n_samples, 1))\ny = 2 * X.ravel() + 3 + np.random.normal(loc=0, scale=2, size=n_samples)\n\n# Manual gradient boosting\nlearning_rate = 0.3\nprediction = np.full(n_samples, np.mean(y))  # Start with mean\n\nprint(f\"{'Round':&gt;6} {'Residual Std':&gt;14} {'MSE':&gt;10}\")\nfor round_num in range(6):\n    residuals = y - prediction\n    mse = np.mean(residuals ** 2)\n    print(f\"{round_num:&gt;6} {np.std(residuals):&gt;14.2f} {mse:&gt;10.2f}\")\n    if round_num &lt; 5:\n        tree = DecisionTreeRegressor(max_depth=1, random_state=42)\n        tree.fit(X, residuals)\n        prediction += learning_rate * tree.predict(X)\n</code></pre> <p>Output: <pre><code>Round   Residual Std        MSE\n     0           5.93      35.12\n     1           4.69      21.99\n     2           3.83      14.64\n     3           3.22      10.35\n     4           2.76       7.62\n     5           2.46       6.06\n</code></pre></p> <p>Interpretation: Each round, a shallow tree predicts the residuals (errors), and we add a fraction of its predictions. MSE drops from 35 to 6 in just 5 rounds as boosting learns the linear pattern y = 2x + 3.</p> <p>Source: <code>slide_computations/module4_examples.py</code> - <code>demo_gradient_boosting_steps()</code></p>"},{"location":"modules/04-ensemble-methods/#key-boosting-hyperparameters","title":"Key Boosting Hyperparameters","text":"Parameter Effect <code>n_estimators</code> More \u2192 more capacity, but overfit risk <code>learning_rate</code> Smaller \u2192 need more trees, often better <code>max_depth</code> Usually 3-8 (much shallower than RF) <p>Trade-off: Lower learning rate + more trees often gives best results but takes longer.</p> <p>Practical guidance for learning rate: - Start with 0.1: Good default, fast enough to iterate - Try 0.01-0.05: If overfitting (training &gt;&gt; test accuracy) - Use 0.3: Only for quick prototyping or if data is very large - Always pair with early stopping: Let the algorithm find optimal n_estimators</p> <p>The key insight: a lower learning rate makes each tree's contribution smaller, requiring more trees to reach the same capacity. This acts as implicit regularization\u2014the model has more chances to \"change its mind\" and doesn't commit too heavily to early patterns.</p> <p>Numerical Example: Learning Rate Effects on Boosting</p> <pre><code>from sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nX, y = make_classification(\n    n_samples=1000, n_features=20, n_informative=10,\n    n_redundant=5, n_classes=2, random_state=42,\n)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42,\n)\n\nconfigs = [(0.3, 50), (0.1, 150), (0.03, 500)]\nfor lr, n_est in configs:\n    gb = GradientBoostingClassifier(\n        learning_rate=lr, n_estimators=n_est, max_depth=3, random_state=42,\n    )\n    gb.fit(X_train, y_train)\n    print(f\"LR={lr:.2f}, Trees={n_est:&gt;3}: Test={gb.score(X_test, y_test):.4f}\")\n</code></pre> <p>Output: <pre><code>LR=0.30, Trees= 50: Test=0.9033\nLR=0.10, Trees=150: Test=0.9067\nLR=0.03, Trees=500: Test=0.9033\n</code></pre></p> <p>Interpretation: All three configurations achieve similar test accuracy, but through different paths. Lower learning rate + more trees is slower to train but often more stable. The medium configuration (0.1, 150) slightly edges out the others here.</p> <p>Source: <code>slide_computations/module4_examples.py</code> - <code>demo_learning_rate_effects()</code></p> <p>Tree depth difference: - Random Forest: Deep, fully-grown trees (low bias, high variance). Averaging reduces variance. - Boosting: Shallow trees (high bias). Sequential correction reduces bias.</p>"},{"location":"modules/04-ensemble-methods/#xgboost-the-competition-champion","title":"XGBoost: The Competition Champion","text":"<p>XGBoost adds optimizations that make it dominant:</p> <ol> <li>Regularization: L1/L2 penalties on leaf weights</li> <li>Parallel processing: Split evaluation parallelized within trees</li> <li>Missing value handling: Learns optimal direction for missing values</li> <li>Histogram-based splitting: Bins features for speed</li> </ol> <p>Why it dominates: Won more Kaggle competitions than any other algorithm. Widely adopted in finance, insurance, tech.</p> <p>When Random Forest is better: (1) Noisy labels\u2014RF more robust, noise doesn't compound; (2) Limited tuning time\u2014RF works well with defaults; (3) Parallelization\u2014RF trees train independently; (4) Small datasets\u2014boosting can overfit quickly. A well-tuned XGBoost beats a well-tuned RF, but default RF often beats default XGBoost. In many real-world scenarios, the difference is 1-2%.</p>"},{"location":"modules/04-ensemble-methods/#xgboost-with-early-stopping","title":"XGBoost with Early Stopping","text":"<p>Always use early stopping with boosting!</p> <pre><code>import xgboost as xgb\n\nxgb_model = xgb.XGBClassifier(\n    n_estimators=1000,\n    learning_rate=0.1,\n    max_depth=5,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    reg_alpha=0.1,\n    reg_lambda=1.0,\n    random_state=42\n)\n\nxgb_model.fit(\n    X_train, y_train,\n    eval_set=[(X_val, y_val)],\n    early_stopping_rounds=10,  # Stop if no improvement for 10 rounds\n    verbose=False\n)\n\nprint(f\"Best iteration: {xgb_model.best_iteration}\")\n</code></pre> <p>Without early stopping, boosting overfits. With it, training stops when validation plateaus.</p> <p>Why early stopping beats fixed n_estimators: The optimal number depends on learning rate, tree depth, data complexity, and sample size\u2014a fixed number can't adapt. Set a large n_estimators as an upper limit, monitor validation loss, stop when no improvement for N consecutive rounds. The model finds its own stopping point, works with any learning rate, and prevents overfitting automatically. Always use a separate validation set for early stopping\u2014not your final test set.</p> <p>Numerical Example: Early Stopping in Action</p> <pre><code>from sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingClassifier\nimport numpy as np\n\nX, y = make_classification(\n    n_samples=500, n_features=20, n_informative=5,\n    n_redundant=10, n_clusters_per_class=3, random_state=42,\n)\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\ngb = GradientBoostingClassifier(\n    n_estimators=300, learning_rate=0.1, max_depth=4, random_state=42,\n)\ngb.fit(X_train, y_train)\n\n# Track validation accuracy at each stage\nval_scores = [np.mean(pred == y_val) for pred in gb.staged_predict(X_val)]\nbest_n = np.argmax(val_scores) + 1\n\nprint(f\"Best validation at {best_n} trees: {val_scores[best_n-1]:.4f}\")\nprint(f\"Final (300 trees):                 {val_scores[-1]:.4f}\")\nprint(f\"Overfit penalty: {(val_scores[best_n-1] - val_scores[-1])*100:.1f} points\")\n</code></pre> <p>Output: <pre><code>Best validation at 14 trees: 0.9000\nFinal (300 trees):           0.8800\nOverfit penalty: 2.0 points\n</code></pre></p> <p>Interpretation: Validation accuracy peaks at just 14 trees, then declines as the model overfits. Training to 300 trees costs 2 percentage points of accuracy. Early stopping would have stopped at 14 trees automatically.</p> <p>Source: <code>slide_computations/module4_examples.py</code> - <code>demo_early_stopping()</code></p>"},{"location":"modules/04-ensemble-methods/#lightgbm-and-catboost","title":"LightGBM and CatBoost","text":"<p>LightGBM: - Even faster than XGBoost - Histogram-based splitting - Great for very large datasets</p> <p>CatBoost: - Excellent categorical feature handling - No one-hot encoding needed - Often works well with defaults</p> <p>Rule of thumb: Start with XGBoost. Try LightGBM for very large data. Try CatBoost for many categorical features.</p>"},{"location":"modules/04-ensemble-methods/#bagging-vs-boosting-when-to-use-each","title":"Bagging vs Boosting: When to Use Each","text":"Scenario Recommendation High-variance (deep trees) Bagging (RF) High-bias (shallow trees) Boosting Fast training needed Bagging (parallelizable) Best accuracy needed Boosting (often wins) Noisy labels Bagging (more robust) Need interpretability Random Forest"},{"location":"modules/04-ensemble-methods/#common-misconceptions_2","title":"Common Misconceptions","text":"Misconception Reality \"XGBoost is always best\" No Free Lunch. Linear models beat it on linear data. Neural networks beat it on images/text. \"Boosting can't overfit\" Very much can! Use early stopping. \"More boosting rounds = better\" Unlike RF, more rounds increases overfit risk. \"XGBoost, LightGBM, CatBoost are completely different\" All gradient boosting variants. Similar core ideas."},{"location":"modules/04-ensemble-methods/#44-other-ensemble-techniques","title":"4.4 Other Ensemble Techniques","text":""},{"location":"modules/04-ensemble-methods/#stacking","title":"Stacking","text":"<p>Use model predictions as features for a \"meta-learner.\"</p> <pre><code>Level 0:   RF_pred    XGB_pred    LR_pred\n              \u2193           \u2193          \u2193\nLevel 1:     Meta-model (e.g., Logistic Regression)\n                         \u2193\n                  Final Prediction\n</code></pre> <p>How it works: 1. Train several level-0 models (RF, XGBoost, logistic regression) 2. Generate predictions using cross-validation (out-of-fold) 3. Use predictions as features for level-1 meta-model 4. Meta-model learns which base models to trust</p> <p>Critical: Must use out-of-fold predictions to avoid leakage!</p> <p>Why out-of-fold matters:</p> <p>If you train RF on all training data and use its predictions on that same data as meta-features, RF makes artificially confident predictions (it's seen those examples). This won't generalize.</p> <p>Correct approach: 1. Split into K folds 2. For fold 1: Train on folds 2-5, predict fold 1 3. For fold 2: Train on folds 1,3-5, predict fold 2 4. Continue for all folds 5. Meta-model trains on these honest predictions</p> <p>Why this matters numerically: Suppose a Random Forest achieves 95% training accuracy but only 85% test accuracy. If you use training predictions as meta-features, the meta-model sees \"RF predicts 0.95 probability\" for examples RF memorized. It learns to over-trust RF. On new data, RF's predictions are less confident, but the meta-model doesn't know this\u2014it still over-trusts RF. Out-of-fold predictions ensure the meta-model only sees RF's \"honest\" performance level.</p> <p>Multi-level stacking: Going deeper is possible but rarely worthwhile. Two levels is usually sufficient (Netflix Prize used two). Each additional level requires proper out-of-fold predictions (complex bookkeeping), increases overfitting risk, and slows inference. In production, a single well-tuned XGBoost or simple two-level stack is almost always preferred.</p>"},{"location":"modules/04-ensemble-methods/#voting-classifiers","title":"Voting Classifiers","text":"<p>Simpler than stacking: combine predictions directly.</p> <p>Hard voting: Each model votes; majority wins.</p> <p>Soft voting: Average probability estimates; pick highest.</p> <pre><code># Model 1: P(A)=0.7, P(B)=0.3\n# Model 2: P(A)=0.4, P(B)=0.6\n# Model 3: P(A)=0.8, P(B)=0.2\n# Average: P(A)=0.63 \u2192 Class A\n</code></pre> <p>Soft voting usually performs better (uses more information).</p> <p>When hard and soft voting disagree: Consider three models predicting classes A vs B: - Model 1: P(A)=0.45, P(B)=0.55 \u2192 predicts B - Model 2: P(A)=0.49, P(B)=0.51 \u2192 predicts B - Model 3: P(A)=0.90, P(B)=0.10 \u2192 predicts A</p> <p>Hard voting: A=1, B=2 \u2192 B wins Soft voting: P(A)=(0.45+0.49+0.90)/3=0.613 \u2192 A wins</p> <p>Soft voting correctly captures that Model 3 is highly confident about A, while Models 1 and 2 are barely confident about B. A 90% confident prediction should count more than two 51% predictions.</p> <p>Numerical Example: Soft vs Hard Voting</p> <pre><code>from sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import (\n    RandomForestClassifier, GradientBoostingClassifier, VotingClassifier,\n)\nfrom sklearn.linear_model import LogisticRegression\n\nX, y = make_classification(\n    n_samples=500, n_features=10, n_informative=5, random_state=42,\n)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42,\n)\n\nrf = RandomForestClassifier(n_estimators=50, random_state=42)\ngb = GradientBoostingClassifier(n_estimators=50, random_state=42)\nlr = LogisticRegression(random_state=42)\n\nhard = VotingClassifier([('rf', rf), ('gb', gb), ('lr', lr)], voting='hard')\nsoft = VotingClassifier([('rf', rf), ('gb', gb), ('lr', lr)], voting='soft')\n\nhard.fit(X_train, y_train)\nsoft.fit(X_train, y_train)\n\nprint(f\"Hard Voting: {hard.score(X_test, y_test):.4f}\")\nprint(f\"Soft Voting: {soft.score(X_test, y_test):.4f}\")\n</code></pre> <p>Output: <pre><code>Hard Voting: 0.9333\nSoft Voting: 0.9200\n</code></pre></p> <p>Interpretation: In this case, hard voting slightly outperforms soft voting. Results vary by dataset\u2014soft voting usually wins when models have well-calibrated probabilities, but hard voting can win when probability estimates are noisy. Try both!</p> <p>Source: <code>slide_computations/module4_examples.py</code> - <code>demo_soft_vs_hard_voting()</code></p> <pre><code>from sklearn.ensemble import VotingClassifier, StackingClassifier\n\n# Soft Voting\nvoting_clf = VotingClassifier(\n    estimators=[\n        ('rf', RandomForestClassifier(n_estimators=100)),\n        ('gb', GradientBoostingClassifier(n_estimators=100)),\n        ('lr', LogisticRegression())\n    ],\n    voting='soft'\n)\n\n# Stacking\nstacking_clf = StackingClassifier(\n    estimators=[\n        ('rf', RandomForestClassifier(n_estimators=100)),\n        ('gb', GradientBoostingClassifier(n_estimators=100))\n    ],\n    final_estimator=LogisticRegression(),\n    cv=5\n)\n</code></pre>"},{"location":"modules/04-ensemble-methods/#when-to-use-each-approach","title":"When to Use Each Approach","text":"Method Use When Simple voting Models roughly equal; quick solution Weighted voting Some models clearly better Stacking Time for complexity; competition setting <p>Avoid sophisticated ensembles when: - Explainability is crucial - Fast inference needed - Limited compute</p>"},{"location":"modules/04-ensemble-methods/#common-misconceptions_3","title":"Common Misconceptions","text":"Misconception Reality \"Stacking always improves performance\" If base models are highly correlated, stacking adds complexity without benefit. \"More diverse base models = better\" Diversity helps, but models still need to be individually competent. \"Stacking is just averaging with extra steps\" Meta-learner can learn complex patterns like \"trust RF for certain input ranges.\""},{"location":"modules/04-ensemble-methods/#reflection-questions","title":"Reflection Questions","text":"<ol> <li> <p>Why does the wisdom of crowds work? Under what conditions would it fail?</p> </li> <li> <p>A colleague says Random Forest can never overfit. How would you respond?</p> </li> <li> <p>Why sample features at each split rather than once per tree?</p> </li> <li> <p>When might boosting overfit more easily than bagging? What would you adjust?</p> </li> <li> <p>A data scientist says they always use XGBoost because \"it wins Kaggle.\" What's your response?</p> </li> <li> <p>You have 5 models with accuracies 82%, 81%, 79%, 78%, 75%. Would you ensemble all 5? Why or why not?</p> </li> </ol>"},{"location":"modules/04-ensemble-methods/#practice-problems","title":"Practice Problems","text":"<ol> <li> <p>Derive why ~63.2% of observations appear in each bootstrap sample</p> </li> <li> <p>If you have 100 features in a classification problem, how many are considered at each split in Random Forest (default)?</p> </li> <li> <p>Explain why Random Forest feature importance might differ from permutation importance</p> </li> <li> <p>Draw a diagram showing how 5 stumps combine in AdaBoost vs how 5 shallow trees combine in Gradient Boosting</p> </li> <li> <p>You train XGBoost without early stopping and see training accuracy at 99% but test accuracy at 75%. Diagnose and fix.</p> </li> </ol>"},{"location":"modules/04-ensemble-methods/#chapter-summary","title":"Chapter Summary","text":"<p>Six key takeaways from Module 4:</p> <ol> <li> <p>Ensembles work by combining diverse models\u2014errors cancel out</p> </li> <li> <p>Bagging (Random Forests) reduces variance through averaging independent trees</p> </li> <li> <p>Boosting (XGBoost) reduces bias through sequential learning from errors</p> </li> <li> <p>Feature importance shows predictive power, not effect direction</p> </li> <li> <p>Early stopping is essential for boosting methods</p> </li> <li> <p>Choose wisely: Random Forest for robustness, XGBoost for accuracy</p> </li> </ol>"},{"location":"modules/04-ensemble-methods/#whats-next","title":"What's Next","text":"<p>In Module 5, we tackle Unsupervised Learning: - Clustering (K-Means, hierarchical) - Dimensionality reduction (PCA) - Finding structure without labels</p> <p>So far, we've had a target variable to predict. In unsupervised learning, there's no target\u2014we're discovering hidden patterns in the data.</p>"},{"location":"modules/05-unsupervised/","title":"Module 5: Unsupervised Learning","text":""},{"location":"modules/05-unsupervised/#introduction","title":"Introduction","text":"<p>Today marks a significant shift in how we think about machine learning.</p> <p>In Modules 2 through 4, we always had a target variable\u2014sales, churn, fraud. We had labels, and we trained models to predict those labels.</p> <p>Now we throw that away. No labels. No target variable.</p> <p>Unsupervised learning is about discovering structure in data when you don't know what you're looking for. You're exploring, not predicting.</p> <p>This might sound less useful, but unsupervised learning solves critical business problems: customer segmentation, anomaly detection, data visualization, feature extraction. These are problems where labels don't exist or are too expensive to obtain.</p> <p>Validating unsupervised learning: \"Right\" is about usefulness, not correctness. Use internal metrics (silhouette, inertia), check stability across runs, and\u2014most importantly\u2014validate with domain experts. Do clusters suggest actionable strategies? A \"statistically optimal\" 7-cluster solution that marketing can't operationalize is less useful than a 3-cluster solution they can act on.</p>"},{"location":"modules/05-unsupervised/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, you should be able to:</p> <ol> <li>Explain the difference between supervised and unsupervised learning</li> <li>Apply K-means and DBSCAN clustering algorithms and interpret results</li> <li>Determine optimal number of clusters using elbow method and silhouette scores</li> <li>Apply PCA for dimensionality reduction and interpret principal components</li> <li>Use manifold learning techniques (t-SNE, UMAP) for visualization</li> <li>Identify business applications for clustering and dimensionality reduction</li> </ol>"},{"location":"modules/05-unsupervised/#51-clustering","title":"5.1 Clustering","text":""},{"location":"modules/05-unsupervised/#supervised-vs-unsupervised","title":"Supervised vs Unsupervised","text":"Supervised Unsupervised Have labels No labels Learn to predict Discover structure Regression, Classification Clustering, Dim. Reduction <p>Supervised: \"Here are the right answers; learn to predict them.\"</p> <p>Unsupervised: \"Here's the data; find interesting patterns.\"</p>"},{"location":"modules/05-unsupervised/#three-components-k-means","title":"Three Components: K-Means","text":"<p>Even unsupervised algorithms fit our three-component framework:</p> Component K-Means Decision Model Cluster assignments \u2014 each point belongs to nearest centroid Quality Measure Within-cluster sum of squares (inertia) Update Method Iterative assignment-update \u2014 alternate between assigning and moving centroids <p>Key difference from supervised learning: Without labels, we define \"quality\" differently. Instead of prediction error, we measure how compact and well-separated clusters are.</p> <p>Distinguishing real structure from noise: Clustering algorithms will always find clusters\u2014even in random data. Use the gap statistic (compares quality to random data), stability analysis (cluster on subsets\u2014real structure is stable), and multiple algorithms (if K-means, DBSCAN, and hierarchical all find similar groups, structure is more credible). Always verify clusters predict something meaningful.</p>"},{"location":"modules/05-unsupervised/#clustering-applications","title":"Clustering Applications","text":"<ul> <li>Customer segmentation \u2014 Group by purchasing behavior, target marketing per segment</li> <li>Document grouping \u2014 Organize by topic without predefined categories</li> <li>Anomaly detection \u2014 Find observations that don't fit any group</li> <li>Image compression \u2014 Reduce color palettes by clustering similar colors</li> <li>Gene expression \u2014 Group genes with similar activation patterns</li> </ul>"},{"location":"modules/05-unsupervised/#k-means-algorithm","title":"K-Means Algorithm","text":"<p>The algorithm: 1. Choose K (number of clusters) 2. Randomly initialize K centroids 3. Assign: Each point to nearest centroid 4. Update: Move centroids to mean of assigned points 5. Repeat until centroids stop moving</p> <p>The objective:</p> \\[\\text{minimize } \\sum_{i=1}^{K}\\sum_{x \\in C_i} ||x - \\mu_i||^2\\] <p>Where \\(\\mu_i\\) is the centroid of cluster \\(C_i\\). Minimize total distance from points to their centroids.</p> <p>Strengths: - Fast and scalable\u2014works on millions of points - Easy to implement and interpret - Works well with spherical clusters</p> <p>Weaknesses: - Must specify K in advance - Sensitive to initialization - Assumes spherical, similar-sized clusters</p> <p>\"Spherical\" clusters: K-means assigns points to the nearest centroid using Euclidean distance, implicitly assuming clusters are ball-shaped with equal spread in all directions. K-means essentially draws Voronoi cells (straight-line boundaries)\u2014any cluster that can't fit in a convex cell will be problematic. For non-spherical shapes, use DBSCAN (any shape), GMMs (elliptical), or spectral clustering (complex manifolds).</p> <pre><code>from sklearn.cluster import KMeans\n\nkmeans = KMeans(\n    n_clusters=5,\n    init='k-means++',    # Smart initialization\n    n_init=10,           # Run 10 times, keep best\n    random_state=42\n)\n\nlabels = kmeans.fit_predict(X_scaled)\ncentroids = kmeans.cluster_centers_\nprint(f\"Inertia: {kmeans.inertia_}\")\n</code></pre> <p>Watching K-means converge: The algorithm's behavior becomes intuitive when you trace it step by step. Start with random centroids. Each iteration has two phases: (1) assignment\u2014each point \"votes\" for its nearest centroid, and (2) update\u2014centroids move to the center of their voters. Points switch allegiance when a centroid moves closer than their current one. Convergence happens when no point switches\u2014a stable equilibrium.</p> <p>Numerical Example: K-Means Iterations Step by Step</p> <pre><code>import numpy as np\n\n# 6 points that form 2 natural clusters\nX = np.array([\n    [1.0, 1.0], [1.5, 2.0], [1.2, 1.5],  # Cluster A\n    [5.0, 5.0], [5.5, 4.5], [5.2, 5.2],  # Cluster B\n])\n\n# Initialize centroids (not optimal on purpose)\ncentroids = np.array([[2.0, 3.0], [4.0, 3.0]])\n\n# Run 2 iterations manually\nfor iteration in range(2):\n    # Assignment: each point to nearest centroid\n    labels = []\n    for point in X:\n        d0 = np.sqrt(np.sum((point - centroids[0])**2))\n        d1 = np.sqrt(np.sum((point - centroids[1])**2))\n        labels.append(0 if d0 &lt; d1 else 1)\n\n    # Update: move centroids to cluster means\n    labels = np.array(labels)\n    for k in range(2):\n        centroids[k] = X[labels == k].mean(axis=0)\n\n    print(f\"Iteration {iteration+1}: labels={labels}, \"\n          f\"centroids={centroids.round(2)}\")\n</code></pre> <p>Output: <pre><code>Iteration 1: labels=[0 0 0 1 1 1], centroids=[[1.23 1.5 ] [5.23 4.9 ]]\nIteration 2: labels=[0 0 0 1 1 1], centroids=[[1.23 1.5 ] [5.23 4.9 ]]\n</code></pre></p> <p>Interpretation: After just one iteration, points correctly grouped and centroids moved to cluster centers. Iteration 2 shows convergence\u2014assignments and centroids are stable. The final inertia is 1.01 (total squared distance from points to their centroids).</p> <p>Source: <code>slide_computations/module5_examples.py</code> - <code>demo_kmeans_iterations()</code></p>"},{"location":"modules/05-unsupervised/#choosing-k-elbow-method","title":"Choosing K: Elbow Method","text":"<p>Process: 1. Run K-means for K = 1, 2, 3, ..., n 2. Plot inertia vs K 3. Look for the \"elbow\" where adding clusters gives diminishing returns</p> <pre><code>inertias = []\nK_range = range(1, 11)\n\nfor k in K_range:\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit(X_scaled)\n    inertias.append(kmeans.inertia_)\n\nplt.plot(K_range, inertias, 'bo-')\nplt.xlabel('Number of Clusters (K)')\nplt.ylabel('Inertia')\nplt.title('Elbow Method')\n</code></pre>"},{"location":"modules/05-unsupervised/#choosing-k-silhouette-score","title":"Choosing K: Silhouette Score","text":"<p>For each point, measure how similar it is to its own cluster vs. other clusters:</p> \\[s(i) = \\frac{b(i) - a(i)}{\\max(a(i), b(i))}\\] <p>Where: - \\(a(i)\\) = average distance to points in same cluster - \\(b(i)\\) = average distance to points in nearest other cluster</p> <p>Interpretation: - s = 1: Well-clustered (far from other clusters) - s = 0: On boundary between clusters - s = -1: Probably in wrong cluster</p> <pre><code>from sklearn.metrics import silhouette_score, silhouette_samples\n\n# Overall score\nscore = silhouette_score(X_scaled, labels)\n\n# Per-sample (for diagnostics)\nsample_scores = silhouette_samples(X_scaled, labels)\n</code></pre> <p>Individual scores are useful too: - Find misclassified points (negative scores) - Identify boundary cases (scores near 0) - Detect outliers (very low scores)</p> <p>Business consideration: Sometimes the \"right\" K comes from domain knowledge, not just metrics!</p> <p>When elbow and silhouette disagree: Elbow (inertia) measures compactness; silhouette measures both compactness AND separation. Adding clusters always reduces inertia but may not improve silhouette if new clusters aren't well-separated. If elbow says 5 and silhouette says 3, clusters 4-5 might be subdividing natural groups. Look at both metrics, examine cluster profiles, consider business constraints, and check stability. There's rarely a single \"correct\" K.</p> <p>Numerical Example: Elbow and Silhouette Comparison</p> <pre><code>from sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\nfrom sklearn.metrics import silhouette_score\n\n# Generate data with 3 true clusters\nX, _ = make_blobs(n_samples=300, centers=3, cluster_std=1.0, random_state=42)\n\nprint(f\"{'K':&gt;4} {'Inertia':&gt;12} {'Silhouette':&gt;12}\")\nfor k in range(2, 8):\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    labels = kmeans.fit_predict(X)\n    sil = silhouette_score(X, labels)\n    print(f\"{k:&gt;4} {kmeans.inertia_:&gt;12.1f} {sil:&gt;12.3f}\")\n</code></pre> <p>Output: <pre><code>   K      Inertia   Silhouette\n   2       5763.5        0.705\n   3        566.9        0.848    \u2190 Both metrics agree!\n   4        496.4        0.664\n   5        427.1        0.490\n   6        375.0        0.517\n   7        308.2        0.358\n</code></pre></p> <p>Interpretation: Both elbow (big drop from K=2 to K=3) and silhouette (maximum at K=3) point to K=3, matching the true structure. When metrics agree, you can be confident. When they disagree, examine cluster profiles and consider business constraints.</p> <p>Source: <code>slide_computations/module5_examples.py</code> - <code>demo_elbow_silhouette()</code></p> <p>Calculating silhouette by hand: The silhouette score measures how well each point fits its cluster. For point i: compute a(i) = average distance to all OTHER points in the same cluster, compute b(i) = average distance to points in the NEAREST different cluster, then s(i) = (b - a) / max(a, b). A point with b &gt;&gt; a is well-placed (s \u2248 1); a point with a &gt;&gt; b is probably in the wrong cluster (s \u2248 -1).</p> <p>Numerical Example: Silhouette Score by Hand</p> <pre><code>import numpy as np\nfrom sklearn.metrics import silhouette_score\n\n# 5 points in 2 clusters\nX = np.array([[0, 0], [1, 0], [0.5, 0.5], [5, 0], [6, 0]])\nlabels = np.array([0, 0, 0, 1, 1])\n\n# For point 0: a(0) = avg dist to points 1,2 in same cluster\n#              b(0) = avg dist to points 3,4 in other cluster\nprint(\"Point  a(i)   b(i)   s(i)\")\nfor i in range(len(X)):\n    same = [j for j in range(len(X)) if labels[j] == labels[i] and j != i]\n    diff = [j for j in range(len(X)) if labels[j] != labels[i]]\n    a_i = np.mean([np.linalg.norm(X[i] - X[j]) for j in same])\n    b_i = np.mean([np.linalg.norm(X[i] - X[j]) for j in diff])\n    s_i = (b_i - a_i) / max(a_i, b_i)\n    print(f\"  {i}    {a_i:.2f}   {b_i:.2f}   {s_i:.3f}\")\n\nprint(f\"\\nAverage silhouette: {silhouette_score(X, labels):.3f}\")\n</code></pre> <p>Output: <pre><code>Point  a(i)   b(i)   s(i)\n  0    0.85   5.50   0.845\n  1    0.85   4.50   0.810\n  2    0.71   5.03   0.859\n  3    1.00   4.51   0.778\n  4    1.00   5.51   0.818\n\nAverage silhouette: 0.822\n</code></pre></p> <p>Interpretation: All points have high silhouette scores (&gt;0.7) because the clusters are well-separated. Points 0-2 are close together (small a) and far from cluster 1 (large b). The overall score of 0.82 indicates excellent clustering.</p> <p>Source: <code>slide_computations/module5_examples.py</code> - <code>demo_silhouette_by_hand()</code></p>"},{"location":"modules/05-unsupervised/#dbscan-density-based-clustering","title":"DBSCAN: Density-Based Clustering","text":"<p>K-means assumes spherical clusters. DBSCAN handles: - Irregular shapes - Different densities - Noise/outliers</p> <p>Core concepts: - Core point: Has at least <code>min_samples</code> points within <code>eps</code> distance - Border point: Within <code>eps</code> of a core point, but not core itself - Noise point: Neither (labeled -1)</p> <p>Algorithm: 1. Find all core points 2. Connect core points within <code>eps</code> of each other (transitively) 3. Assign border points to nearest core point's cluster 4. Everything else is noise</p> <p>Connecting core points (step 2) in detail:</p> <p>Two core points belong to the same cluster if they're \"density-reachable\": - Direct: Within <code>eps</code> of each other - Transitive: A connects to B, B connects to C \u2192 A and C same cluster</p> <p>This is graph traversal where core points are nodes and edges exist between points within <code>eps</code>. Each connected component becomes a cluster.</p> <p>Choosing eps and min_samples: For eps, use the k-distance plot\u2014compute each point's distance to its k-th nearest neighbor, sort and plot, look for the elbow. For min_samples, start with dimensions + 1 or 2\u00d7dimensions. Larger min_samples = more conservative. If DBSCAN parameter tuning is frustrating, try HDBSCAN\u2014it removes the eps parameter entirely.</p> <pre><code>from sklearn.cluster import DBSCAN\n\ndbscan = DBSCAN(eps=0.5, min_samples=5)\nlabels = dbscan.fit_predict(X_scaled)\n\nn_clusters = len(set(labels)) - (1 if -1 in labels else 0)\nn_noise = list(labels).count(-1)\nprint(f\"Clusters: {n_clusters}, Noise: {n_noise}\")\n</code></pre> <p>Social network analogy for DBSCAN: Think of data points as people at a party. Core points are \"popular\" people with at least <code>min_samples</code> friends within arm's reach (<code>eps</code>). Border points are \"acquaintances\"\u2014not popular themselves, but friends with at least one popular person. Noise points are \"wallflowers\" standing alone, not connected to any group. A cluster forms when popular people introduce each other: if Alice knows Bob and Bob knows Carol, they're all in the same social circle\u2014even if Alice and Carol never met directly.</p> <p>Numerical Example: DBSCAN Core, Border, and Noise</p> <pre><code>import numpy as np\nfrom sklearn.cluster import DBSCAN\n\n# Create 2 clusters + 3 outliers\nnp.random.seed(42)\ncluster1 = np.random.randn(15, 2) * 0.5 + [0, 0]\ncluster2 = np.random.randn(15, 2) * 0.5 + [4, 0]\noutliers = np.array([[2, 3], [-3, 2], [7, -2]])\nX = np.vstack([cluster1, cluster2, outliers])\n\ndbscan = DBSCAN(eps=1.0, min_samples=5)\nlabels = dbscan.fit_predict(X)\n\nn_clusters = len(set(labels)) - (1 if -1 in labels else 0)\nn_core = len(dbscan.core_sample_indices_)\nn_noise = list(labels).count(-1)\nn_border = len(X) - n_core - n_noise\n\nprint(f\"Clusters found: {n_clusters}\")\nprint(f\"Core points: {n_core}\")\nprint(f\"Border points: {n_border}\")\nprint(f\"Noise points: {n_noise}\")\n</code></pre> <p>Output: <pre><code>Clusters found: 2\nCore points: 29\nBorder points: 1\nNoise points: 3\n</code></pre></p> <p>Interpretation: DBSCAN found 2 clusters automatically (no K specified!). The 3 outliers we planted were correctly identified as noise (label=-1). Core points have \u22655 neighbors within eps=1.0; border points are on cluster edges.</p> <p>Source: <code>slide_computations/module5_examples.py</code> - <code>demo_dbscan_classification()</code></p>"},{"location":"modules/05-unsupervised/#k-means-vs-dbscan","title":"K-Means vs DBSCAN","text":"Aspect K-Means DBSCAN # Clusters Must specify Auto-detected Shapes Spherical Arbitrary Handles noise No Yes (labels -1) Speed Very fast Slower"},{"location":"modules/05-unsupervised/#hdbscan-hierarchical-dbscan","title":"HDBSCAN: Hierarchical DBSCAN","text":"<p>HDBSCAN addresses DBSCAN's sensitivity to the <code>eps</code> parameter:</p> Aspect DBSCAN HDBSCAN Parameters <code>eps</code> and <code>min_samples</code> Just <code>min_samples</code> Cluster densities Assumes uniform Handles varying <p>How it works: 1. Build a hierarchy considering all possible <code>eps</code> values 2. Find stable clusters that persist across <code>eps</code> range 3. Extract flat clustering from the hierarchy</p> <pre><code>import hdbscan\nclusterer = hdbscan.HDBSCAN(min_cluster_size=15)\nlabels = clusterer.fit_predict(X)\n</code></pre>"},{"location":"modules/05-unsupervised/#hierarchical-clustering","title":"Hierarchical Clustering","text":"<p>Build a tree of nested clusters.</p> <p>Agglomerative (bottom-up): 1. Start: Each point is its own cluster 2. Find two closest clusters 3. Merge them 4. Repeat until one cluster remains</p> <p>Linkage methods (distance between clusters): - Single: Minimum distance between any points - Complete: Maximum distance between any points - Average: Average distance between all pairs - Ward: Minimize variance increase when merging</p> <p>Beware the chaining effect: Single linkage can create long \"chains\" that connect distant clusters through a series of close pairs. Imagine two dense clusters with one stray point halfway between them. Single linkage might merge both clusters through that bridge point, even though the clusters themselves are far apart. Ward linkage (default in many packages) is usually the safest choice\u2014it merges clusters that minimize within-cluster variance, producing compact, similar-sized groups.</p> <p>Dendrogram: Tree showing merge history - Y-axis = distance at which clusters merged - Cut at any height to get that many clusters</p> <pre><code>from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n\nZ = linkage(X_scaled, method='ward')\n\nplt.figure(figsize=(12, 6))\ndendrogram(Z)\nplt.xlabel('Sample Index')\nplt.ylabel('Distance')\n\n# Cut to get 3 clusters\nlabels = fcluster(Z, t=3, criterion='maxclust')\n</code></pre>"},{"location":"modules/05-unsupervised/#common-misconceptions","title":"Common Misconceptions","text":"Misconception Reality \"There's one correct number of clusters\" Clustering is exploratory. Multiple valid solutions exist. Business context matters. \"K-means always works\" Fails on complex shapes, varying densities, outliers. \"Silhouette = 0.9 means perfect clustering\" Silhouette measures separation, not business meaning. \"More clusters is always better\" Reduces variance but may not be useful. Aim for interpretable segments."},{"location":"modules/05-unsupervised/#52-dimensionality-reduction","title":"5.2 Dimensionality Reduction","text":""},{"location":"modules/05-unsupervised/#why-reduce-dimensions","title":"Why Reduce Dimensions?","text":"<p>The curse of dimensionality: - High-dimensional spaces are sparse - Distance metrics become less meaningful - Models overfit more easily</p> <p>Benefits: - Visualization: Can't plot 50 dimensions. Can plot 2. - Noise reduction: Remove uninformative dimensions - Faster training: Fewer features - Feature extraction: Create meaningful composites</p> <p>Are we losing important information? You ARE losing information\u2014the question is signal vs. noise. If 10 components capture 95% of variance, the last 40 combined contribute 5% (mostly noise). Verify by comparing model performance with/without reduction. Caveats: rare but important patterns may have low variance; PCA doesn't know your target, so captured variance might not be predictive.</p>"},{"location":"modules/05-unsupervised/#principal-component-analysis-pca","title":"Principal Component Analysis (PCA)","text":"<p>Find new axes (principal components) that: 1. Are linear combinations of original features 2. Capture maximum variance 3. Are orthogonal (uncorrelated)</p> <p>Algorithm: 1. Center the data (subtract mean) 2. Find direction of maximum variance \u2192 PC1 3. Find direction of max remaining variance, perpendicular to PC1 \u2192 PC2 4. Continue...</p> <p>PCA as rotation: Imagine your data forms an elongated cloud tilted at 45\u00b0. The original X and Y axes don't align with the cloud's natural shape. PCA rotates the coordinate system so PC1 runs along the cloud's longest axis (maximum spread) and PC2 runs perpendicular (maximum remaining spread). You haven't changed the data\u2014just how you describe it. Now most information is concentrated in the first few axes.</p> <pre><code>from sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\n# Variance explained\nprint(f\"Variance explained: {pca.explained_variance_ratio_}\")\nprint(f\"Total: {sum(pca.explained_variance_ratio_):.2%}\")\n</code></pre>"},{"location":"modules/05-unsupervised/#choosing-number-of-components","title":"Choosing Number of Components","text":"<ul> <li>Scree plot: Variance explained vs component number</li> <li>Cumulative variance: Keep enough for 80-95%</li> <li>Kaiser criterion: Keep components with eigenvalue &gt; 1</li> </ul> <p>Thinking about variance as \"information\": Variance represents how much features differ across observations\u2014their information content. If a feature is constant, it tells you nothing (zero variance). PCA finds the axes where data varies most and preserves that variability. The 95% threshold is like \"keep 95% of the signal, discard 5% noise.\" It's a lossy compression, like JPEG\u2014you lose some detail but preserve the recognizable structure.</p> <pre><code>pca_full = PCA()\npca_full.fit(X_scaled)\n\n# Cumulative variance plot\nplt.plot(range(1, len(pca_full.explained_variance_ratio_) + 1),\n         np.cumsum(pca_full.explained_variance_ratio_), 'bo-')\nplt.axhline(y=0.95, color='r', linestyle='--')\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Variance')\n</code></pre> <p>Numerical Example: PCA Variance Explained</p> <pre><code>import numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n# Create data with 3 latent factors + noise (10 features total)\nnp.random.seed(42)\nn = 200\nz1, z2, z3 = np.random.randn(3, n)  # 3 underlying factors\n\nX = np.column_stack([\n    z1, 0.8*z1 + 0.2*z2, z2, 0.5*z2 + 0.5*z3, z3,  # Signal\n    0.7*z1 + 0.3*z3, 0.4*z1 + 0.4*z2 + 0.2*z3,     # Mixed\n    0.3*z1 + 0.3*z2 + 0.4*z3,                       # Mixed\n    0.1*np.random.randn(n), 0.1*np.random.randn(n) # Noise\n])\n\npca = PCA()\npca.fit(StandardScaler().fit_transform(X))\n\ncumulative = np.cumsum(pca.explained_variance_ratio_)\nfor i, (var, cum) in enumerate(zip(pca.explained_variance_ratio_, cumulative)):\n    print(f\"PC{i+1}: {var:5.1%} (cumulative: {cum:5.1%})\")\n</code></pre> <p>Output: <pre><code>PC1: 45.6% (cumulative: 45.6%)\nPC2: 21.5% (cumulative: 67.2%)\nPC3: 12.5% (cumulative: 79.7%)\nPC4: 10.5% (cumulative: 90.2%)\nPC5:  8.9% (cumulative: 99.1%)  \u2190 95% threshold crossed\nPC6:  0.3% (cumulative: 99.3%)\n...\n</code></pre></p> <p>Interpretation: The first 3 components capture ~80% of variance\u2014matching our 3 underlying factors. Components 6-10 capture &lt;1% combined (the noise). We could reduce 10 dimensions \u2192 5 with minimal information loss.</p> <p>Source: <code>slide_computations/module5_examples.py</code> - <code>demo_pca_variance_explained()</code></p>"},{"location":"modules/05-unsupervised/#interpreting-pca-loadings","title":"Interpreting PCA Loadings","text":"<p>Loadings show how original features contribute to each component:</p> Feature PC1 PC2 Income 0.8 0.1 Age 0.7 -0.2 Spending 0.6 0.8 <p>Interpretation: - PC1 loads on Income, Age, Spending \u2192 \"Overall affluence\" - PC2 loads mainly on Spending \u2192 \"Spending tendency\"</p> <p>Naming is subjective: Component naming is interpretation, not discovery. Two analysts might name the same loadings differently (\"Wealth\" vs \"Financial Stability\" vs \"Affluence Score\"). Report actual loadings alongside interpretation, acknowledge subjectivity, and validate with domain experts. If you can't tell a coherent story, the component may not be meaningfully interpretable.</p> <pre><code># Loadings are in components_ (rows = components, cols = features)\nloadings = pca.components_\n\nimport polars as pl\nloadings_df = pl.DataFrame(\n    loadings,\n    schema=feature_names\n).with_row_index(\"PC\")\n</code></pre> <p>Numerical Example: PCA Loadings Interpretation</p> <pre><code>import numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n# Customer data: 3 financial + 3 engagement features\nnp.random.seed(42)\nn = 200\nwealth = np.random.randn(n)      # Latent factor 1\nengagement = np.random.randn(n)  # Latent factor 2\n\nX = np.column_stack([\n    50000 + 20000*wealth,    # Income\n    10000 + 8000*wealth,     # Savings\n    200000 + 80000*wealth,   # Home_Value\n    20 + 10*engagement,      # Transactions\n    10 + 8*engagement,       # Logins\n    2 + 3*engagement,        # Support_Calls\n])\nfeatures = ['Income', 'Savings', 'Home_Value', 'Transactions', 'Logins', 'Support_Calls']\n\npca = PCA(n_components=2)\npca.fit(StandardScaler().fit_transform(X))\n\nprint(\"Feature         PC1      PC2\")\nfor name, l1, l2 in zip(features, pca.components_[0], pca.components_[1]):\n    print(f\"{name:15} {l1:7.3f}  {l2:7.3f}\")\n</code></pre> <p>Output: <pre><code>Feature         PC1      PC2\nIncome           0.425   -0.390\nSavings          0.421   -0.396\nHome_Value       0.420   -0.397\nTransactions     0.390    0.425\nLogins           0.397    0.420\nSupport_Calls    0.396    0.420\n</code></pre></p> <p>Interpretation: PC1 loads positively on ALL features but more heavily on financial ones \u2192 \"Overall Customer Value.\" PC2 contrasts financial (negative) with engagement (positive) \u2192 \"Activity vs. Wealth.\" A customer high on PC2 is very active but not wealthy; high on PC1 is valuable overall.</p> <p>Source: <code>slide_computations/module5_examples.py</code> - <code>demo_pca_loadings()</code></p>"},{"location":"modules/05-unsupervised/#t-sne-for-visualization","title":"t-SNE for Visualization","text":"<p>PCA assumes linear relationships. t-SNE handles non-linear manifolds.</p> <p>Goal: Preserve local neighborhoods in 2D - Points close in high-D stay close - Points far apart can move freely</p> <p>Key parameter: <code>perplexity</code> (~5-50) - Roughly expected number of neighbors - Try multiple values</p> <p>Perplexity as binoculars: Low perplexity (5-10) is like zooming in\u2014t-SNE focuses on very local neighborhoods, which can fragment single clusters into multiple blobs. High perplexity (50-100) is like zooming out\u2014t-SNE considers more neighbors, preserving global structure but potentially merging distinct clusters. The default of 30 usually balances these tradeoffs. Always try at least 3 values (e.g., 10, 30, 50). If results change dramatically, the structure may be ambiguous.</p> <p>Critical caveats: - Stochastic\u2014different runs give different results - Cluster sizes are meaningless (distances distorted) - Can create false patterns in random data - Slow for large datasets - ONLY for visualization, NOT preprocessing!</p> <pre><code>from sklearn.manifold import TSNE\n\ntsne = TSNE(n_components=2, perplexity=30, random_state=42)\nX_tsne = tsne.fit_transform(X_scaled)\n</code></pre> <p>Numerical Example: t-SNE Perplexity Sensitivity</p> <pre><code>from sklearn.manifold import TSNE\nfrom sklearn.datasets import make_blobs\nfrom sklearn.metrics import silhouette_score\n\n# 300 points in 5 clusters\nX, labels = make_blobs(n_samples=300, centers=5, cluster_std=1.0, random_state=42)\n\nprint(\"Perplexity   Silhouette (2D)   Notes\")\nfor perp in [5, 30, 100]:\n    tsne = TSNE(n_components=2, perplexity=perp, random_state=42)\n    X_2d = tsne.fit_transform(X)\n    sil = silhouette_score(X_2d, labels)\n    notes = {5: \"Local focus\", 30: \"Balanced\", 100: \"Global focus\"}[perp]\n    print(f\"    {perp:3}           {sil:.3f}        {notes}\")\n</code></pre> <p>Output: <pre><code>Perplexity   Silhouette (2D)   Notes\n      5           0.620        Local focus\n     30           0.775        Balanced\n    100           0.719        Global focus\n</code></pre></p> <p>Interpretation: Perplexity 30 gives best cluster separation in 2D. Low perplexity (5) fragments clusters; high perplexity (100) loses local detail. The \"right\" perplexity depends on your data\u2014always try multiple values.</p> <p>Source: <code>slide_computations/module5_examples.py</code> - <code>demo_tsne_perplexity()</code></p> <p>Never use t-SNE coordinates as features for a classifier. Distances are distorted. Use PCA for preprocessing.</p> <p>Trusting t-SNE clusters: t-SNE preserves local neighborhoods but distorts global distances, cluster sizes, and densities. To avoid being fooled: run multiple times with different seeds/perplexity, validate with clustering on the original high-D data (if K-means finds no structure there, t-SNE may be misleading), and check perplexity sensitivity. t-SNE is for visualization and hypothesis generation\u2014always verify clusters with methods on the original data.</p>"},{"location":"modules/05-unsupervised/#umap","title":"UMAP","text":"<p>UMAP (Uniform Manifold Approximation and Projection) is often better than t-SNE:</p> <ul> <li>Faster, especially for large data</li> <li>Preserves global structure better</li> <li>Can be used for preprocessing (not just visualization)</li> <li>More reproducible</li> </ul> <pre><code>import umap\n\nreducer = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1)\nX_umap = reducer.fit_transform(X_scaled)\n</code></pre>"},{"location":"modules/05-unsupervised/#method-comparison","title":"Method Comparison","text":"Method Speed Global Structure Use For PCA Fast Preserved Preprocessing, visualization t-SNE Slow Lost Visualization only UMAP Medium Partially preserved Both <p>Rule of thumb: - PCA for preprocessing and quick visualization - t-SNE or UMAP for beautiful visualizations - UMAP if you want the best of both worlds</p> <p>Choosing a method\u2014decision flowchart: 1. Need features for a downstream model? \u2192 Use PCA (stable, invertible, fast) 2. Just need a 2D visualization? \u2192 Try t-SNE or UMAP first 3. Data has non-linear structure? \u2192 t-SNE/UMAP will outperform PCA 4. Dataset is large (&gt;10K points)? \u2192 UMAP is much faster than t-SNE 5. Need reproducibility? \u2192 PCA (deterministic) or UMAP (more stable than t-SNE)</p> <p>Numerical Example: PCA vs t-SNE on Structured Data</p> <pre><code>from sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.datasets import make_moons, make_blobs\nfrom sklearn.metrics import silhouette_score\n\n# Non-linear data: two interlocking half-moons\nX_moons, labels = make_moons(n_samples=300, noise=0.05, random_state=42)\n\n# PCA projection\nX_pca = PCA(n_components=2).fit_transform(X_moons)\nsil_pca = silhouette_score(X_pca, labels)\n\n# t-SNE projection\nX_tsne = TSNE(n_components=2, perplexity=30, random_state=42).fit_transform(X_moons)\nsil_tsne = silhouette_score(X_tsne, labels)\n\nprint(f\"Two Moons (non-linear):\")\nprint(f\"  PCA silhouette:   {sil_pca:.3f}  (moons overlap)\")\nprint(f\"  t-SNE silhouette: {sil_tsne:.3f}  (moons separated)\")\n</code></pre> <p>Output: <pre><code>Two Moons (non-linear):\n  PCA silhouette:   0.331  (moons overlap)\n  t-SNE silhouette: 0.646  (moons separated)\n</code></pre></p> <p>Interpretation: PCA's linear projection fails on the curved \"two moons\" structure\u2014the classes overlap in 2D. t-SNE's non-linear approach separates them clearly. For linear cluster structures (spherical blobs), both methods work well. Use PCA for preprocessing; use t-SNE/UMAP for visualization.</p> <p>Source: <code>slide_computations/module5_examples.py</code> - <code>demo_pca_vs_tsne()</code></p>"},{"location":"modules/05-unsupervised/#mnist-example","title":"MNIST Example","text":"<ul> <li>Original: 784 dimensions (28\u00d728 pixels)</li> <li>PCA to 2D: Blurry separation</li> <li>t-SNE/UMAP to 2D: Clear digit clusters!</li> </ul> <p>Why? The digit manifold is non-linear. PCA's linear assumption can't capture it. t-SNE and UMAP can.</p>"},{"location":"modules/05-unsupervised/#common-misconceptions_1","title":"Common Misconceptions","text":"Misconception Reality \"PCA finds the most important features\" PCA finds linear combinations. Components may not correspond to individual features. \"t-SNE cluster sizes are meaningful\" t-SNE distorts distances. A big cluster in t-SNE might be same size as small one in reality. \"More components = better\" More preserves more info but may include noise. Choose based on task. \"Dimensionality reduction always helps ML models\" Sometimes original features are better. Compare performance."},{"location":"modules/05-unsupervised/#reflection-questions","title":"Reflection Questions","text":"<ol> <li> <p>You're segmenting customers for marketing. K-means suggests 5 clusters, but your team can only create 3 campaigns. What do you do?</p> </li> <li> <p>Your clustering puts 95% of data in one cluster and creates 4 tiny ones. Is this a problem? What might cause this?</p> </li> <li> <p>When would you choose DBSCAN over K-means? Give a business example.</p> </li> <li> <p>A colleague says they found \"the optimal number of clusters.\" Why should you be skeptical?</p> </li> <li> <p>PCA on customer data shows PC1 explains 80% of variance. Should you only use PC1?</p> </li> <li> <p>You run t-SNE twice and get different-looking plots. Is one wrong?</p> </li> <li> <p>A colleague says \"UMAP proves our data has 5 clusters.\" What's wrong with this statement?</p> </li> </ol>"},{"location":"modules/05-unsupervised/#practice-problems","title":"Practice Problems","text":"<ol> <li> <p>Given cluster assignments, calculate silhouette score by hand for a small example</p> </li> <li> <p>Interpret PCA loadings for a business dataset (name the components)</p> </li> <li> <p>Choose between K-means and DBSCAN for different data scenarios</p> </li> <li> <p>Explain why t-SNE shouldn't be used for preprocessing</p> </li> <li> <p>For customer data with features {Income, Age, Transactions, Days_Since_Purchase}, describe what PC1 and PC2 might represent</p> </li> </ol>"},{"location":"modules/05-unsupervised/#chapter-summary","title":"Chapter Summary","text":"<p>Six key takeaways from Module 5:</p> <ol> <li> <p>Unsupervised learning discovers structure without labels</p> </li> <li> <p>K-means is fast but needs spherical clusters and specified K</p> </li> <li> <p>DBSCAN handles arbitrary shapes and identifies outliers</p> </li> <li> <p>Silhouette scores measure cluster quality (but not business meaning)</p> </li> <li> <p>PCA finds linear combinations that maximize variance</p> </li> <li> <p>t-SNE/UMAP reveal non-linear structure\u2014use t-SNE for visualization only!</p> </li> </ol>"},{"location":"modules/05-unsupervised/#whats-next","title":"What's Next","text":"<p>In Module 6, we tackle Neural Networks Fundamentals: - Perceptrons and multi-layer networks - Activation functions - Backpropagation - Deep learning basics</p> <p>Here's an interesting connection: dimensionality reduction is related to neural network feature learning. Neural networks automatically learn compressed representations of inputs\u2014that's partly why deep learning works so well.</p>"},{"location":"modules/06-neural-networks/","title":"Module 6: Neural Networks Fundamentals","text":""},{"location":"modules/06-neural-networks/#introduction","title":"Introduction","text":"<p>Today we cross a threshold\u2014we're entering deep learning.</p> <p>Everything we've covered so far\u2014regression, classification, ensemble methods, unsupervised learning\u2014those are \"classical\" machine learning. Powerful, interpretable, widely used. But deep learning has transformed what's possible with images, text, audio, and complex patterns.</p> <p>Here's the key insight: neural networks are not magic. They're built on the same principles we've been learning. Remember gradient descent from Module 2? You'll see it again. Remember the bias-variance tradeoff from Module 1? It applies here too.</p> <p>What makes neural networks special is their ability to learn hierarchical representations\u2014layer by layer, from simple patterns to complex concepts.</p> <p>Hierarchical learning is automatic: We design the architecture and loss function; the specific representations are discovered, not designed. Through backpropagation, weights organize themselves to extract useful features. Researchers visualizing trained networks find edges in layer 1, textures in layer 2, object parts in later layers\u2014this emerges from optimization as the most efficient solution.</p>"},{"location":"modules/06-neural-networks/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, you should be able to:</p> <ol> <li>Explain the historical development and architecture of neural networks</li> <li>Describe the components of a neural network (weights, biases, activations)</li> <li>Understand backpropagation and gradient-based optimization</li> <li>Implement a simple neural network in PyTorch</li> <li>Train and evaluate networks on classification tasks</li> <li>Apply regularization techniques to prevent overfitting</li> </ol>"},{"location":"modules/06-neural-networks/#61-introduction-to-neural-networks","title":"6.1 Introduction to Neural Networks","text":""},{"location":"modules/06-neural-networks/#three-components-neural-networks","title":"Three Components: Neural Networks","text":"<p>The same framework applies here:</p> Component Neural Network Decision Model Stacked layers with non-linear activations Quality Measure Cross-entropy (classification) or MSE (regression) Update Method Backpropagation + gradient descent (SGD, Adam) <p>In Module 2, you implemented gradient descent for two parameters (\\(\\beta_0\\), \\(\\beta_1\\)). Neural networks apply the same idea to millions of parameters. The algorithm is the same; the scale is different.</p>"},{"location":"modules/06-neural-networks/#historical-context","title":"Historical Context","text":"<p>1957: Frank Rosenblatt invents the Perceptron\u2014a single layer of weights that could learn simple patterns. The New York Times predicted thinking machines within a decade.</p> <p>1969: Minsky and Papert publish \"Perceptrons,\" proving single-layer networks can't learn XOR. Funding dries up. First \"AI Winter.\"</p> <p>1986: Rumelhart, Hinton, and Williams popularize backpropagation\u2014making deep network training practical.</p> <p>2012: AlexNet wins ImageNet by a massive margin, demonstrating that deep networks trained on GPUs could dramatically outperform traditional methods.</p> <p>Today: Transformers, GPT, and large language models.</p> <p>The lesson: Neural networks have existed for 70 years. What changed is data, compute, and better training techniques.</p> <p>Why deep learning works now: Three factors combined: (1) Data\u2014ImageNet provided 14M labeled images; the internet generated billions of documents. (2) GPUs\u2014parallel operations for matrix multiplication, turning weeks into hours. (3) Better techniques\u2014ReLU solved vanishing gradients, dropout provided regularization, batch norm stabilized training, Adam made optimization robust. AlexNet (2012) combined all three and won ImageNet decisively.</p>"},{"location":"modules/06-neural-networks/#the-xor-problem","title":"The XOR Problem","text":"<p>The XOR function outputs 1 if exactly one input is 1:</p> x\u2081 x\u2082 XOR 0 0 0 0 1 1 1 0 1 1 1 0 <p>A single-layer perceptron can only learn linearly separable patterns. XOR isn't linearly separable\u2014you can't draw a single straight line to separate the 1s from the 0s.</p> <p>The solution: Add a hidden layer. The hidden layer \"transforms\" the space to make the problem linearly separable.</p> <p>How the hidden layer transforms space: Each neuron computes a weighted sum (defining a hyperplane) plus activation (bending space around it). For XOR, one neuron might learn \"x\u2081 + x\u2082 &gt; 0.5\" and another \"x\u2081 + x\u2082 &lt; 1.5\"\u2014together creating a representation where (0,1) and (1,0) map similarly while (0,0) and (1,1) map differently. The output layer can now draw a line in this transformed space.</p> <p>Numerical Example: XOR with a Hidden Layer</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\ntorch.manual_seed(42)\n\n# XOR data\nX = torch.tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])\ny = torch.tensor([[0.0], [1.0], [1.0], [0.0]])\n\n# Network: 2 inputs -&gt; 4 hidden (tanh) -&gt; 1 output (sigmoid)\nclass XORNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.hidden = nn.Linear(2, 4)\n        self.output = nn.Linear(4, 1)\n\n    def forward(self, x):\n        h = torch.tanh(self.hidden(x))\n        return torch.sigmoid(self.output(h))\n\nmodel = XORNet()\noptimizer = optim.Adam(model.parameters(), lr=0.5)\ncriterion = nn.BCELoss()\n\nfor epoch in range(2000):\n    optimizer.zero_grad()\n    loss = criterion(model(X), y)\n    loss.backward()\n    optimizer.step()\n\n# Test\nmodel.eval()\nwith torch.no_grad():\n    for i in range(4):\n        pred = model(X[i:i+1]).item()\n        print(f\"({X[i,0]:.0f}, {X[i,1]:.0f}) -&gt; {pred:.3f} -&gt; {1 if pred &gt; 0.5 else 0}\")\n</code></pre> <p>Output: <pre><code>(0, 0) -&gt; 0.000 -&gt; 0\n(0, 1) -&gt; 1.000 -&gt; 1\n(1, 0) -&gt; 1.000 -&gt; 1\n(1, 1) -&gt; 0.000 -&gt; 0\n</code></pre></p> <p>Interpretation: The network learns XOR perfectly. The hidden layer transforms the 2D input space so that (0,0) and (1,1) map to one region while (0,1) and (1,0) map to another\u2014making the problem linearly separable for the output layer.</p> <p>Source: <code>slide_computations/module6_examples.py</code> - <code>demo_xor_hidden_layer()</code></p>"},{"location":"modules/06-neural-networks/#multi-layer-perceptron-mlp-architecture","title":"Multi-Layer Perceptron (MLP) Architecture","text":"<p>Reading the diagram: This network has three input neurons (x1, x2, x3) shown in blue on the left, two hidden layers with four neurons each shown in purple in the middle, and a single output neuron (\u0177) shown in gray on the right. Every neuron in one layer connects to every neuron in the next layer\u2014these gray lines represent the weights that the network learns during training. Information flows left to right: inputs enter, get transformed through hidden layers, and produce a prediction. The \"depth\" of this network is 2 (two hidden layers), and the \"width\" of each hidden layer is 4. Notice that the input layer is not counted when describing network depth\u2014it's just the raw data entry point.</p> <p>Terminology: - Input layer: Raw features (not counted in \"layers\") - Hidden layers: Intermediate representations - Output layer: Final predictions - Depth: Number of hidden layers - Width: Neurons per layer</p> Network Type Hidden Layers Typical Use Shallow 1-2 Simple patterns Deep 3+ Complex patterns Very Deep 50+ State-of-the-art"},{"location":"modules/06-neural-networks/#why-depth-matters","title":"Why Depth Matters","text":"<p>Each layer learns more abstract features: - Layer 1: Edges, simple patterns - Layer 2: Textures, shapes - Layer 3: Object parts - Layer N: Complete concepts</p> <p>Deep networks learn hierarchical representations that match how complex patterns are actually structured.</p>"},{"location":"modules/06-neural-networks/#universal-approximation-theorem","title":"Universal Approximation Theorem","text":"<p>A feedforward network with a single hidden layer can approximate any continuous function, given enough neurons.</p> <p>What it means: With enough neurons, any reasonable function can be approximated.</p> <p>What it doesn't mean: It doesn't tell you how many neurons you need, how to find the weights, or that one layer is optimal.</p> <p>In practice, deep networks represent the same functions more efficiently than wide shallow ones.</p> <p>Why depth over width? A function that a 10-layer network represents with 1,000 neurons might require millions in a single layer. Complex patterns are compositional (faces = eyes + nose + mouth; eyes = curves + colors)\u2014deep networks represent this hierarchy naturally. Shallow networks must learn all combinations directly, which explodes exponentially. Deeper architectures outperform shallow ones with the same parameter count on complex benchmarks.</p>"},{"location":"modules/06-neural-networks/#network-components","title":"Network Components","text":"<p>1. Weights (W): Learnable parameters connecting neurons 2. Biases (b): Learnable offset per neuron 3. Activation functions: Non-linear transformations</p> <p>The computation at each neuron:</p> \\[output = activation(Wx + b)\\]"},{"location":"modules/06-neural-networks/#activation-functions","title":"Activation Functions","text":"<p>ReLU (Rectified Linear Unit) \u2014 most common:</p> \\[\\text{ReLU}(x) = \\max(0, x)\\] <ul> <li>Simple: negative \u2192 0, positive \u2192 pass through</li> <li>Default choice for hidden layers</li> <li>Helps with vanishing gradients</li> </ul> <p>Sigmoid:</p> \\[\\sigma(x) = \\frac{1}{1 + e^{-x}}\\] <ul> <li>Output between 0 and 1</li> <li>Good for binary output layer</li> <li>Suffers from vanishing gradients in deep networks</li> </ul> <p>Softmax (for multi-class):</p> \\[\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}\\] <ul> <li>Outputs sum to 1 (probabilities)</li> <li>Used in final layer for classification</li> </ul>"},{"location":"modules/06-neural-networks/#why-non-linear-activations","title":"Why Non-linear Activations?","text":"<p>Without non-linearity:</p> \\[Layer_2(Layer_1(x)) = W_2(W_1 x) = (W_2 W_1)x = Wx\\] <p>Multiple linear layers = one linear layer!</p> <p>No matter how many linear layers you stack, the result is still linear. Non-linear activations allow each layer to transform representations in ways linear functions can't.</p> <p>Why ReLU works: (1) Vanishing gradient solution\u2014sigmoid's gradient approaches zero for large inputs; ReLU has gradient 1 for positives, letting gradients pass through unchanged. (2) Computational efficiency\u2014just max(0,x), orders of magnitude faster than sigmoid. (3) Sparse activation\u201450% of neurons may be \"dead\" for any input, improving efficiency. Despite being piecewise linear, stacking many ReLUs can approximate any continuous function.</p> <p>Understanding \"dead\" ReLU neurons: When a neuron's input is negative, ReLU outputs 0 and its gradient is also 0. This means negative-input neurons don't contribute to predictions or learning for that example. While this sounds problematic, it's actually beneficial: (1) it creates sparsity\u2014only a subset of neurons activate for any given input, making computation efficient, and (2) different inputs activate different neuron subsets, so the network implicitly learns specialized sub-networks for different patterns. However, if a neuron's weights drift so that it always receives negative inputs (for all training examples), it becomes permanently \"dead\" and stops learning. This is the \"dying ReLU\" problem, which techniques like Leaky ReLU address.</p> <p>Numerical Example: ReLU vs Sigmoid Gradients</p> <pre><code>import numpy as np\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef sigmoid_gradient(x):\n    s = sigmoid(x)\n    return s * (1 - s)\n\ndef relu_gradient(x):\n    return 1.0 if x &gt; 0 else 0.0\n\n# Simulate gradient flowing backward through 10 layers\n# (assuming all neurons in saturated sigmoid region, z=2)\nprint(\"Gradient flowing backward through 10 layers:\")\nprint(f\"{'Layer':&gt;6} {'Sigmoid grad':&gt;15} {'ReLU grad':&gt;15}\")\n\nsigmoid_grad = 1.0\nrelu_grad = 1.0\nfor layer in range(10, 0, -1):\n    print(f\"{layer:&gt;6} {sigmoid_grad:&gt;15.6f} {relu_grad:&gt;15.1f}\")\n    sigmoid_grad *= sigmoid_gradient(2.0)  # Saturated region\n    relu_grad *= relu_gradient(2.0)        # Positive region\n\nprint(f\"\\nAfter 10 layers: sigmoid={sigmoid_grad:.2e}, ReLU={relu_grad:.1f}\")\n</code></pre> <p>Output: <pre><code>Gradient flowing backward through 10 layers:\n Layer    Sigmoid grad        ReLU grad\n    10        1.000000             1.0\n     9        0.104994             1.0\n     8        0.011024             1.0\n     7        0.001157             1.0\n     6        0.000122             1.0\n     5        0.000013             1.0\n     4        0.000001             1.0\n     3        0.000000             1.0\n     2        0.000000             1.0\n     1        0.000000             1.0\n\nAfter 10 layers: sigmoid=1.63e-10, ReLU=1.0\n</code></pre></p> <p>Interpretation: With sigmoid activations, the gradient shrinks by ~10x at each layer. After 10 layers, it's essentially zero (1.63\u00d710\u207b\u00b9\u2070)\u2014early layers receive no learning signal. ReLU maintains gradient magnitude, enabling training of very deep networks. This is the vanishing gradient problem that plagued early deep learning.</p> <p>Source: <code>slide_computations/module6_examples.py</code> - <code>demo_relu_vs_sigmoid_gradients()</code></p>"},{"location":"modules/06-neural-networks/#parameter-counting","title":"Parameter Counting","text":"<p>For a fully connected layer:</p> \\[Parameters = (input \\times output) + output = weights + biases\\] <p>Example: Network with layers [784, 256, 128, 10] - Layer 1: 784\u00d7256 + 256 = 200,960 - Layer 2: 256\u00d7128 + 128 = 32,896 - Layer 3: 128\u00d710 + 10 = 1,290 - Total: 235,146 parameters</p> <pre><code>def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n</code></pre> <p>Is 10 million parameters a lot? It depends on your data. If you have 1,000 examples and 10 million parameters, you'll overfit. If you have 10 million examples, it's reasonable. The ratio matters.</p> <p>Numerical Example: Parameter Counting Walkthrough</p> <pre><code>def count_params(architecture):\n    \"\"\"Count parameters for a fully connected network.\"\"\"\n    total = 0\n    for i in range(len(architecture) - 1):\n        weights = architecture[i] * architecture[i + 1]\n        biases = architecture[i + 1]\n        total += weights + biases\n        print(f\"Layer {i+1}: {architecture[i]}x{architecture[i+1]} \"\n              f\"= {weights:,} weights + {biases} biases = {weights + biases:,}\")\n    return total\n\n# Three different architectures for MNIST (784 inputs, 10 outputs)\nprint(\"Architecture 1: [784, 256, 128, 10]\")\ntotal1 = count_params([784, 256, 128, 10])\nprint(f\"Total: {total1:,}\\n\")\n\nprint(\"Architecture 2 (deeper): [784, 128, 64, 32, 16, 10]\")\ntotal2 = count_params([784, 128, 64, 32, 16, 10])\nprint(f\"Total: {total2:,}\\n\")\n\nprint(\"Architecture 3 (wider): [784, 512, 10]\")\ntotal3 = count_params([784, 512, 10])\nprint(f\"Total: {total3:,}\")\n</code></pre> <p>Output: <pre><code>Architecture 1: [784, 256, 128, 10]\nLayer 1: 784x256 = 200,704 weights + 256 biases = 200,960\nLayer 2: 256x128 = 32,768 weights + 128 biases = 32,896\nLayer 3: 128x10 = 1,280 weights + 10 biases = 1,290\nTotal: 235,146\n\nArchitecture 2 (deeper): [784, 128, 64, 32, 16, 10]\nLayer 1: 784x128 = 100,352 weights + 128 biases = 100,480\nLayer 2: 128x64 = 8,192 weights + 64 biases = 8,256\nLayer 3: 64x32 = 2,048 weights + 32 biases = 2,080\nLayer 4: 32x16 = 512 weights + 16 biases = 528\nLayer 5: 16x10 = 160 weights + 10 biases = 170\nTotal: 111,514\n\nArchitecture 3 (wider): [784, 512, 10]\nLayer 1: 784x512 = 401,408 weights + 512 biases = 401,920\nLayer 2: 512x10 = 5,120 weights + 10 biases = 5,130\nTotal: 407,050\n</code></pre></p> <p>Interpretation: The first layer (connecting to high-dimensional input) dominates the parameter count. Deeper networks can actually have fewer parameters than wide shallow ones while achieving better representational power. Architecture 2 has 5 layers but only 111K parameters, while architecture 3 has just 2 layers but 407K parameters.</p> <p>Source: <code>slide_computations/module6_examples.py</code> - <code>demo_parameter_counting()</code></p>"},{"location":"modules/06-neural-networks/#common-misconceptions","title":"Common Misconceptions","text":"Misconception Reality \"Deep learning is different from ML\" Deep learning IS machine learning. Same principles apply. \"More layers always better\" Deeper = harder to train, can overfit. Match depth to complexity. \"Neural networks are black boxes\" Many interpretability tools exist. The criticism is overstated. \"Need millions of data points\" Transfer learning enables NNs with small datasets."},{"location":"modules/06-neural-networks/#62-training-neural-networks","title":"6.2 Training Neural Networks","text":""},{"location":"modules/06-neural-networks/#loss-functions","title":"Loss Functions","text":"<p>Regression \u2014 Mean Squared Error (MSE):</p> \\[L = \\frac{1}{n}\\sum(y_i - \\hat{y}_i)^2\\] <p>Binary Classification \u2014 Binary Cross-Entropy:</p> \\[L = -\\frac{1}{n}\\sum[y\\log(\\hat{y}) + (1-y)\\log(1-\\hat{y})]\\] <p>Multi-class \u2014 Cross-Entropy:</p> \\[L = -\\frac{1}{n}\\sum_{i}\\sum_{c} y_{ic}\\log(\\hat{y}_{ic})\\] <p>Why cross-entropy? The log function severely penalizes confident wrong predictions: - \\(\\log(1) = 0\\) \u2014 no penalty for correct confidence - \\(\\log(0.5) \\approx -0.69\\) \u2014 moderate penalty - \\(\\log(0.01) \\approx -4.6\\) \u2014 severe penalty</p> <p>The information-theoretic intuition: Cross-entropy measures \"surprise.\" If you're 99% confident an email is spam and it turns out to be legitimate, that's very surprising\u2014and the network should pay a heavy penalty for that confident mistake. MSE treats all errors linearly, but cross-entropy's log penalty means confident-wrong is exponentially worse than uncertain-wrong. This provides much stronger gradients to fix the most problematic predictions.</p> <p>Numerical Example: Cross-Entropy vs MSE for Classification</p> <pre><code>import numpy as np\n\ndef mse_gradient(y_true, y_pred):\n    return -2 * (y_true - y_pred)\n\ndef cross_entropy_gradient(y_true, y_pred):\n    eps = 1e-10\n    return -(y_true / (y_pred + eps)) + (1 - y_true) / (1 - y_pred + eps)\n\n# True label is 1 (positive class), compare gradients at different predictions\ny_true = 1.0\npredictions = [0.99, 0.9, 0.5, 0.1, 0.01]\n\nprint(f\"True label: {y_true} (positive class)\")\nprint(f\"{'Prediction':&gt;12} {'MSE Grad':&gt;12} {'CE Grad':&gt;12}\")\nfor p in predictions:\n    print(f\"{p:&gt;12.2f} {mse_gradient(y_true, p):&gt;12.2f} \"\n          f\"{cross_entropy_gradient(y_true, p):&gt;12.2f}\")\n</code></pre> <p>Output: <pre><code>True label: 1.0 (positive class)\n  Prediction     MSE Grad      CE Grad\n        0.99        -0.02        -1.01\n        0.90        -0.20        -1.11\n        0.50        -1.00        -2.00\n        0.10        -1.80       -10.00\n        0.01        -1.98      -100.00\n</code></pre></p> <p>Interpretation: When the model predicts 0.01 for a true positive (confidently wrong), cross-entropy provides a gradient of -100 while MSE gives only -1.98. This 50x stronger signal means cross-entropy can fix catastrophic mistakes much faster. MSE's gradients plateau near the extremes, making it sluggish at correcting confident errors.</p> <p>Source: <code>slide_computations/module6_examples.py</code> - <code>demo_cross_entropy_vs_mse()</code></p>"},{"location":"modules/06-neural-networks/#backpropagation","title":"Backpropagation","text":"<p>The algorithm that makes deep learning possible.</p> <ol> <li>Forward pass: Compute predictions</li> <li>Compute loss: How wrong are we?</li> <li>Backward pass: Compute gradients using chain rule</li> <li>Update: Adjust weights</li> </ol> <p>The chain rule lets us compute how each weight contributed to error, layer by layer, from output back to input.</p> <p>Why gradient computation is fast: Backpropagation reuses computations\u2014when computing gradients for layer 5, you reuse gradient info from layers 6-10. Total cost is ~2\u00d7 the forward pass, O(n) in weights. GPUs parallelize matrix multiplications across thousands of cores. Processing 64 examples in parallel takes almost the same time as 1. A network with 100M parameters takes seconds per batch on modern GPUs.</p> <p>Key point: PyTorch does this automatically!</p> <pre><code>loss.backward()   # Computes all gradients\noptimizer.step()  # Updates all parameters\n</code></pre> <p>One line computes gradients. One line updates weights.</p> <p>Numerical Example: Backpropagation by Hand</p> <p>Let's trace a single training step through a minimal network: 1 input \u2192 1 hidden (ReLU) \u2192 1 output (sigmoid).</p> <pre><code>Initial: w1=0.5, b1=0.1, w2=0.8, b2=-0.2\nInput: x=0.5, True label: y=1\n\n--- FORWARD PASS ---\nz1 = w1*x + b1 = 0.5*0.5 + 0.1 = 0.35\nh1 = ReLU(z1) = max(0, 0.35) = 0.35\nz2 = w2*h1 + b2 = 0.8*0.35 + (-0.2) = 0.08\ny_pred = sigmoid(z2) = 0.5200\n\nLoss = -[y*log(y_pred)] = -log(0.52) = 0.6539\n\n--- BACKWARD PASS ---\ndL/dy_pred = -1/y_pred = -1.9231\ndy_pred/dz2 = y_pred*(1-y_pred) = 0.2496\ndL/dz2 = -1.9231 * 0.2496 = -0.4800\n\ndL/dw2 = dL/dz2 * h1 = -0.4800 * 0.35 = -0.1680\ndL/db2 = dL/dz2 = -0.4800\n\ndL/dh1 = dL/dz2 * w2 = -0.4800 * 0.8 = -0.3840\ndh1/dz1 = ReLU'(0.35) = 1.0  (since z1 &gt; 0)\ndL/dz1 = -0.3840 * 1.0 = -0.3840\n\ndL/dw1 = dL/dz1 * x = -0.3840 * 0.5 = -0.1920\ndL/db1 = dL/dz1 = -0.3840\n\n--- UPDATE (lr=0.1) ---\nw1_new = 0.5 - 0.1*(-0.1920) = 0.5192\nw2_new = 0.8 - 0.1*(-0.1680) = 0.8168\n</code></pre> <p>Interpretation: The chain rule propagates error backward through each operation. Negative gradients mean we should increase the weights (moving opposite to the gradient decreases loss). After this single step, all weights increased slightly, which will push y_pred higher toward the true label of 1. PyTorch's <code>loss.backward()</code> computes all these gradients automatically.</p> <p>Source: <code>slide_computations/module6_examples.py</code> - <code>demo_backprop_by_hand()</code></p>"},{"location":"modules/06-neural-networks/#optimization-algorithms","title":"Optimization Algorithms","text":"<p>SGD (Stochastic Gradient Descent):</p> \\[W \\leftarrow W - \\alpha \\cdot \\nabla L\\] <p>Same as Module 2. Simple but can be slow.</p> <p>SGD + Momentum:</p> \\[v \\leftarrow \\beta v + \\nabla L\\] \\[W \\leftarrow W - \\alpha \\cdot v\\] <p>Accumulates velocity in consistent directions. Like a ball rolling downhill.</p> <p>Why momentum helps: Imagine a loss surface shaped like a long, narrow valley. Plain SGD oscillates back and forth across the narrow dimension while making slow progress along the valley floor. Momentum accumulates velocity in the consistent direction (along the valley) while canceling out oscillations (across the valley). It also helps escape shallow local minima and saddle points\u2014the accumulated momentum carries optimization past small bumps that would trap vanilla SGD.</p> <p>Adam (Adaptive Moment Estimation) \u2014 most popular: - Combines momentum with adaptive learning rates - Per-parameter learning rates - Usually works well with defaults</p> <p>How Adam works (simplified): - Track moving average of gradients (momentum) - Track moving average of squared gradients (adapt rates) - Parameters with large gradients get smaller learning rates</p> <p>The intuition behind adaptive rates: Not all parameters need the same learning rate. A weight connected to a frequently-activated feature gets gradients on every batch\u2014it should take smaller steps to avoid overshooting. A weight connected to a rare feature (like an uncommon word in NLP) gets gradients infrequently\u2014when it does get a signal, it should take a larger step to make progress. Adam automatically scales learning rates: divide by the root-mean-square of recent gradients, so high-gradient parameters get smaller effective rates and vice versa.</p> <pre><code>optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n</code></pre> <p>Practical advice: Start with Adam. Try SGD with momentum if you have time to tune.</p>"},{"location":"modules/06-neural-networks/#learning-rate","title":"Learning Rate","text":"<p>The most important hyperparameter.</p> Too High Just Right Too Low Loss explodes Steady decrease Very slow Diverges Converges Gets stuck <p>Tips: - Start with 0.001 for Adam, 0.01 for SGD - If loss explodes: divide by 10 - If loss barely moves: multiply by 3-10 - Use schedulers to reduce rate during training</p> <p>Numerical Example: Learning Rate Effects on Neural Networks</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\ntorch.manual_seed(42)\n\n# Simple regression network\nX = torch.linspace(-2, 2, 100).reshape(-1, 1)\ny = torch.sin(X * 3.14) + torch.randn_like(X) * 0.1\n\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(1, 32)\n        self.fc2 = nn.Linear(32, 1)\n    def forward(self, x):\n        return self.fc2(torch.relu(self.fc1(x)))\n\nfor lr, label in [(0.0001, \"Too small\"), (0.01, \"Good\"), (1.0, \"Too large\")]:\n    torch.manual_seed(42)\n    model = Net()\n    optimizer = optim.SGD(model.parameters(), lr=lr)\n    for _ in range(50):\n        loss = nn.MSELoss()(model(X), y)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    print(f\"lr={lr}: Final loss = {loss.item():.4f} ({label})\")\n</code></pre> <p>Output: <pre><code>lr=0.0001: Final loss = 0.4968 (Too small)\nlr=0.01: Final loss = 0.3692 (Good)\nlr=1.0: Final loss = inf (Too large)\n</code></pre></p> <p>Interpretation: With lr=0.0001, the network barely learns in 50 epochs. With lr=0.01, it converges to a reasonable solution. With lr=1.0, the loss explodes to infinity\u2014the optimizer overshoots so badly that weights become NaN. The fix is simple: if loss explodes, reduce learning rate by 10x.</p> <p>Source: <code>slide_computations/module6_examples.py</code> - <code>demo_learning_rate_effects_nn()</code></p>"},{"location":"modules/06-neural-networks/#batch-size","title":"Batch Size","text":"Variant Batch Size Trade-off Batch GD All data Stable but slow SGD 1 sample Fast but noisy Mini-batch 32-256 Best of both <p>Standard practice: 32, 64, 128, or 256</p> <p>Trade-offs: - Larger: More stable, more memory, may generalize worse - Smaller: Noisier (regularizing), faster per epoch</p>"},{"location":"modules/06-neural-networks/#regularization-dropout","title":"Regularization: Dropout","text":"<p>Randomly zero neurons during training.</p> <pre><code>self.dropout = nn.Dropout(0.5)  # 50% dropout\n</code></pre> <ul> <li>Forces network to not rely on any single neuron</li> <li>Like training an ensemble of sub-networks</li> <li>Only active during training, not inference</li> </ul> <p>Connection to ensembles: Dropout trains many different sub-networks (different neurons dropped each time) and averages at test time. It's bagging for neural networks.</p> <p>How dropout learning works: Each training example sees a different random subset of neurons. Features that depend on one specific neuron won't work consistently (it might be dropped), forcing distributed, robust representations. At test time, ALL neurons are used but scaled by the dropout rate. The ensemble interpretation: training exponentially many sub-networks simultaneously, averaging at test time.</p> <p>Numerical Example: Dropout Effect on Overfitting</p> <pre><code>import torch\nimport torch.nn as nn\n\ntorch.manual_seed(42)\n\n# Small dataset (easy to overfit): 50 train, 200 test, 20 features\nn_train, n_test, n_features = 50, 200, 20\nX_train = torch.randn(n_train, n_features)\ntrue_w = torch.randn(n_features, 1)\ny_train = X_train @ true_w + torch.randn(n_train, 1) * 0.5\nX_test = torch.randn(n_test, n_features)\ny_test = X_test @ true_w + torch.randn(n_test, 1) * 0.5\n\nclass Net(nn.Module):\n    def __init__(self, dropout_rate):\n        super().__init__()\n        self.fc1, self.fc2, self.fc3 = nn.Linear(20, 64), nn.Linear(64, 32), nn.Linear(32, 1)\n        self.dropout = nn.Dropout(dropout_rate)\n    def forward(self, x):\n        x = self.dropout(torch.relu(self.fc1(x)))\n        x = self.dropout(torch.relu(self.fc2(x)))\n        return self.fc3(x)\n\nfor dropout in [0.0, 0.3, 0.5]:\n    torch.manual_seed(42)\n    model = Net(dropout)\n    opt = torch.optim.Adam(model.parameters(), lr=0.01)\n    for _ in range(200):\n        opt.zero_grad()\n        nn.MSELoss()(model(X_train), y_train).backward()\n        opt.step()\n    model.eval()\n    with torch.no_grad():\n        train_mse = nn.MSELoss()(model(X_train), y_train).item()\n        test_mse = nn.MSELoss()(model(X_test), y_test).item()\n    print(f\"Dropout={dropout}: Train MSE={train_mse:.4f}, Test MSE={test_mse:.4f}, Gap={test_mse-train_mse:.4f}\")\n</code></pre> <p>Output: <pre><code>Dropout=0.0: Train MSE=0.0000, Test MSE=3.7633, Gap=3.7633\nDropout=0.3: Train MSE=0.1016, Test MSE=2.6384, Gap=2.5368\nDropout=0.5: Train MSE=0.1672, Test MSE=2.7283, Gap=2.5611\n</code></pre></p> <p>Interpretation: Without dropout, the network achieves near-zero training error but terrible test error (gap of 3.76)\u2014classic overfitting. With dropout=0.3, training error increases slightly but test error drops substantially. The train/test gap shrinks from 3.76 to 2.54, indicating better generalization. Dropout forces the network to learn robust features that don't depend on any single neuron.</p> <p>Source: <code>slide_computations/module6_examples.py</code> - <code>demo_dropout_effect()</code></p>"},{"location":"modules/06-neural-networks/#regularization-batch-normalization","title":"Regularization: Batch Normalization","text":"<p>Normalize activations within each mini-batch.</p> <pre><code>self.bn1 = nn.BatchNorm1d(256)\n</code></pre> <ul> <li>Stabilizes training</li> <li>Allows higher learning rates</li> <li>Add after linear layer, before activation</li> </ul> <p>What \"stabilizes\" means: As a network trains, the distribution of inputs to each layer keeps shifting because the previous layer's weights changed. Layer 5 has to constantly adapt to a moving target. This \"internal covariate shift\" makes training unstable and requires tiny learning rates. Batch normalization fixes this by normalizing each layer's inputs to zero mean and unit variance, then learning optimal scale and shift parameters. The layer always sees similarly-distributed inputs, regardless of what earlier layers are doing. This allows much higher learning rates and faster convergence.</p>"},{"location":"modules/06-neural-networks/#regularization-early-stopping","title":"Regularization: Early Stopping","text":"<p>Stop when validation loss stops improving.</p> <pre><code>if val_loss &lt; best_val_loss:\n    best_val_loss = val_loss\n    save_model()\nelse:\n    patience_counter += 1\n    if patience_counter &gt;= patience:\n        stop_training()\n</code></pre> <p>Simple and effective.</p> <p>Numerical Example: Early Stopping in Action</p> <pre><code>import torch\nimport torch.nn as nn\nimport numpy as np\n\ntorch.manual_seed(42)\n\n# Data that's easy to overfit\nn_train, n_val, n_features = 100, 100, 10\nX_train = torch.randn(n_train, n_features)\ntrue_w = torch.randn(n_features, 1)\ny_train = X_train @ true_w + torch.randn(n_train, 1) * 0.3\nX_val = torch.randn(n_val, n_features)\ny_val = X_val @ true_w + torch.randn(n_val, 1) * 0.3\n\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(10, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, 1)\n    def forward(self, x):\n        return self.fc3(torch.relu(self.fc2(torch.relu(self.fc1(x)))))\n\nmodel = Net()\nopt = torch.optim.Adam(model.parameters(), lr=0.01)\ntrain_losses, val_losses = [], []\n\nfor epoch in range(150):\n    model.train()\n    opt.zero_grad()\n    loss = nn.MSELoss()(model(X_train), y_train)\n    loss.backward()\n    opt.step()\n    train_losses.append(loss.item())\n    model.eval()\n    with torch.no_grad():\n        val_losses.append(nn.MSELoss()(model(X_val), y_val).item())\n\nbest_epoch = np.argmin(val_losses)\nprint(f\"Best epoch: {best_epoch}, Val loss: {val_losses[best_epoch]:.4f}\")\nprint(f\"Final epoch: 149, Val loss: {val_losses[-1]:.4f}\")\n</code></pre> <p>Output: <pre><code>Epoch    Train Loss    Val Loss\n    0       21.9408     16.1734\n   25        0.4885      0.5259\n   50        0.0437      0.3270\n   78        0.0098      0.2645  &lt;-- best\n  100        0.0038      0.2733\n  149        0.0006      0.2816\n\nBest epoch: 78, Val loss: 0.2645\nFinal epoch: 149, Val loss: 0.2816\n</code></pre></p> <p>Interpretation: Training loss keeps decreasing to near-zero, but validation loss hits a minimum at epoch 78 then starts rising\u2014the classic overfitting pattern. Early stopping saves the model at epoch 78, preventing 71 epochs of wasted computation and a worse final model. The gap between train (0.0098) and val (0.2645) at the stopping point is already notable; by epoch 149, train is near-perfect (0.0006) but val is worse (0.2816).</p> <p>Source: <code>slide_computations/module6_examples.py</code> - <code>demo_early_stopping()</code></p>"},{"location":"modules/06-neural-networks/#diagnosing-overfitting","title":"Diagnosing Overfitting","text":"<p>Signs: - Training loss decreasing - Validation loss increasing - Large gap between train/val accuracy</p> <p>Solutions: - More data - Dropout - Early stopping - Simpler architecture - Data augmentation</p>"},{"location":"modules/06-neural-networks/#common-misconceptions_1","title":"Common Misconceptions","text":"Misconception Reality \"Lower training loss is always better\" If validation loss increases, you're overfitting. \"Dropout makes the network weaker\" Only during training. At test, all neurons active. \"Just use Adam defaults\" Tuning learning rate still helps. \"Train until loss is zero\" Zero training loss usually means severe overfitting."},{"location":"modules/06-neural-networks/#63-pytorch-overview","title":"6.3 PyTorch Overview","text":""},{"location":"modules/06-neural-networks/#why-pytorch","title":"Why PyTorch?","text":"<ul> <li>Dynamic computation graphs (easier debugging)</li> <li>Pythonic and intuitive</li> <li>Strong research community</li> <li>Seamless GPU support</li> <li>Great documentation</li> </ul>"},{"location":"modules/06-neural-networks/#tensors-and-autograd","title":"Tensors and Autograd","text":"<p>Tensors: Like NumPy arrays but with GPU support and automatic differentiation.</p> <pre><code>import torch\n\n# Create tensors\nx = torch.randn(3, 4)  # Random normal\n\n# Move to GPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nx = x.to(device)\n</code></pre> <p>Autograd: Automatic differentiation</p> <pre><code>x = torch.tensor([2.0], requires_grad=True)\ny = x ** 2\ny.backward()\nprint(x.grad)  # dy/dx = 2x = 4 at x=2\n</code></pre>"},{"location":"modules/06-neural-networks/#building-models-with-nnmodule","title":"Building Models with nn.Module","text":"<pre><code>import torch.nn as nn\n\nclass MLP(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super().__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, num_classes)\n        self.dropout = nn.Dropout(0.2)\n\n    def forward(self, x):\n        x = x.view(-1, input_size)  # Flatten\n        x = torch.relu(self.fc1(x))\n        x = self.dropout(x)\n        return self.fc2(x)\n</code></pre> <p>Define layers in <code>__init__</code>, define forward pass in <code>forward</code>.</p>"},{"location":"modules/06-neural-networks/#the-training-loop","title":"The Training Loop","text":"<p>This is the heart of neural network training. Learn this pattern:</p> <pre><code>model = MLP(784, 256, 10).to(device)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\nfor epoch in range(num_epochs):\n    model.train()\n    for data, target in train_loader:\n        data, target = data.to(device), target.to(device)\n\n        optimizer.zero_grad()       # 1. Clear gradients\n        output = model(data)        # 2. Forward pass\n        loss = criterion(output, target)\n        loss.backward()             # 3. Backward pass\n        optimizer.step()            # 4. Update weights\n</code></pre> <p>The pattern: 1. <code>optimizer.zero_grad()</code> \u2014 Clear old gradients 2. <code>output = model(data)</code> \u2014 Forward pass 3. <code>loss.backward()</code> \u2014 Compute gradients 4. <code>optimizer.step()</code> \u2014 Update weights</p>"},{"location":"modules/06-neural-networks/#evaluation-mode","title":"Evaluation Mode","text":"<pre><code>model.eval()  # Disables dropout\n\nwith torch.no_grad():  # No gradient tracking\n    for data, target in test_loader:\n        output = model(data)\n        pred = output.argmax(dim=1)\n</code></pre> <p>Key points: - <code>model.eval()</code> disables dropout (uses all neurons) - <code>torch.no_grad()</code> saves memory</p> <p>Always switch to eval mode for validation and testing!</p>"},{"location":"modules/06-neural-networks/#complete-mnist-example","title":"Complete MNIST Example","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\n# Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Data\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\n\ntrain_data = datasets.MNIST('./data', train=True, download=True, transform=transform)\ntest_data = datasets.MNIST('./data', train=False, transform=transform)\n\ntrain_loader = DataLoader(train_data, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_data, batch_size=1000)\n\n# Model\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(784, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, 10)\n        self.dropout = nn.Dropout(0.2)\n\n    def forward(self, x):\n        x = x.view(-1, 784)\n        x = torch.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = torch.relu(self.fc2(x))\n        return self.fc3(x)\n\nmodel = Net().to(device)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\n# Training\ndef train(epoch):\n    model.train()\n    for data, target in train_loader:\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n\ndef test():\n    model.eval()\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            pred = model(data).argmax(dim=1)\n            correct += pred.eq(target).sum().item()\n    return 100. * correct / len(test_loader.dataset)\n\n# Run\nfor epoch in range(10):\n    train(epoch)\n    print(f'Epoch {epoch}: Test Accuracy: {test():.2f}%')\n</code></pre>"},{"location":"modules/06-neural-networks/#reflection-questions","title":"Reflection Questions","text":"<ol> <li> <p>Why couldn't the original perceptron learn XOR? Draw the XOR data and explain.</p> </li> <li> <p>If a neural network can approximate any function with one hidden layer (Universal Approximation), why do we need deep networks?</p> </li> <li> <p>Why do we need non-linear activation functions? What would happen with only linear activations?</p> </li> <li> <p>A model has 10 million parameters. Is that a lot? What determines if this is appropriate?</p> </li> <li> <p>Your training loss is decreasing but validation loss is increasing. What's happening and how do you fix it?</p> </li> <li> <p>Why might Adam work better than vanilla SGD without tuning?</p> </li> <li> <p>How is dropout similar to ensemble methods like Random Forest?</p> </li> </ol>"},{"location":"modules/06-neural-networks/#practice-problems","title":"Practice Problems","text":"<ol> <li> <p>Calculate parameters for a [784, 512, 256, 128, 10] network</p> </li> <li> <p>Identify overfitting from training curves (given a plot description)</p> </li> <li> <p>Choose appropriate activation for: (a) hidden layers, (b) binary output, (c) multi-class output</p> </li> <li> <p>Debug: \"My training loss keeps increasing.\" Most likely cause?</p> </li> <li> <p>Write the PyTorch training loop pattern from memory</p> </li> </ol>"},{"location":"modules/06-neural-networks/#chapter-summary","title":"Chapter Summary","text":"<p>Six key takeaways from Module 6:</p> <ol> <li> <p>Neural networks = stacked layers + non-linear activations</p> </li> <li> <p>Depth enables learning hierarchical features</p> </li> <li> <p>Backpropagation computes gradients via chain rule</p> </li> <li> <p>Adam is a good default optimizer; learning rate is the key hyperparameter</p> </li> <li> <p>Dropout + early stopping prevent overfitting</p> </li> <li> <p>PyTorch pattern: zero_grad \u2192 forward \u2192 backward \u2192 step</p> </li> </ol>"},{"location":"modules/06-neural-networks/#whats-next","title":"What's Next","text":"<p>In Module 7, we tackle Computer Vision &amp; CNNs: - Convolutional layers for images - Pooling and feature maps - Famous architectures (LeNet, VGG, ResNet) - Transfer learning</p> <p>Same training principles, but specialized for images. Instead of fully connected layers, we'll use convolutional layers that exploit spatial structure.</p>"},{"location":"modules/07-computer-vision/","title":"Module 7: Computer Vision &amp; CNNs","text":""},{"location":"modules/07-computer-vision/#introduction","title":"Introduction","text":"<p>Last module we learned neural network fundamentals\u2014layers, activations, backpropagation, PyTorch. Today we specialize those concepts for images.</p> <p>Images are everywhere in business: quality control in manufacturing, inventory management in retail, medical imaging in healthcare, document processing in finance. Computer vision has transformed all of these industries.</p> <p>But images present unique challenges. A single photo is millions of numbers. Fully connected networks can't scale. And we need spatial awareness\u2014a cat in the corner is still a cat, but its pixels are in completely different positions.</p> <p>Convolutional Neural Networks solve these problems. By the end of today, you'll understand how CNNs work, and critically, you'll know how to leverage transfer learning so you don't have to train from scratch.</p> <p>Transfer learning works broadly: Early CNN layers learn universal visual primitives (edges, textures) that transfer to any domain. Studies show ImageNet transfer helps on X-rays, satellite images, even art classification. Train from scratch only with massive domain data AND truly different image statistics\u2014even then, ImageNet weights as initialization usually help.</p>"},{"location":"modules/07-computer-vision/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, you should be able to:</p> <ol> <li>Explain how images are represented as data (matrices, channels)</li> <li>Describe why fully connected networks are inefficient for images</li> <li>Explain the mechanics of convolutional layers and pooling</li> <li>Implement a CNN in PyTorch for image classification</li> <li>Apply transfer learning using pre-trained models</li> <li>Understand modern CV applications (detection, segmentation, ViT)</li> </ol>"},{"location":"modules/07-computer-vision/#71-working-with-images","title":"7.1 Working with Images","text":""},{"location":"modules/07-computer-vision/#how-images-are-represented","title":"How Images Are Represented","text":"<p>Digital images are matrices of numbers.</p> <p>Grayscale: 2D matrix (Height \u00d7 Width). Each pixel is an intensity from 0 (black) to 255 (white).</p> <p>Color (RGB): 3D tensor (Height \u00d7 Width \u00d7 3). Three channels\u2014Red, Green, Blue\u2014each with its own intensity matrix.</p> <p>Example: A 224\u00d7224 color image - Shape: (224, 224, 3) - Total values: 224 \u00d7 224 \u00d7 3 = 150,528 numbers</p> <p>Think of RGB as a layer cake: Imagine three transparent sheets stacked on top of each other\u2014one tinted red, one green, one blue. Each sheet has the same dimensions (Height \u00d7 Width), and each position has an intensity value. When you look through all three layers at once, the colors combine to produce the full-color image. A pixel isn't just \"one number\"\u2014it's a stack of three numbers, one from each color channel. This stacking concept extends to CNNs: as you go deeper, instead of 3 channels (RGB), you might have 64, 128, or 512 \"feature channels\"\u2014each representing a different learned feature like edges, textures, or shapes.</p> <p>PyTorch convention: (Batch, Channels, Height, Width)\u2014NCHW format.</p> <pre><code>from PIL import Image\nimport numpy as np\n\nimg = Image.open('photo.jpg')\nimg_array = np.array(img)\nprint(f\"Shape: {img_array.shape}\")  # (Height, Width, Channels)\n</code></pre>"},{"location":"modules/07-computer-vision/#imagenet-the-benchmark-that-changed-everything","title":"ImageNet: The Benchmark That Changed Everything","text":"Year Winner Top-5 Error Significance 2010 Traditional 28.2% Pre-deep learning 2012 AlexNet 16.4% CNN breakthrough 2015 ResNet 3.6% Beat humans (~5%) <p>In 2012, AlexNet\u2014a convolutional neural network\u2014crushed the competition. Error dropped from 28% to 16%. That's not incremental improvement; that's a paradigm shift.</p> <p>By 2015, ResNet beat human performance on ImageNet classification.</p>"},{"location":"modules/07-computer-vision/#why-fully-connected-networks-fail","title":"Why Fully Connected Networks Fail","text":"<p>Problem 1: Too many parameters - 224\u00d7224\u00d73 input with 1000 hidden neurons - = 150 million parameters in first layer alone! - Impossible to train, will overfit immediately</p> <p>Problem 2: No spatial understanding - Fully connected layers treat each pixel independently - A cat in the corner has completely different pixel positions than a cat in the center - The network can't generalize</p> <p>The solution: Convolutional Neural Networks</p> <p>Numerical Example: Parameter Explosion in Fully Connected Networks</p> <pre><code># Calculate first FC layer parameters for different image sizes\nimage_configs = [\n    (\"MNIST\", 28, 28, 1),      # Grayscale\n    (\"CIFAR-10\", 32, 32, 3),   # Color\n    (\"ImageNet\", 224, 224, 3), # Standard photo\n]\nhidden_neurons = 1000\n\nfor name, h, w, c in image_configs:\n    input_features = h * w * c\n    parameters = input_features * hidden_neurons + hidden_neurons\n    print(f\"{name}: {input_features:,} inputs \u2192 {parameters:,} parameters\")\n</code></pre> <p>Output: <pre><code>MNIST: 784 inputs \u2192 785,000 parameters\nCIFAR-10: 3,072 inputs \u2192 3,073,000 parameters\nImageNet: 150,528 inputs \u2192 150,529,000 parameters\n</code></pre></p> <p>Interpretation: A single FC layer on a 224\u00d7224 image requires 150 million parameters\u2014just to connect inputs to the first hidden layer! This is why FC networks are impractical for images. CNNs achieve the same task with ~100x fewer parameters through local connectivity and weight sharing.</p> <p>Source: <code>slide_computations/module7_examples.py</code> - <code>demo_fc_parameter_explosion()</code></p> <p>Why position matters: A fully connected network treats each pixel independently\u2014\"pixel 1,000 is orange\" vs. \"pixel 50,000 is orange\" are completely different inputs. To recognize cats anywhere, it would need examples at every possible position (billions of configurations). CNNs solve this with weight sharing: the same filter scans all positions, so learning to detect a cat's eye at one position automatically applies everywhere.</p>"},{"location":"modules/07-computer-vision/#72-convolutional-neural-networks","title":"7.2 Convolutional Neural Networks","text":""},{"location":"modules/07-computer-vision/#the-convolution-operation","title":"The Convolution Operation","text":"<p>Instead of connecting every input to every output, we slide a small filter across the image.</p> <p>The operation: 1. Take a small filter (e.g., 3\u00d73) 2. Slide it across the image 3. At each position, compute dot product of filter and patch 4. Output is a \"feature map\"</p> <p>Key parameters: - Filter size: 3\u00d73 or 5\u00d75 typical - Stride: How many pixels to move (1 or 2) - Padding: Zeros around edges to control output size - Number of filters: Each learns a different feature</p> <p>The sliding window intuition: Imagine holding a magnifying glass (the filter) over a photograph (the input image). You look at a small 3\u00d73 patch, write down a summary number, then slide the magnifying glass one position to the right and repeat. When you reach the edge, you move down one row and start from the left again. The \"summary number\" is the dot product: multiply each pixel by the corresponding filter weight and sum them all. After scanning the entire image, you've produced a new, smaller image called a \"feature map\"\u2014where each position tells you \"how strongly does this local region match what this filter is looking for?\"</p> <p>Numerical Example: Convolution by Hand</p> <pre><code>import numpy as np\n\n# 5\u00d75 image with bright center\nimage = np.array([\n    [10, 10, 10, 10, 10],\n    [10, 50, 50, 50, 10],\n    [10, 50, 100, 50, 10],\n    [10, 50, 50, 50, 10],\n    [10, 10, 10, 10, 10],\n])\n\n# Horizontal edge detector\nfilter_h = np.array([[-1, -2, -1],\n                     [ 0,  0,  0],\n                     [ 1,  2,  1]])\n\n# Convolve center position (1,1)\npatch = image[1:4, 1:4]  # Extract 3\u00d73 patch\nresult = np.sum(patch * filter_h)\nprint(f\"Patch:\\n{patch}\")\nprint(f\"Element-wise product sum: {result}\")\n</code></pre> <p>Output: <pre><code>Patch:\n[[50 50 50]\n [50 100 50]\n [50 50 50]]\nElement-wise product sum: 0\n</code></pre></p> <p>Interpretation: The center patch is symmetric top-to-bottom, so the horizontal edge detector outputs 0 (no horizontal edge). At the top of the image where intensity changes from 10\u219250, the filter outputs +170, detecting the edge. The filter automatically responds to edges wherever they occur.</p> <p>Source: <code>slide_computations/module7_examples.py</code> - <code>demo_convolution_by_hand()</code></p>"},{"location":"modules/07-computer-vision/#multi-channel-convolution","title":"Multi-Channel Convolution","text":"<p>Key insight: A \"3\u00d73 filter\" on an RGB image is actually a 3\u00d73\u00d73 tensor.</p> <p>When we say \"3\u00d73 filter,\" we're describing the spatial dimensions. But the filter must match the depth of the input.</p> <p>For an RGB image with 3 channels: - Filter shape: 3 \u00d7 3 \u00d7 3 = 27 weights (plus 1 bias) - Each channel (R, G, B) has its own 3\u00d73 slice</p> <p>How the computation works:</p> <pre><code>At each spatial position:\n1. Extract the 3\u00d73\u00d73 patch from the input\n2. Multiply element-wise with the 3\u00d73\u00d73 filter (27 multiplications)\n3. Sum ALL 27 products + bias \u2192 ONE output value\n</code></pre> <p>Multiple filters \u2192 Multiple output channels:</p> <p>If we want 64 output channels, we need 64 separate filters, each with shape 3\u00d73\u00d73. Total parameters: 64 \u00d7 (27 + 1) = 1,792.</p> <p>The \"deep handshake\" intuition: A filter doesn't just look at one color\u2014it reaches through all input channels simultaneously, like a hand reaching through stacked sheets to grab information from every layer at once. If the input has 3 channels (RGB), the filter has 3 slices. If the input has 64 feature channels from a previous layer, the filter has 64 slices. Each slice learns what to look for in that specific input channel, and the results are summed into a single output value. This is why deeper layers can detect complex combinations: a filter might learn \"look for vertical edges in channel 12 AND horizontal edges in channel 37\" by having strong weights in those specific filter slices.</p> <pre><code>conv = nn.Conv2d(\n    in_channels=3,      # RGB input\n    out_channels=64,    # Number of filters\n    kernel_size=3,      # 3\u00d73 filter\n    stride=1,\n    padding=1\n)\n</code></pre> <p>Numerical Example: Output Size Formula in Action</p> <pre><code># Formula: output = (W - K + 2P) / S + 1\n# W=input, K=kernel, P=padding, S=stride\n\n# Trace 32\u00d732 image through 3 conv+pool blocks\nsize = 32\nprint(f\"Input: {size}\u00d7{size}\")\n\nfor i in range(3):\n    # Conv with padding=1 preserves size\n    size = (size - 3 + 2*1) // 1 + 1\n    print(f\"After Conv{i+1}: {size}\u00d7{size}\")\n    # MaxPool 2\u00d72 halves dimensions\n    size = size // 2\n    print(f\"After Pool{i+1}: {size}\u00d7{size}\")\n</code></pre> <p>Output: <pre><code>Input: 32\u00d732\nAfter Conv1: 32\u00d732\nAfter Pool1: 16\u00d716\nAfter Conv2: 16\u00d716\nAfter Pool2: 8\u00d78\nAfter Conv3: 8\u00d78\nAfter Pool3: 4\u00d74\n</code></pre></p> <p>Interpretation: With padding=1 on 3\u00d73 convolutions, spatial dimensions are preserved. Each 2\u00d72 max pool halves the dimensions. A 32\u00d732 image becomes 4\u00d74 after three pool layers\u2014a 64x reduction in spatial positions, concentrating information into fewer, more meaningful locations.</p> <p>Source: <code>slide_computations/module7_examples.py</code> - <code>demo_output_size_formula()</code></p>"},{"location":"modules/07-computer-vision/#what-filters-learn","title":"What Filters Learn","text":"<p>Filters automatically learn features through training:</p> <ul> <li>Early layers: Edges, colors, simple textures</li> <li>Middle layers: Textures, patterns, shapes</li> <li>Deep layers: Object parts, semantic concepts</li> </ul> <p>The first layer might learn vertical edges, horizontal edges, color gradients. The second combines those into textures. The third combines textures into shapes. This is hierarchical feature learning.</p> <p>What an edge detector actually looks like: A horizontal edge detector might have weights like: <pre><code>[-1, -1, -1]\n[ 0,  0,  0]\n[ 1,  1,  1]\n</code></pre> This filter responds strongly when it sees dark pixels above and bright pixels below (a horizontal edge). The negative weights say \"penalize brightness here,\" the positive weights say \"reward brightness here,\" and zeros mean \"don't care.\" When this filter slides over a horizontal edge in the image, the dark-above-light-below pattern produces a large positive output. Over uniform regions, positives and negatives cancel out. The network learns these patterns automatically through backpropagation\u2014we don't hand-design them.</p> <p>Hierarchy emerges automatically: You don't design what each layer learns. Early layers only see raw pixels (can only learn edges); deep layers receive processed representations (can combine into complex features). When researchers visualize trained networks, they find edges in layer 1, textures in layers 2-3, object parts in mid-layers\u2014discovered, not programmed.</p>"},{"location":"modules/07-computer-vision/#pooling-layers","title":"Pooling Layers","text":"<p>After convolution, we reduce spatial dimensions with pooling.</p> <p>Max Pooling: Take maximum value in each patch - Reduces spatial dimensions (224 \u2192 112 \u2192 56...) - Adds translation invariance\u2014slight shifts don't change output - Keeps strongest activations</p> <pre><code>pool = nn.MaxPool2d(kernel_size=2, stride=2)\n# 224\u00d7224 \u2192 112\u00d7112\n</code></pre> <p>A 2\u00d72 max pool with stride 2 halves each dimension.</p> <p>Why max pooling dominates over average pooling: Imagine a feature map where most values are near zero (no edge detected) but one position has a strong response (edge found!). Max pooling preserves that strong signal\u2014\"there's definitely an edge somewhere in this 2\u00d72 region.\" Average pooling would dilute it with the zeros: \"there's maybe a weak edge here.\" For detecting features, we care that a feature is present, not its average strength. Exception: Global Average Pooling at the very end of a network (averaging across the entire spatial dimension) works well because by that point, strong features have already been isolated.</p> <p>Numerical Example: Pooling Dimension Tracking</p> <pre><code># Track ImageNet-standard 224\u00d7224 through pooling layers\nsize = 224\nprint(f\"Input: {size}\u00d7{size} = {size*size:,} positions\")\n\nfor i in range(5):\n    size = size // 2\n    reduction = (224*224) / (size*size)\n    print(f\"Pool {i+1}: {size}\u00d7{size} = {size*size:,} positions ({reduction:.0f}x smaller)\")\n</code></pre> <p>Output: <pre><code>Input: 224\u00d7224 = 50,176 positions\nPool 1: 112\u00d7112 = 12,544 positions (4x smaller)\nPool 2: 56\u00d756 = 3,136 positions (16x smaller)\nPool 3: 28\u00d728 = 784 positions (64x smaller)\nPool 4: 14\u00d714 = 196 positions (256x smaller)\nPool 5: 7\u00d77 = 49 positions (1024x smaller)\n</code></pre></p> <p>Interpretation: Each 2\u00d72 max pool halves each dimension, quartering the spatial positions. After 5 pooling layers, 50,176 positions compress to just 49\u2014over 1000x reduction. This progressive compression forces the network to distill spatial information into increasingly abstract \"what is here\" representations rather than \"where exactly is it.\"</p> <p>Source: <code>slide_computations/module7_examples.py</code> - <code>demo_pooling_dimension_tracking()</code></p>"},{"location":"modules/07-computer-vision/#classic-cnn-pattern","title":"Classic CNN Pattern","text":"<p>Reading the diagram: This shows the classic CNN architecture pattern as a data flow pipeline. Data enters from the left and flows through repeated blocks: Conv (blue) applies learned filters to detect features, ReLU (green) introduces non-linearity by zeroing negative values, and Pool (purple) reduces spatial dimensions. This Conv\u2192ReLU\u2192Pool pattern typically repeats 2-5 times, with each cycle detecting higher-level features while shrinking the spatial dimensions. After the final pooling layer, Flatten (orange) reshapes the 2D feature maps into a 1D vector, which feeds into FC (red)\u2014a fully connected layer that makes the final classification. The key insight: early stages are \"looking\" (detecting edges, textures, shapes), while the final FC layer is \"deciding\" (combining features into class predictions).</p> <pre><code>class SimpleCNN(nn.Module):\n    def __init__(self, num_classes=10):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n        self.fc2 = nn.Linear(512, num_classes)\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x):\n        x = self.pool(torch.relu(self.conv1(x)))\n        x = self.pool(torch.relu(self.conv2(x)))\n        x = self.pool(torch.relu(self.conv3(x)))\n        x = x.view(x.size(0), -1)  # Flatten\n        x = torch.relu(self.fc1(x))\n        x = self.dropout(x)\n        return self.fc2(x)\n</code></pre>"},{"location":"modules/07-computer-vision/#parameter-efficiency","title":"Parameter Efficiency","text":"<p>For 32\u00d732 RGB image, 64 outputs:</p> Layer Type Parameters Fully Connected 196,672 Conv2d (3\u00d73) 1,792 <p>~100x fewer parameters!</p> <p>Why? 1. Local connectivity: Each neuron connects only to a small patch 2. Weight sharing: Same filter applied everywhere</p> <p>Numerical Example: CNN vs FC Parameter Comparison</p> <pre><code># Task: 32\u00d732\u00d73 input \u2192 64 output features\ninput_size = 32 * 32 * 3  # 3,072\noutput_channels = 64\n\n# Fully connected\nfc_params = input_size * output_channels + output_channels\nprint(f\"FC layer: {fc_params:,} parameters\")\n\n# Conv2d (3\u00d73 filter)\nconv_params = (3 * 3 * 3) * output_channels + output_channels\nprint(f\"Conv layer: {conv_params:,} parameters\")\nprint(f\"Ratio: {fc_params / conv_params:.1f}x fewer with CNN\")\n</code></pre> <p>Output: <pre><code>FC layer: 196,672 parameters\nConv layer: 1,792 parameters\nRatio: 109.8x fewer with CNN\n</code></pre></p> <p>Interpretation: For the same input\u2192output mapping, CNNs use ~110x fewer parameters. The FC layer needs a separate weight for every input-output pair. The CNN reuses 27 weights (3\u00d73\u00d73 filter) across all 1,024 spatial positions. This efficiency enables training on limited data and reduces overfitting risk.</p> <p>Source: <code>slide_computations/module7_examples.py</code> - <code>demo_cnn_vs_fc_parameters()</code></p>"},{"location":"modules/07-computer-vision/#historical-architectures","title":"Historical Architectures","text":"<p>AlexNet (2012): 8 layers, ReLU, dropout, GPU training. The breakthrough.</p> <p>VGG (2014): 16-19 layers, all 3\u00d73 convolutions. Showed depth matters.</p> <p>ResNet (2015): Skip connections enabling 150+ layers.</p>"},{"location":"modules/07-computer-vision/#skip-residual-connections","title":"Skip (Residual) Connections","text":"<p>The problem: Very deep networks suffer from vanishing gradients.</p> <p>The solution: Add the input directly to the output.</p> \\[Output = F(x) + x\\] <pre><code>class ResidualBlock(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)\n        self.bn1 = nn.BatchNorm2d(channels)\n        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)\n        self.bn2 = nn.BatchNorm2d(channels)\n\n    def forward(self, x):\n        residual = x\n        out = torch.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += residual  # Skip connection\n        return torch.relu(out)\n</code></pre> <p>If the network can't improve on the input, it can at least pass it through unchanged. This creates direct paths for gradients and enables training 100+ layer networks.</p> <p>The \"highway on-ramp\" analogy: In a deep network without skip connections, gradients must travel through every layer sequentially\u2014like driving through 100 stoplights to get across town. Each layer can shrink the gradient (vanishing) or explode it. Skip connections add highway on-ramps: gradients can take the direct route (the skip) or the scenic route (through the layers), or both. Even if the scenic route has problems, the highway ensures signals get through. During training, early layers actually receive useful gradient information because it doesn't have to survive passage through dozens of potentially problematic layers.</p> <p>Skip connection trade-offs: Memory overhead (must store earlier activations) and architectural constraints (dimensions must match, may need 1\u00d71 convolutions). In shallow networks (3-5 layers), minimal benefit\u2014skip connections solve a deep network problem. For networks &gt;10 layers, skip connections almost always help and are now considered essential in modern architectures.</p>"},{"location":"modules/07-computer-vision/#common-misconceptions","title":"Common Misconceptions","text":"Misconception Reality \"CNNs only work for images\" CNNs work on any grid data: audio, time series, etc. \"Deeper is always better\" Without skip connections, very deep nets fail. Architecture matters. \"You need to design CNNs from scratch\" Transfer learning is usually better."},{"location":"modules/07-computer-vision/#73-transfer-learning","title":"7.3 Transfer Learning","text":""},{"location":"modules/07-computer-vision/#the-core-idea","title":"The Core Idea","text":"<p>Pre-trained ImageNet models learned general visual features: edges, textures, shapes, patterns. These features are useful for almost any image task!</p> <p>Learning to see before learning your task: A child doesn't learn \"what is a cat\" from scratch\u2014they already know how to see edges, shapes, colors, and textures from years of visual experience. Teaching them \"cat\" is just connecting those existing visual concepts to a new label. Transfer learning works the same way: ImageNet training teaches a network \"how to see\" (edges, textures, shapes, object parts), and your task-specific training just connects those visual features to your labels. That's why 500 images can work: you're not teaching the network to see\u2014you're just teaching it what to call things it can already perceive.</p> <p>Two approaches: 1. Feature extraction: Freeze pre-trained layers, train only new classifier 2. Fine-tuning: Train all layers, but with lower learning rate for pre-trained layers</p> <p>This is how most real-world computer vision is done. You rarely train from scratch anymore.</p>"},{"location":"modules/07-computer-vision/#feature-extraction","title":"Feature Extraction","text":"<p>Freeze pre-trained layers, train only new classifier.</p> <pre><code>import torchvision.models as models\n\nmodel = models.resnet50(pretrained=True)\n\n# Freeze all layers\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Replace final classifier\nmodel.fc = nn.Linear(model.fc.in_features, num_classes)\n\n# Only train new classifier\noptimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n</code></pre> <p>The pre-trained ResNet extracts features. You just train a simple classifier on top.</p>"},{"location":"modules/07-computer-vision/#fine-tuning","title":"Fine-Tuning","text":"<p>Train pre-trained layers with lower learning rate.</p> <pre><code>model = models.resnet50(pretrained=True)\nmodel.fc = nn.Linear(model.fc.in_features, num_classes)\n\n# Different learning rates\noptimizer = optim.Adam([\n    {'params': model.layer4.parameters(), 'lr': 1e-4},\n    {'params': model.fc.parameters(), 'lr': 1e-3}\n])\n</code></pre> <p>Pre-trained layers get smaller learning rate (they're already good). New layers get larger learning rate.</p>"},{"location":"modules/07-computer-vision/#when-to-use-which","title":"When to Use Which","text":"Dataset Size Similarity to ImageNet Approach Small High Feature extraction Small Low Light fine-tuning Large High Fine-tuning Large Low Train from scratch <p>Example: You have 500 X-ray images. Train from scratch or transfer learning?</p> <p>Transfer learning! 500 images isn't enough to train from scratch. Even though X-rays look different from ImageNet photos, early-layer features (edges, textures) are still useful.</p> <p>How similar is \"similar enough\"? There's no bright line\u2014empirically test: train a classifier on frozen pre-trained features vs. random features. If pre-trained beats random, transfer helps. Even domains that seem \"completely different\" (medical imaging, industrial defects) usually benefit. Start with transfer learning, try fine-tuning if unsatisfactory, consider training from scratch only with millions of examples AND truly foreign image statistics.</p> <p>Numerical Example: Transfer Learning vs Random Features</p> <pre><code># Simulate: classify images with limited training data\n# Pre-trained CNN extracts meaningful features\n# Random CNN outputs noise\n\nfrom sklearn.linear_model import LogisticRegression\n\ntrain_sizes = [25, 50, 100, 200, 500]\nfor n in train_sizes:\n    # Train classifier on pre-trained features\n    acc_pretrained = train_on_features(X_pretrained[:n], y[:n])\n    # Train classifier on random features\n    acc_random = train_on_features(X_random[:n], y[:n])\n    print(f\"n={n}: Pre-trained={acc_pretrained:.0%}, Random={acc_random:.0%}\")\n</code></pre> <p>Output: <pre><code>n=25:  Pre-trained=40%, Random=12%\nn=50:  Pre-trained=61%, Random=18%\nn=100: Pre-trained=74%, Random=18%\nn=200: Pre-trained=82%, Random=25%\nn=500: Pre-trained=89%, Random=19%\n</code></pre></p> <p>Interpretation: With only 25 training examples, pre-trained features achieve 40% accuracy vs 12% for random (5-class chance = 20%). The gap widens with more data. Random features plateau near chance because they contain no useful information\u2014the classifier is guessing. Pre-trained features capture real visual patterns that generalize to new images.</p> <p>Source: <code>slide_computations/module7_examples.py</code> - <code>demo_transfer_learning_comparison()</code></p>"},{"location":"modules/07-computer-vision/#business-value-of-transfer-learning","title":"Business Value of Transfer Learning","text":"<ul> <li>Cost savings: Days of training \u2192 hours</li> <li>Data efficiency: Good results with hundreds of images (not millions)</li> <li>Time to deployment: Quick proof-of-concept</li> <li>No massive compute: Fine-tuning on a laptop is possible</li> </ul>"},{"location":"modules/07-computer-vision/#74-modern-vision-applications","title":"7.4 Modern Vision Applications","text":""},{"location":"modules/07-computer-vision/#object-detection","title":"Object Detection","text":"<p>Task: Find objects AND their locations (bounding boxes)</p> <p>Not just \"there's a dog\" but \"there's a dog at coordinates (x, y, w, h).\"</p> <p>Key architectures: - YOLO: Fast, single-pass detection (\"You Only Look Once\") - Faster R-CNN: Two-stage, more accurate but slower</p> <p>Applications: Autonomous vehicles, security cameras, retail inventory</p>"},{"location":"modules/07-computer-vision/#image-segmentation","title":"Image Segmentation","text":"<p>Semantic segmentation: Label every pixel with a class (road, car, person)</p> <p>Instance segmentation: Separate individual objects (this car vs that car)</p> <p>Key architecture: U-Net\u2014encoder-decoder with skip connections</p> <p>Applications: Medical imaging, autonomous driving, photo editing</p>"},{"location":"modules/07-computer-vision/#vision-transformers-vit","title":"Vision Transformers (ViT)","text":"<p>The latest revolution: apply transformer architecture to images.</p> <p>How it works: 1. Split image into 16\u00d716 patches 2. Flatten patches into sequences 3. Apply transformer encoder (same architecture as NLP!)</p> <p>Why it matters: - State-of-the-art on many benchmarks - Unified architecture for vision AND language - Enables CLIP, DALL-E, multimodal AI</p> <p>Patches as visual words: In NLP, transformers process sequences of word tokens. ViT creates a similar setup for images: each 16\u00d716 patch becomes a \"visual word.\" A 224\u00d7224 image becomes a sequence of (224/16)\u00b2 = 196 tokens. The transformer then asks \"how does patch 45 relate to patch 120?\" just like it asks \"how does word 3 relate to word 15?\" in text. This unification is powerful: the same attention mechanism that learns \"the word 'cat' relates to 'furry'\" can learn \"this patch of fur relates to that patch showing ears.\" It's why models like CLIP can connect images and text\u2014they're processing both as sequences of tokens.</p>"},{"location":"modules/07-computer-vision/#business-applications","title":"Business Applications","text":"Industry Application Retail Inventory monitoring, checkout-free stores Manufacturing Defect detection, quality control Healthcare Radiology, pathology analysis Agriculture Crop monitoring, disease detection"},{"location":"modules/07-computer-vision/#reflection-questions","title":"Reflection Questions","text":"<ol> <li> <p>An image is 1000\u00d71000 pixels RGB. How many input features? Why is this problematic for fully connected networks?</p> </li> <li> <p>If you shift a cat 10 pixels to the right, how would a fully connected network's perception change vs. a CNN?</p> </li> <li> <p>A 3\u00d73 conv filter has 9 weights per channel. How does this compare to fully connected for the same output?</p> </li> <li> <p>After 3 max pooling layers of 2\u00d72, what happens to a 224\u00d7224 image?</p> </li> <li> <p>How do skip connections help train very deep networks?</p> </li> <li> <p>You have 500 X-ray images. Train from scratch or transfer learning? Why?</p> </li> <li> <p>Why fine-tune later layers before earlier layers?</p> </li> </ol>"},{"location":"modules/07-computer-vision/#practice-problems","title":"Practice Problems","text":"<ol> <li> <p>Calculate output size: 64\u00d764 input, 3\u00d73 kernel, stride=1, padding=0</p> </li> <li> <p>Calculate parameters: Conv2d with in_channels=32, out_channels=64, kernel_size=3</p> </li> <li> <p>Design a CNN for 28\u00d728 grayscale images (MNIST) with 3 conv layers</p> </li> <li> <p>Set up transfer learning code for a 5-class classification problem using ResNet18</p> </li> <li> <p>Explain why a 7\u00d77 filter might be replaced by two 3\u00d73 filters</p> </li> </ol>"},{"location":"modules/07-computer-vision/#chapter-summary","title":"Chapter Summary","text":"<p>Six key takeaways from Module 7:</p> <ol> <li> <p>Images are high-dimensional; FC networks don't scale</p> </li> <li> <p>CNNs use local filters with weight sharing (100x fewer parameters)</p> </li> <li> <p>Pooling reduces dimensions and adds translation invariance</p> </li> <li> <p>Skip connections enable training very deep networks</p> </li> <li> <p>Transfer learning is usually better than training from scratch</p> </li> <li> <p>Modern CV: detection, segmentation, Vision Transformers</p> </li> </ol>"},{"location":"modules/07-computer-vision/#whats-next","title":"What's Next","text":"<p>In Module 8, we tackle Natural Language Processing: - Text as sequences - Word embeddings - Transformers and attention - Pre-trained language models</p> <p>Vision Transformers connect both domains\u2014the same architecture that powers GPT and BERT can also process images!</p>"},{"location":"modules/08-nlp/","title":"Module 8: Natural Language Processing","text":""},{"location":"modules/08-nlp/#introduction","title":"Introduction","text":"<p>Today we tackle natural language processing\u2014teaching machines to understand and generate text.</p> <p>Text is everywhere in business: customer reviews, support tickets, emails, social media, contracts, reports. Being able to automatically classify, extract information from, and generate text is incredibly valuable.</p> <p>In Module 7, we saw how CNNs revolutionized image processing. Today, we'll see how transformers revolutionized NLP. The transformer architecture\u2014introduced in 2017\u2014is the foundation for BERT, GPT, and essentially every language model you've heard of.</p> <p>By the end of this module, you'll understand how text becomes numbers, why transformers work so well, and how to leverage pre-trained models for your own applications.</p> <p>What is machine \"understanding\"? Machines don't understand text like humans\u2014they operate on statistical representations where similar meanings cluster together. What we call \"understanding\" is sophisticated pattern matching: a model that predicts masked words correctly has learned syntax, semantics, and world knowledge encoded as neural network weights. Whether this constitutes \"understanding\" or merely simulates it remains philosophically contested.</p>"},{"location":"modules/08-nlp/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, you should be able to:</p> <ol> <li>Explain different text representation methods (BoW, TF-IDF, embeddings)</li> <li>Understand why word order and context matter in NLP</li> <li>Describe RNN architecture and the vanishing gradient problem</li> <li>Explain the transformer architecture and self-attention mechanism</li> <li>Apply pre-trained language models (BERT, GPT) for NLP tasks</li> <li>Identify appropriate NLP approaches for business problems</li> </ol>"},{"location":"modules/08-nlp/#81-text-representation","title":"8.1 Text Representation","text":""},{"location":"modules/08-nlp/#the-challenge-of-text","title":"The Challenge of Text","text":"<p>Text is fundamentally different from tabular data: - Variable length: Sentences can be 5 words or 500 - Order matters: \"Dog bites man\" \u2260 \"Man bites dog\" - Same word, different meanings: \"bank\" (river) vs \"bank\" (financial) - Vast vocabulary: Hundreds of thousands of words</p> <p>Goal: Convert text to numerical vectors that capture meaning.</p>"},{"location":"modules/08-nlp/#bag-of-words-bow","title":"Bag of Words (BoW)","text":"<p>The simplest approach: count word occurrences.</p> Document \"love\" \"machine\" \"learning\" \"I love machine learning\" 1 1 1 \"Machine learning is great\" 0 1 1 <p>Each document becomes a vector of word counts.</p> <pre><code>from sklearn.feature_extraction.text import CountVectorizer\n\ncorpus = [\n    \"I love machine learning\",\n    \"Machine learning is great\",\n    \"I love deep learning\"\n]\n\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names_out())\n# ['deep', 'great', 'is', 'learning', 'love', 'machine']\n</code></pre> <p>Limitations: - Ignores word order: \"dog bites man\" = \"man bites dog\" - Sparse and high-dimensional - No semantic similarity: \"good\" and \"great\" are unrelated</p> <p>Think of it like a recipe vs. a shopping list: BoW gives you the ingredients (flour, eggs, sugar, butter) but loses the recipe (the order and method matter!). \"Cream butter and sugar, then add eggs\" produces cake. \"Add eggs, then cream butter and sugar\" produces scrambled eggs with a butter problem. Same ingredients, completely different outcomes. BoW can't tell these apart.</p> <p>Numerical Example: BoW Sparsity Problem</p> <pre><code># Compare BoW vs embedding representations\nvocab_size = 10000\nembedding_dim = 300\nsentence_words = 4  # \"I love machine learning\"\n\n# BoW: huge sparse vector\nbow_nonzero = sentence_words\nbow_sparsity = (vocab_size - bow_nonzero) / vocab_size * 100\nbow_memory = vocab_size * 4 / 1024  # KB (float32)\n\n# Embeddings: small dense vector\nemb_memory = embedding_dim * 4 / 1024  # KB\n\nprint(f\"BoW vector:       {vocab_size:,} dims, {bow_nonzero} non-zero, {bow_sparsity:.2f}% sparse\")\nprint(f\"Embedding vector: {embedding_dim} dims, all non-zero, 0% sparse\")\nprint(f\"Memory: BoW={bow_memory:.1f} KB vs Embedding={emb_memory:.1f} KB\")\nprint(f\"Dimensionality reduction: {vocab_size/embedding_dim:.0f}x\")\n</code></pre> <p>Output: <pre><code>BoW vector:       10,000 dims, 4 non-zero, 99.96% sparse\nEmbedding vector: 300 dims, all non-zero, 0% sparse\nMemory: BoW=39.1 KB vs Embedding=1.2 KB\nDimensionality reduction: 33x\n</code></pre></p> <p>Interpretation: A 4-word sentence creates a 10,000-dimensional vector that's 99.96% zeros\u2014wasteful and uninformative. Embeddings compress this to 300 dense dimensions that actually encode meaning. For a corpus of 1 million documents, that's 39 GB vs 1.2 GB.</p> <p>Source: <code>slide_computations/module8_examples.py</code> - <code>demo_bow_sparsity()</code></p>"},{"location":"modules/08-nlp/#tf-idf","title":"TF-IDF","text":"<p>Improvement: Weight words by importance.</p> \\[\\text{TF-IDF} = \\text{TF}(t,d) \\times \\log\\frac{N}{\\text{DF}(t)}\\] <ul> <li>TF (Term Frequency): How often the word appears in this document</li> <li>IDF (Inverse Document Frequency): How rare the word is across all documents</li> </ul> <p>Common words like \"the\" and \"is\" \u2192 low weight Distinctive words \u2192 high weight</p> <p>Why the log in IDF?</p> <ol> <li> <p>Dampening effect: Without log, a word appearing in 1 vs 1,000 documents would have a 1,000x difference. Log compresses this to about 3x.</p> </li> <li> <p>Prevents domination: Extremely rare words would otherwise overwhelm everything else.</p> </li> </ol> <p>Think of it: the difference between appearing in 1 vs 10 documents is more meaningful than 10,000 vs 10,010. The log captures this diminishing-returns intuition.</p> <pre><code>from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf = TfidfVectorizer()\nX = tfidf.fit_transform(corpus)\n</code></pre> <p>Numerical Example: TF-IDF Calculation by Hand</p> <pre><code>import numpy as np\n\n# Small corpus\ncorpus = [\n    \"the cat sat on the mat\",\n    \"the dog ran in the park\",\n    \"the cat chased the dog\",\n    \"bankruptcy filing announced today\",\n]\nn_docs = 4\n\n# Analyze \"the\" vs \"bankruptcy\" in Doc 1\ndoc1_words = corpus[0].split()\ndoc1_len = len(doc1_words)  # 6 words\n\n# \"the\": appears 2x in doc1, in 3/4 docs\ntf_the = 2 / 6  # 0.333\nidf_the = np.log(4 / 3) + 1  # 1.288\ntfidf_the = tf_the * idf_the\nprint(f\"'the':        TF={tf_the:.3f}, IDF={idf_the:.3f}, TF-IDF={tfidf_the:.3f}\")\n\n# \"bankruptcy\": appears 0x in doc1, in 1/4 docs\ntf_bank = 0 / 6  # 0.000\nidf_bank = np.log(4 / 1) + 1  # 2.386\ntfidf_bank = tf_bank * idf_bank\nprint(f\"'bankruptcy': TF={tf_bank:.3f}, IDF={idf_bank:.3f}, TF-IDF={tfidf_bank:.3f}\")\n</code></pre> <p>Output: <pre><code>'the':        TF=0.333, IDF=1.288, TF-IDF=0.429\n'bankruptcy': TF=0.000, IDF=2.386, TF-IDF=0.000\n</code></pre></p> <p>Interpretation: Even though \"the\" appears twice in Doc 1, its TF-IDF is low because it appears in almost every document (low IDF). \"Bankruptcy\" has high IDF (rare word) but zero TF-IDF in Doc 1 because it doesn't appear there. TF-IDF rewards words that are both frequent in a document AND rare across the corpus.</p> <p>Source: <code>slide_computations/module8_examples.py</code> - <code>demo_tfidf_by_hand()</code></p>"},{"location":"modules/08-nlp/#word-embeddings","title":"Word Embeddings","text":"<p>The breakthrough: Learn dense vectors where similar words are close.</p> <p>Word2Vec (2013): Train a neural network on word prediction. - Skip-gram: Given a word, predict its context words - CBOW: Given context words, predict the target word - Result: 100-300 dimensional vectors per word</p> <p>The key insight: The embedding layer weights ARE the word vectors. Words appearing in similar contexts get similar embeddings.</p> <p>Famous example:</p> \\[king - man + woman \\approx queen\\] <pre><code>from gensim.models import Word2Vec\n\nmodel = Word2Vec(sentences, vector_size=100, window=5, min_count=1)\nmodel.wv.most_similar(positive=['king', 'woman'], negative=['man'])\n</code></pre> <p>This works because the embedding captures semantic relationships! \"King\" and \"queen\" differ in the same way that \"man\" and \"woman\" differ.</p> <p>Think of embeddings as neighborhoods: In the embedding space, similar words are neighbors. The \"royalty neighborhood\" contains king, queen, prince, throne. The \"food neighborhood\" contains apple, banana, pizza. Words can belong to multiple neighborhoods\u2014\"apple\" is near both \"banana\" (fruit) and \"iPhone\" (company). When you subtract \"man\" from \"king,\" you're finding the direction from the \"male\" neighborhood to... somewhere. Adding \"woman\" then moves in the \"female\" direction. You end up in the same relative position as queen.</p> <p>How Word2Vec learns relationships: Word2Vec never sees labeled examples of gender or royalty\u2014these emerge from the distributional hypothesis (words in similar contexts have similar meanings). The model sees \"king\" near \"throne,\" \"crown,\" \"ruled\"; so does \"queen.\" To minimize prediction error, the embedding must encode that \"king \u2192 queen\" is the same direction as \"man \u2192 woman.\" This emergent structure falls out naturally from simple prediction tasks on large corpora.</p> <p>Numerical Example: Embedding Similarity</p> <pre><code>import numpy as np\n\n# Simulated word embeddings (50 dimensions, normalized)\n# Constructed so king-man+woman \u2248 queen\nnp.random.seed(42)\n\ndef cosine_sim(a, b):\n    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n\n# Create embeddings with semantic structure\nbase = np.random.randn(50)\ngender_dir = np.random.randn(50) * 0.5\nroyalty_dir = np.random.randn(50) * 0.5\n\nembeddings = {\n    \"man\": base,\n    \"woman\": base + gender_dir,\n    \"king\": base + royalty_dir,\n    \"queen\": base + gender_dir + royalty_dir,\n    \"banana\": np.random.randn(50),\n}\n\n# Cosine similarities\nprint(\"Cosine similarities:\")\nprint(f\"  king \u2194 queen:  {cosine_sim(embeddings['king'], embeddings['queen']):+.3f}\")\nprint(f\"  king \u2194 man:    {cosine_sim(embeddings['king'], embeddings['man']):+.3f}\")\nprint(f\"  king \u2194 banana: {cosine_sim(embeddings['king'], embeddings['banana']):+.3f}\")\n\n# Analogy: king - man + woman = ?\nresult = embeddings[\"king\"] - embeddings[\"man\"] + embeddings[\"woman\"]\nprint(f\"\\nking - man + woman closest to:\")\nfor word, emb in embeddings.items():\n    print(f\"  {word}: {cosine_sim(result, emb):+.3f}\")\n</code></pre> <p>Output: <pre><code>Cosine similarities:\n  king \u2194 queen:  +0.915\n  king \u2194 man:    +0.860\n  king \u2194 banana: +0.233\n\nking - man + woman closest to:\n  man: +0.765\n  woman: +0.863\n  king: +0.920\n  queen: +1.000 \u2190 closest!\n  banana: +0.228\n</code></pre></p> <p>Interpretation: Similar words (king/queen) have high cosine similarity (~0.9), while unrelated words (king/banana) have low similarity (~0.2). The analogy works because vector arithmetic preserves the learned relationships: subtracting \"man\" and adding \"woman\" moves in the gender direction, landing closest to \"queen.\"</p> <p>Source: <code>slide_computations/module8_examples.py</code> - <code>demo_embedding_similarity()</code></p>"},{"location":"modules/08-nlp/#why-context-matters","title":"Why Context Matters","text":"<p>Word embeddings are powerful, but they miss context:</p> <p>Word order: - \"Nick ate the pizza\" vs \"The pizza ate Nick\" - Same words, completely different meaning</p> <p>Negation: - \"The movie was good\" vs \"The movie was not good\" - BoW and simple embeddings can't distinguish these</p> <p>Reference: - \"The dog didn't cross the road because it was tired\" - \"The dog didn't cross the road because it was wide\" - What does \"it\" refer to? Depends on context!</p> <p>Key insight: We need models that understand sequences and context.</p>"},{"location":"modules/08-nlp/#common-misconceptions","title":"Common Misconceptions","text":"Misconception Reality \"Word embeddings understand meaning\" Embeddings capture statistical patterns, not true understanding \"Pre-trained embeddings work for any domain\" Domain-specific training often helps (medical, legal) \"More dimensions = better embeddings\" Diminishing returns; 100-300 usually sufficient"},{"location":"modules/08-nlp/#82-recurrent-neural-networks","title":"8.2 Recurrent Neural Networks","text":""},{"location":"modules/08-nlp/#rnn-architecture","title":"RNN Architecture","text":"<p>Problem: Standard neural networks can't handle variable-length sequences or remember previous inputs.</p> <p>Solution: Process sequences one element at a time, maintaining memory.</p> \\[h_t = \\tanh(W_{xh}x_t + W_{hh}h_{t-1} + b)\\] <p></p> <p>Reading the diagram: This shows an RNN \"unrolled\" through time\u2014the same network repeated at each timestep. Reading left to right: blue circles (x1, x2, x3, x4) are inputs at each timestep (e.g., word embeddings). Each input feeds into a purple hidden state box (h1, h2, h3, h4), which also receives information from the previous hidden state via the orange arrows. Orange outputs (y1, y2, y3, y4) can be produced at each step. The key insight: h2 contains information from both x2 AND x1 (via h1). By h4, the hidden state has seen the entire sequence\u2014but early information may be degraded after passing through multiple transformations.</p> <p>The hidden state \\(h\\) carries information through time.</p> <p>Think of it like passing notes in class: Each student (timestep) receives a note from the previous student, reads the new information (input), writes a combined summary, and passes it forward. By the end of the row, the final note contains a compressed summary of everything\u2014but details from early students may be garbled or lost. This is both the power and limitation of RNNs: the hidden state must compress all history into a fixed-size vector.</p> <p>Why tanh? 1. Output range [-1, 1]: Can represent \"opposite\" concepts 2. Zero-centered: Helps gradients flow in both directions 3. Stronger gradients: Maximum gradient is 1 (vs 0.25 for sigmoid) 4. Bounded: Prevents hidden states from exploding</p> <p>Numerical Example: RNN Hidden State Evolution</p> <pre><code>import numpy as np\n\nnp.random.seed(42)\n\n# Simple RNN: h_t = tanh(W_xh @ x_t + W_hh @ h_{t-1})\ninput_dim, hidden_dim = 4, 3\nW_xh = np.random.randn(hidden_dim, input_dim) * 0.5\nW_hh = np.random.randn(hidden_dim, hidden_dim) * 0.5\n\n# Word embeddings for \"I love ML\"\nwords = [\"I\", \"love\", \"ML\"]\nembeddings = {\n    \"I\": np.array([0.2, -0.1, 0.3, 0.1]),\n    \"love\": np.array([0.8, 0.5, -0.2, 0.3]),\n    \"ML\": np.array([0.1, 0.4, 0.6, -0.1]),\n}\n\nh = np.zeros(hidden_dim)  # Initial hidden state\n\nfor t, word in enumerate(words):\n    x = embeddings[word]\n    h_new = np.tanh(W_xh @ x + W_hh @ h)\n    print(f\"t={t+1} '{word}': h = [{', '.join(f'{v:+.2f}' for v in h_new)}]\")\n    h = h_new\n\nprint(f\"\\nFinal h encodes: 'I' \u2192 'love' \u2192 'ML'\")\n</code></pre> <p>Output: <pre><code>t=1 'I':    h = [+0.23, +0.26, -0.17]\nt=2 'love': h = [+0.25, -0.39, -0.45]\nt=3 'ML':   h = [+0.72, +0.41, -0.19]\n\nFinal h encodes: 'I' \u2192 'love' \u2192 'ML'\n</code></pre></p> <p>Interpretation: Each hidden state combines the current input with the previous hidden state. By t=3, h\u2083 contains information from all three words\u2014but compressed into just 3 numbers. The same weights (W_xh, W_hh) are used at every timestep, so the RNN learns patterns that generalize across positions.</p> <p>Source: <code>slide_computations/module8_examples.py</code> - <code>demo_rnn_hidden_state()</code></p>"},{"location":"modules/08-nlp/#the-vanishing-gradient-problem","title":"The Vanishing Gradient Problem","text":"<p>The challenge: Gradients shrink exponentially through timesteps.</p> <p>If you're processing a 100-word sentence, gradients from word 100 need to flow back to word 1. But multiplied through 100 steps, they become tiny.</p> <p>The multiplicative decay problem: If each backpropagation step multiplies the gradient by 0.9 (a reasonable value for tanh derivatives), after 100 steps you have 0.9\u00b9\u2070\u2070 \u2248 0.00003. The gradient has shrunk to 0.003% of its original size! Information from word 1 effectively has no influence on learning by the time the gradient reaches it.</p> <p>Result: The RNN \"forgets\" early parts of long sequences.</p> <p>Numerical Example: Vanishing Gradient</p> <pre><code>import numpy as np\n\n# Gradient multiplied at each timestep by factor &lt; 1\nfactor = 0.9  # Typical tanh derivative average\n\nprint(\"Gradient decay through sequence:\")\nprint(f\"{'Timesteps':&gt;12} {'Remaining Gradient':&gt;20}\")\nprint(\"-\" * 35)\n\nfor t in [1, 10, 25, 50, 100]:\n    remaining = factor ** t\n    print(f\"{t:&gt;12} {remaining:&gt;20.10f}\")\n\n# What this means for learning\nprint(f\"\\nAfter 100 timesteps:\")\nprint(f\"  Gradient reduced to: {0.9**100:.6f} = {0.9**100*100:.4f}%\")\nprint(f\"  Information from word 1 has almost no influence on learning\")\n</code></pre> <p>Output: <pre><code>Gradient decay through sequence:\n   Timesteps    Remaining Gradient\n-----------------------------------\n           1           0.9000000000\n          10           0.3486784401\n          25           0.0717897988\n          50           0.0051537752\n         100           0.0000265614\n\nAfter 100 timesteps:\n  Gradient reduced to: 0.000027 = 0.0027%\n  Information from word 1 has almost no influence on learning\n</code></pre></p> <p>Interpretation: With each timestep, gradients are multiplied by ~0.9. After just 50 steps, only 0.5% of the gradient remains. After 100 steps, the gradient is 0.003% of its original value\u2014essentially zero. This is why standard RNNs cannot learn long-range dependencies: the error signal from late words never reaches early words during training.</p> <p>Source: <code>slide_computations/module8_examples.py</code> - <code>demo_vanishing_gradient()</code></p>"},{"location":"modules/08-nlp/#lstm-long-short-term-memory","title":"LSTM: Long Short-Term Memory","text":"<p>Solution: Gated architecture with explicit memory.</p> <p>Three gates: 1. Forget gate: What to remove from memory 2. Input gate: What new information to add 3. Output gate: What to output</p> <p>Cell state: A highway for information to flow unchanged through time.</p> <p>Think of it like a secretary managing a filing cabinet: The filing cabinet (cell state) holds long-term memory. When new information arrives, the secretary decides: (1) What old files to shred (forget gate), (2) What new information to file away (input gate), and (3) What to pull out for the current task (output gate). Unlike the \"passing notes\" RNN where everything gets rewritten each step, the filing cabinet preserves information until explicitly discarded.</p> <p>The gates learn when to keep information and when to forget it.</p> <pre><code>lstm = nn.LSTM(\n    input_size=100,\n    hidden_size=256,\n    num_layers=2,\n    batch_first=True,\n    bidirectional=True\n)\n</code></pre> <p>Connection to attention: LSTM gates pioneered the idea of selective information access. Attention generalizes this\u2014instead of a single memory cell, attention lets the model look back at any previous position.</p>"},{"location":"modules/08-nlp/#gru-gated-recurrent-unit","title":"GRU: Gated Recurrent Unit","text":"<p>Simplified LSTM with fewer parameters.</p> <p>Two gates: 1. Reset gate: How much past to forget 2. Update gate: How much to update the hidden state</p> <p>Often performs similarly to LSTM but trains faster.</p>"},{"location":"modules/08-nlp/#rnn-limitations","title":"RNN Limitations","text":"<ol> <li>Sequential processing: Can't parallelize\u2014each step depends on the previous</li> <li>Long-range dependencies: Still struggle with very long sequences</li> <li>Fixed representation: A single hidden vector must capture everything</li> </ol> <p>These limitations motivated transformers.</p> <p>Why RNNs dominated before transformers: They were the best available option. Before RNNs: n-gram models (limited context, exponential parameters) and HMMs (restrictive assumptions). LSTMs/GRUs mitigated vanishing gradients; attention mechanisms (2014-2015) addressed the fixed-representation bottleneck. The 2017 transformer paper showed attention alone was sufficient, but required significant innovations (positional encoding, Q/K/V formulation) plus computational resources. Progress looks obvious in retrospect.</p>"},{"location":"modules/08-nlp/#83-transformers","title":"8.3 Transformers","text":""},{"location":"modules/08-nlp/#attention-is-all-you-need-2017","title":"\"Attention Is All You Need\" (2017)","text":"<p>This paper changed everything.</p> <p>The key insight: Replace recurrence with attention.</p> <p>Benefits: - Parallel processing: Process all tokens simultaneously - Direct connections: Any position can attend to any other - Better long-range dependencies: No vanishing gradient through 100 steps</p>"},{"location":"modules/08-nlp/#self-attention","title":"Self-Attention","text":"<p>Core idea: Each word looks at all other words to understand context.</p> <p>Query, Key, Value: - Query (Q): What am I looking for? - Key (K): What do I contain? - Value (V): What information do I provide?</p> <p>Think of it like searching a library: You have a question (Query). Each book has a title and keywords (Keys) that describe what it contains. The book's actual content is the Value. You compare your question against all book titles (Q\u00b7K), find the most relevant matches (softmax), then read and combine information from those books (weighted sum of Values). A word asking \"what does 'it' refer to?\" searches all other words' keys, finds \"cat\" is most relevant, and copies cat's information.</p> \\[\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\\] <p>Intuition: 1. Compute similarity between query and all keys 2. Normalize with softmax \u2192 attention weights 3. Weighted sum of values</p> <p>Example: \"The cat sat on the mat because it was tired\"</p> <p>When processing \"it\": - Compute similarity with all words - \"it\" should attend most strongly to \"cat\" - Copy information from \"cat\" to understand what \"it\" refers to</p> <p>How attention learns coreference: Entirely through training\u2014nothing programmed in. \"It was tired\" makes sense if \"it\" attends to \"cat\" (animals get tired), not \"mat.\" The Q/K/V projection matrices adjust so \"it\" and \"cat\" have high dot product. Different heads specialize: one for coreference, another for syntax, another for local context. The model discovers these patterns; engineers didn't program them.</p> <p>Numerical Example: Self-Attention Step by Step</p> <pre><code>import numpy as np\n\nnp.random.seed(42)\n\n# 3-word sentence, 4-dim embeddings, 3-dim Q/K/V\nwords = [\"The\", \"cat\", \"sat\"]\nX = np.array([[0.1, 0.2, 0.3, 0.4],    # The\n              [0.5, 0.6, -0.2, 0.1],   # cat\n              [0.2, -0.1, 0.4, 0.3]])  # sat\n\nW_Q = np.random.randn(4, 3) * 0.5\nW_K = np.random.randn(4, 3) * 0.5\n\nQ = X @ W_Q  # Queries\nK = X @ W_K  # Keys\n\n# Attention scores: Q @ K^T\nscores = Q @ K.T\nscaled = scores / np.sqrt(3)  # Scale by \u221ad_k\n\n# Softmax\ndef softmax(x):\n    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n\nattention = softmax(scaled)\n\nprint(\"Attention weights (each row = what that word attends to):\")\nprint(f\"       {'The':&gt;6} {'cat':&gt;6} {'sat':&gt;6}\")\nfor i, word in enumerate(words):\n    print(f\"{word:&gt;6} {attention[i,0]:&gt;6.2f} {attention[i,1]:&gt;6.2f} {attention[i,2]:&gt;6.2f}\")\n</code></pre> <p>Output: <pre><code>Attention weights (each row = what that word attends to):\n          The    cat    sat\n   The   0.32   0.35   0.33\n   cat   0.33   0.34   0.33\n   sat   0.33   0.34   0.33\n</code></pre></p> <p>Interpretation: Each row shows where that word \"looks.\" With random weights, attention is nearly uniform. After training, you'd see patterns like \"sat\" attending strongly to \"cat\" (subject-verb relationship) or pronouns attending to their referents. The softmax ensures weights sum to 1, creating a weighted average of all positions.</p> <p>Source: <code>slide_computations/module8_examples.py</code> - <code>demo_self_attention()</code></p>"},{"location":"modules/08-nlp/#why-scale-by-d_k","title":"Why Scale by \u221ad_k?","text":"<p>Dot products grow with dimension. If d_k is large, dot products can be very large, pushing softmax into saturation (all attention on one token). Scaling keeps variance roughly constant.</p>"},{"location":"modules/08-nlp/#multi-head-attention","title":"Multi-Head Attention","text":"<p>Why multiple heads? Different heads can attend to different things.</p> <ul> <li>One head might focus on syntax (subject-verb agreement)</li> <li>Another might focus on semantics (what \"it\" refers to)</li> <li>Another might focus on nearby context</li> </ul> <pre><code>multihead_attn = nn.MultiheadAttention(\n    embed_dim=512,\n    num_heads=8\n)\n</code></pre> <p>Eight heads, each with 64 dimensions, capturing different relationships.</p>"},{"location":"modules/08-nlp/#positional-encoding","title":"Positional Encoding","text":"<p>Problem: Attention is permutation-invariant. It doesn't know word order!</p> <p>Solution: Add position information to embeddings.</p> \\[PE_{pos,2i} = \\sin(pos / 10000^{2i/d})\\] \\[PE_{pos,2i+1} = \\cos(pos / 10000^{2i/d})\\] <p>Where: - \\(pos\\) = position in sequence (0, 1, 2, ...) - \\(d\\) = embedding dimension - \\(i\\) = dimension index, ranging from 0 to \\(d/2 - 1\\)</p> <p>Think of it like clock hands: Low dimensions are like a second hand\u2014they cycle rapidly (short wavelength), changing noticeably between adjacent positions. High dimensions are like an hour hand\u2014they cycle slowly (long wavelength), barely changing between nearby positions but clearly different across the sequence. Together, they create a unique \"fingerprint\" for each position. Just as you can tell time by combining all hands, the model can determine position from the combined pattern.</p> <p>Numerical Example: Positional Encoding Patterns</p> <pre><code>import numpy as np\n\nd_model = 8\nmax_pos = 6\n\n# Compute positional encodings\nPE = np.zeros((max_pos, d_model))\nfor pos in range(max_pos):\n    for i in range(d_model // 2):\n        denom = 10000 ** (2 * i / d_model)\n        PE[pos, 2*i] = np.sin(pos / denom)\n        PE[pos, 2*i + 1] = np.cos(pos / denom)\n\nprint(\"Positional encodings (dims 0-3):\")\nprint(f\"Pos  {'dim0':&gt;7} {'dim1':&gt;7} {'dim2':&gt;7} {'dim3':&gt;7}\")\nfor pos in range(max_pos):\n    print(f\"{pos:&gt;3}  {PE[pos,0]:+.3f}  {PE[pos,1]:+.3f}  {PE[pos,2]:+.3f}  {PE[pos,3]:+.3f}\")\n\n# Wavelengths\nprint(f\"\\nWavelength (positions per cycle):\")\nfor i in range(d_model // 2):\n    wl = 2 * np.pi * (10000 ** (2*i/d_model))\n    print(f\"  dims {2*i},{2*i+1}: {wl:.1f} positions\")\n</code></pre> <p>Output: <pre><code>Positional encodings (dims 0-3):\nPos     dim0    dim1    dim2    dim3\n  0  +0.000  +1.000  +0.000  +1.000\n  1  +0.841  +0.540  +0.100  +0.995\n  2  +0.909  -0.416  +0.199  +0.980\n  3  +0.141  -0.990  +0.296  +0.955\n  4  -0.757  -0.654  +0.389  +0.921\n  5  -0.959  +0.284  +0.479  +0.878\n\nWavelength (positions per cycle):\n  dims 0,1: 6.3 positions\n  dims 2,3: 62.8 positions\n  dims 4,5: 628.3 positions\n  dims 6,7: 6283.2 positions\n</code></pre></p> <p>Interpretation: Dims 0,1 complete a full cycle every ~6 positions (fast \"second hand\"). Dims 6,7 take ~6,000 positions to cycle (slow \"hour hand\"). Each position gets a unique combination of values. The model learns to use these patterns to determine both absolute position and relative distances between tokens.</p> <p>Source: <code>slide_computations/module8_examples.py</code> - <code>demo_positional_encoding()</code></p> <p>Different frequencies let the model learn to attend to relative positions.</p>"},{"location":"modules/08-nlp/#encoder-vs-decoder","title":"Encoder vs Decoder","text":"<p>Encoder (BERT-style): - Processes entire sequence at once - Bidirectional context (see past and future) - Good for understanding and classification</p> <p>Decoder (GPT-style): - Generates sequence left-to-right - Causal masking (can only see past) - Good for text generation</p> <p>Encoder-Decoder (T5): - Encoder processes input - Decoder generates output - Good for translation, summarization</p>"},{"location":"modules/08-nlp/#common-misconceptions_1","title":"Common Misconceptions","text":"Misconception Reality \"Transformers understand language\" They learn statistical patterns, not true understanding \"Attention = interpretability\" Attention weights don't always align with human intuition \"Bigger models are always better\" Diminishing returns; efficiency matters"},{"location":"modules/08-nlp/#84-foundation-models","title":"8.4 Foundation Models","text":""},{"location":"modules/08-nlp/#pre-training-fine-tuning","title":"Pre-training \u2192 Fine-tuning","text":"<p>Pre-training: Train on massive text (expensive!) - Billions of words - Millions of dollars in compute - Done once by big labs</p> <p>Fine-tuning: Adapt to your task (cheap!) - Your data + pre-trained model - Hours, not weeks</p> <p>Zero-shot: Use directly with prompts - No training needed - Just ask the model</p>"},{"location":"modules/08-nlp/#bert","title":"BERT","text":"<p>Bidirectional Encoder Representations from Transformers</p> <p>Pre-training: - Masked Language Modeling: Predict masked words from context - Next Sentence Prediction: Does sentence B follow sentence A?</p> <p>Use cases: - Text classification - Named entity recognition - Question answering - Semantic similarity</p> <pre><code>from transformers import BertTokenizer, BertForSequenceClassification\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertForSequenceClassification.from_pretrained(\n    'bert-base-uncased',\n    num_labels=2\n)\n\ninputs = tokenizer(\n    \"This movie was great!\",\n    return_tensors=\"pt\",\n    padding=True,\n    truncation=True\n)\noutputs = model(**inputs)\n</code></pre>"},{"location":"modules/08-nlp/#gpt-family","title":"GPT Family","text":"<p>Generative Pre-trained Transformer</p> <p>Architecture: Decoder-only (autoregressive)</p> <p>Capabilities: - Text generation - Zero/few-shot learning - Instruction following (ChatGPT)</p> <p>Scale evolution: - GPT (2018): 117M parameters - GPT-2 (2019): 1.5B parameters - GPT-3 (2020): 175B parameters - GPT-4 (2023): Multimodal, even larger</p>"},{"location":"modules/08-nlp/#bert-vs-gpt","title":"BERT vs GPT","text":"Aspect BERT GPT Architecture Encoder Decoder Context Bidirectional Left-to-right Best for Understanding Generation Training Masked LM Next token prediction <p>When to use which?</p> <p>Use BERT for classification, NER, and understanding tasks\u2014especially with labeled training data.</p> <p>Use GPT for generation tasks, or when you want to leverage prompting without training data.</p> <p>A practical decision framework: | Your Situation | Recommendation | |----------------|----------------| | &lt;100 labeled examples | GPT zero/few-shot | | 100-1,000 labeled examples | Try both, compare | | &gt;1,000 labeled examples | Fine-tune BERT (likely wins) | | Need real-time inference | BERT (faster, cheaper) | | Need to generate text | GPT | | Domain-specific vocabulary | Fine-tune either on domain text |</p> <p>Why fine-tune BERT vs. zero-shot GPT? (1) Task-specific performance: fine-tuned BERT typically achieves higher accuracy with sufficient training data. (2) Cost/latency: BERT-base (110M params) is orders of magnitude cheaper than GPT-4 (1T+ params). (3) Consistency: fine-tuned models are deterministic; GPT varies with temperature and prompts. (4) Domain adaptation and data privacy (local training vs. API calls). Use both strategically: GPT for exploration, fine-tuned BERT for production systems.</p> <p>Numerical Example: BERT vs GPT Scale Comparison</p> <pre><code>models = {\n    \"BERT-base\":  {\"params\": 110_000_000,  \"layers\": 12, \"hidden\": 768},\n    \"BERT-large\": {\"params\": 340_000_000,  \"layers\": 24, \"hidden\": 1024},\n    \"GPT-2\":      {\"params\": 1_500_000_000, \"layers\": 48, \"hidden\": 1600},\n    \"GPT-3\":      {\"params\": 175_000_000_000, \"layers\": 96, \"hidden\": 12288},\n}\n\nprint(f\"{'Model':&lt;12} {'Parameters':&gt;12} {'Layers':&gt;8} {'Hidden':&gt;8}\")\nprint(\"-\" * 45)\nfor name, specs in models.items():\n    p = specs['params']\n    p_str = f\"{p/1e9:.1f}B\" if p &gt;= 1e9 else f\"{p/1e6:.0f}M\"\n    print(f\"{name:&lt;12} {p_str:&gt;12} {specs['layers']:&gt;8} {specs['hidden']:&gt;8}\")\n\n# Relative cost (rough)\nbert_base = 110e6\nprint(f\"\\nRelative inference cost (vs BERT-base):\")\nfor name, specs in models.items():\n    ratio = specs['params'] / bert_base\n    print(f\"  {name}: ~{ratio:.0f}x\")\n</code></pre> <p>Output: <pre><code>Model         Parameters   Layers   Hidden\n---------------------------------------------\nBERT-base          110M       12      768\nBERT-large         340M       24     1024\nGPT-2              1.5B       48     1600\nGPT-3            175.0B       96    12288\n\nRelative inference cost (vs BERT-base):\n  BERT-base: ~1x\n  BERT-large: ~3x\n  GPT-2: ~14x\n  GPT-3: ~1591x\n</code></pre></p> <p>Interpretation: GPT-3 is ~1,600x more expensive to run than BERT-base. For a classification task processing 1 million documents, BERT-base might cost $10 while GPT-3 costs $16,000. This is why production systems often use fine-tuned BERT for tasks where it performs well\u2014the cost difference is dramatic.</p> <p>Source: <code>slide_computations/module8_examples.py</code> - <code>demo_bert_vs_gpt_scale()</code></p>"},{"location":"modules/08-nlp/#business-applications","title":"Business Applications","text":"Application Model Example Sentiment Analysis BERT Product reviews Chatbot GPT Customer support Classification BERT Email routing Named Entity Recognition BERT Extract entities Text Generation GPT Marketing copy Summarization T5, BART Meeting notes"},{"location":"modules/08-nlp/#85-beyond-text","title":"8.5 Beyond Text","text":""},{"location":"modules/08-nlp/#vision-transformers-vit","title":"Vision Transformers (ViT)","text":"<ul> <li>Split images into patches</li> <li>Treat patches as \"tokens\"</li> <li>Apply transformer encoder</li> <li>State-of-the-art on many vision benchmarks</li> </ul>"},{"location":"modules/08-nlp/#audio-processing","title":"Audio Processing","text":"<ul> <li>Whisper: Speech recognition</li> <li>wav2vec: Audio embeddings</li> </ul>"},{"location":"modules/08-nlp/#multimodal-models","title":"Multimodal Models","text":"<ul> <li>CLIP: Connect images and text</li> <li>DALL-E: Generate images from text</li> <li>GPT-4V: Vision + language</li> </ul> <p>Key insight: Transformer architecture is general-purpose, not just for text.</p>"},{"location":"modules/08-nlp/#reflection-questions","title":"Reflection Questions","text":"<ol> <li> <p>Why does 'king - man + woman \u2248 queen' work with word embeddings?</p> </li> <li> <p>A BoW model can't tell 'dog bites man' from 'man bites dog'. Why not? What's needed to fix this?</p> </li> <li> <p>You're building a document search engine. Would you use BoW, TF-IDF, or embeddings? Why?</p> </li> <li> <p>Why can't a standard feedforward network process variable-length text?</p> </li> <li> <p>An LSTM processes a 100-word sentence. How does information from word 1 reach the output?</p> </li> <li> <p>Why is self-attention more parallelizable than RNNs?</p> </li> <li> <p>In \"The animal didn't cross the road because it was tired\", what should 'it' attend to?</p> </li> <li> <p>Why does BERT use bidirectional attention while GPT uses causal attention?</p> </li> <li> <p>When would you fine-tune BERT vs use GPT with prompting?</p> </li> </ol>"},{"location":"modules/08-nlp/#practice-problems","title":"Practice Problems","text":"<ol> <li> <p>For a vocabulary of 10,000 words and a 5-word document, what's the dimensionality of BoW vs a 300-dim embedding?</p> </li> <li> <p>Calculate TF-IDF for a word appearing 3 times in a document, when it appears in 100 of 10,000 documents.</p> </li> <li> <p>Explain why RNNs suffer from vanishing gradients but LSTMs partially solve this.</p> </li> <li> <p>Given Q, K, V matrices, trace through the self-attention computation.</p> </li> <li> <p>A company wants to classify support tickets. Recommend BERT vs GPT and justify.</p> </li> </ol>"},{"location":"modules/08-nlp/#chapter-summary","title":"Chapter Summary","text":"<p>Six key takeaways from Module 8:</p> <ol> <li> <p>Text representation evolves: BoW \u2192 TF-IDF \u2192 embeddings \u2192 contextual embeddings</p> </li> <li> <p>RNNs process sequences but struggle with long-range dependencies</p> </li> <li> <p>Transformers use attention for parallel, effective processing</p> </li> <li> <p>Self-attention lets each token consider all others</p> </li> <li> <p>BERT for understanding, GPT for generation</p> </li> <li> <p>Transfer learning makes NLP accessible</p> </li> </ol>"},{"location":"modules/08-nlp/#whats-next","title":"What's Next","text":"<p>In Module 9, we tackle Model Interpretability: - Why do models make decisions? - SHAP values - Attention visualization - Building trust in ML systems</p> <p>We'll use attention from transformers to understand what NLP models focus on!</p>"},{"location":"modules/09-interpretability/","title":"Module 9: Model Interpretability &amp; Explainability","text":""},{"location":"modules/09-interpretability/#introduction","title":"Introduction","text":"<p>We've covered a wide range of modeling techniques: linear models, decision trees, random forests, XGBoost, neural networks, CNNs, and transformers. Some are simple\u2014you can look at coefficients. Others are complex\u2014millions of parameters that no human can comprehend directly.</p> <p>Here's the challenge: A model that can't be explained often can't be deployed.</p> <p>Think about it. A bank denies someone a loan. A hospital's AI recommends a treatment. An insurance company sets a premium. In all these cases, people deserve to know why. And in many cases, the law requires it.</p> <p>This module bridges the gap between model performance and real-world deployment. You'll learn how to explain any model\u2014black box or not\u2014and how to communicate those explanations to stakeholders who don't know (or care) about gradient descent.</p> <p>Interpretability vs. performance: Modern tools largely eliminate this tradeoff. Train a complex model for maximum performance, then use SHAP/LIME to explain it\u2014you get both accuracy and explanations. Intrinsically interpretable models (linear regression, short decision trees) provide explanations directly if regulations require them. A well-regularized linear model can often match tree ensemble performance anyway.</p>"},{"location":"modules/09-interpretability/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, you should be able to:</p> <ol> <li>Explain why model interpretability matters for business and regulatory compliance</li> <li>Distinguish between global and local interpretability</li> <li>Apply SHAP and LIME to explain model predictions</li> <li>Create effective visualizations of model behavior</li> <li>Communicate model insights to non-technical stakeholders</li> <li>Document models with model cards and limitations</li> </ol>"},{"location":"modules/09-interpretability/#91-why-interpretability-matters","title":"9.1 Why Interpretability Matters","text":""},{"location":"modules/09-interpretability/#the-business-case","title":"The Business Case","text":"<p>The best model in the world is worthless if no one trusts it.</p> <p>You could build a fraud detection system with 99% accuracy. But if the compliance team can't explain why it flagged a transaction, they can't defend that decision to regulators. If loan officers can't explain why an application was denied, they can't legally send that denial letter.</p>"},{"location":"modules/09-interpretability/#regulatory-requirements","title":"Regulatory Requirements","text":"<p>GDPR (EU General Data Protection Regulation): - Citizens have a \"right to explanation\" for automated decisions - If a machine makes a decision that significantly affects someone, they can demand to know why - Applies to credit scoring, hiring, insurance, healthcare</p> <p>Fair Lending Laws (US): - Equal Credit Opportunity Act requires reasons for adverse actions - \"Your application was denied because...\" is legally required - \"The algorithm said no\" doesn't satisfy the law</p> <p>Healthcare Regulations: - FDA scrutinizes AI medical devices - Clinicians need to understand recommendations before acting - Liability concerns: if something goes wrong, why did the AI recommend that?</p>"},{"location":"modules/09-interpretability/#building-stakeholder-trust","title":"Building Stakeholder Trust","text":"<p>Business stakeholders want to know: - Why did the model make this prediction? - Which factors are most important? - Can we trust this prediction? - What would change the prediction?</p> <p>Without trust: - Models won't be adopted\u2014people ignore recommendations - Decisions get overridden\u2014defeating the model's purpose - ML investment value is lost\u2014months of work unused</p>"},{"location":"modules/09-interpretability/#debugging-and-improving-models","title":"Debugging and Improving Models","text":"<p>Interpretability helps identify: - Spurious correlations: Model learned wrong patterns - Data leakage: Model using information it shouldn't have - Bias in training data: Historical biases encoded in predictions - Overfitting: Model memorized patterns that won't generalize</p> <p>The pneumonia example:</p> <p>Researchers trained a model to predict pneumonia severity from X-rays. The model performed exceptionally well\u2014too well.</p> <p>Investigation revealed: The model learned to associate \"portable X-ray\" equipment markers with low risk. Why? Portable X-rays were used for patients well enough to not need a trip to the radiology department. The model was predicting equipment type, not disease severity.</p> <p>Without interpretability tools, this would have been deployed and potentially harmed patients.</p> <p>Catching spurious correlations: For consequential models, investigating what the model learned is a professional responsibility. Use SHAP/LIME/PDP in your standard workflow. Show top features to domain experts (a radiologist would question equipment markers). Ask: \"What shortcuts could the model have taken?\" Test on out-of-distribution data. The investigation level should match the stakes\u2014product recommendations warrant less scrutiny than medical diagnosis.</p>"},{"location":"modules/09-interpretability/#discovering-bias","title":"Discovering Bias","text":"<p>ML models can encode and amplify biases: - Historical bias in training data - Proxy variables for protected attributes - Feedback loops</p> <p>Interpretability reveals: - Which features drive predictions for different groups - Whether protected attributes have indirect influence - Unexpected correlations that might indicate bias</p> <p>Example: A hiring model heavily weights ZIP code. ZIP code correlates with race and income. The model might be making discriminatory decisions even without explicit race features. This is proxy discrimination\u2014often unethical and sometimes illegal.</p>"},{"location":"modules/09-interpretability/#common-misconceptions","title":"Common Misconceptions","text":"Misconception Reality \"Accuracy is all that matters\" Without interpretability, you can't trust, debug, or deploy responsibly \"Deep learning can never be interpreted\" Many techniques exist (SHAP, attention, feature visualization) \"Simple models are always more interpretable\" A 100-feature linear model isn't necessarily interpretable"},{"location":"modules/09-interpretability/#92-interpretation-techniques","title":"9.2 Interpretation Techniques","text":""},{"location":"modules/09-interpretability/#global-vs-local-interpretability","title":"Global vs Local Interpretability","text":"<p>Global interpretability: Understand overall model behavior - Which features are generally important? - What patterns does the model use?</p> <p>Local interpretability: Understand individual predictions - Why was THIS customer predicted to churn? - What would change THIS decision?</p> <p>Both matter. Executives want global insights: \"What drives churn?\" Customer service needs local explanations: \"Why was this specific customer flagged?\"</p> <p>The forest vs. tree analogy: Think of global interpretability as understanding the forest\u2014stepping back to see the overall patterns, which species are most common, how the ecosystem works. Local interpretability is examining a single tree\u2014why is this particular tree thriving or dying? You need both perspectives. A forester managing the whole forest needs global patterns; a botanist treating a sick tree needs local diagnosis.</p>"},{"location":"modules/09-interpretability/#permutation-importance","title":"Permutation Importance","text":"<p>The idea: 1. Train model, measure baseline performance 2. Shuffle one feature's values (break its signal) 3. Measure performance drop 4. Larger drop = more important feature</p> <p>Why it works: If a feature is important, breaking its signal hurts predictions.</p> <p>The blindfold test: Imagine testing how much a basketball player relies on their vision. Blindfold them and see how much worse they play. If performance drops dramatically, vision was important. If they still play well (maybe they're great at listening for the ball), vision wasn't crucial. Permutation importance \"blindfolds\" each feature one at a time and measures how much the model's performance degrades.</p> <pre><code>from sklearn.inspection import permutation_importance\n\nresult = permutation_importance(\n    model,\n    X_test,\n    y_test,\n    n_repeats=10,\n    random_state=42\n)\n\n# Sort by importance\nfor i in result.importances_mean.argsort()[::-1]:\n    print(f\"{feature_names[i]}: {result.importances_mean[i]:.3f}\")\n</code></pre> <p>Advantages: - Works with any model (model-agnostic) - Uses held-out test data (reliable)</p> <p>Disadvantages: - Slow for many features - Misleading with correlated features (shuffling one is compensated by another)</p> <p>Numerical Example: Permutation Importance Step by Step</p> <pre><code># Dataset: x1 (strong signal), x2 (moderate), x3 (noise)\n# Target depends on: 2*x1 + 0.5*x2, not on x3\n\n# Train Random Forest and measure baseline accuracy\nbaseline_accuracy = 0.647  # 64.7%\n\n# Shuffle each feature and measure performance drop\n# x1 (strong): shuffle \u2192 accuracy drops to 40.0%\n# x2 (moderate): shuffle \u2192 accuracy stays ~same\n# x3 (noise): shuffle \u2192 accuracy stays ~same\n\nimportance_x1 = 0.647 - 0.400  # = 0.247 (24.7%)\nimportance_x2 = 0.647 - 0.707  # = -0.06 (noise)\nimportance_x3 = 0.647 - 0.660  # = -0.01 (noise)\n</code></pre> <p>Output: <pre><code>x1 (strong): Shuffle \u2192 accuracy drops to 40.0%\n             Importance = 64.7% - 40.0% = 24.7%\nx2 (moderate): Importance \u2248 0% (signal carried by x1)\nx3 (noise): Importance \u2248 0% (model doesn't use it)\n</code></pre></p> <p>Interpretation: Shuffling x1 destroys the main signal, causing a 24.7% accuracy drop. Shuffling noise features has no effect\u2014the model wasn't using them anyway. This is the \"blindfold test\" in action.</p> <p>Source: <code>slide_computations/module9_examples.py</code> - <code>demo_permutation_importance()</code></p>"},{"location":"modules/09-interpretability/#partial-dependence-plots-pdp","title":"Partial Dependence Plots (PDP)","text":"<p>PDPs show the average effect of a feature on predictions.</p> <p>How it works: 1. For each value of feature X (e.g., age from 20 to 80) 2. Set ALL samples to that value 3. Average the predictions 4. Plot average prediction vs feature value</p> <p>The what-if slider: Imagine a dashboard with a slider for each feature. When you drag the \"age\" slider from 20 to 80, the PDP shows how the average prediction changes. It's answering: \"If I could magically set everyone's age to 50, what would the average prediction be?\" This isolates the marginal effect of that feature, averaging over all the other features in the data.</p> <pre><code>from sklearn.inspection import PartialDependenceDisplay\n\nPartialDependenceDisplay.from_estimator(\n    model,\n    X_train,\n    features=['age', 'income']\n)\n</code></pre> <p>Interpretation: - Upward slope: Higher feature value \u2192 higher prediction - Flat line: Little average effect - Non-linear shape: Complex relationship</p> <p>Limitation: Assumes feature independence. Can show impossible combinations (20-year-olds with $500K income).</p> <p>Numerical Example: Building a Partial Dependence Plot</p> <pre><code># Churn prediction model with age, income, tenure\n# PDP for 'age': What happens to average churn as we vary age?\n\n# For each age value, set ALL customers to that age\n# and average the predictions\nage_values = [25, 35, 45, 55, 65]\navg_churn_probs = []\n\nfor age in age_values:\n    X_modified = X.copy()\n    X_modified['age'] = age  # Everyone is now this age\n    avg_prob = model.predict_proba(X_modified)[:, 1].mean()\n    avg_churn_probs.append(avg_prob)\n</code></pre> <p>Output: <pre><code>Age Value    Avg Churn Prob\n--------------------------------\n      25              71.0%\n      35              64.0%\n      45              53.9%\n      55              47.2%\n      65              28.7%\n</code></pre></p> <p>Interpretation: The PDP shows a clear downward trend\u2014as age increases, average churn probability decreases. This is the \"what-if slider\": drag age from 25\u219265 and watch the average prediction drop from 71%\u219229%.</p> <p>Source: <code>slide_computations/module9_examples.py</code> - <code>demo_partial_dependence()</code></p>"},{"location":"modules/09-interpretability/#shap-shapley-additive-explanations","title":"SHAP (SHapley Additive exPlanations)","text":"<p>Foundation: Shapley values from game theory\u2014fairly distribute \"credit\" among players.</p> <p>Applied to ML: How much did each feature contribute to pushing this prediction away from the average?</p> <p>Key properties (mathematically proven): 1. Local accuracy: SHAP values sum to prediction minus baseline 2. Consistency: More important features get higher values 3. Missingness: Unused features get zero attribution</p> <p>Interpretation: - SHAP &gt; 0: Feature pushed prediction higher - SHAP &lt; 0: Feature pushed prediction lower - Magnitude: Strength of effect</p> <p>Understanding Shapley through a concrete example: Before the formula, consider three data scientists (A, B, C) working on a project. Alone, A generates $50k, B generates $40k, C generates $20k. But together, A+B generate $120k (synergy!), and all three generate $150k. How do you fairly split the $150k? Shapley values average each person's marginal contribution across all possible orderings they could have joined. Player A's Shapley value is $66.7k\u2014they get more because they add value in every combination. This is the same math SHAP uses: features are \"players\" and the prediction is the \"payoff.\"</p> <p>The Shapley formula:</p> \\[\\phi_j = \\sum_{S \\subseteq N \\setminus \\{j\\}} \\frac{|S|!(|N|-|S|-1)!}{|N|!} [f(S \\cup \\{j\\}) - f(S)]\\] <p>In plain English: Consider all possible subsets of features. For each subset, measure how much adding feature j changes the prediction. Average these contributions with weights ensuring fairness.</p> <p>Computational complexity: Exact Shapley computation is exponential (2^n subsets). TreeSHAP exploits tree structure for polynomial-time exact values\u2014use it for random forests, XGBoost, LightGBM. DeepSHAP uses gradient approximations for neural networks. KernelSHAP handles arbitrary models but is slow. This often influences model choice: if interpretability + speed are required, tree-based models with TreeSHAP become attractive.</p> <p>Numerical Example: Shapley Values in a Simple Game</p> <pre><code># Three data scientists (A, B, C) work on a project\n# Coalition payoffs (in $1000s):\npayoffs = {\n    '\u2205': 0,      '{A}': 50,   '{B}': 40,   '{C}': 20,\n    '{A,B}': 120,  # A+B have synergy!\n    '{A,C}': 80,   '{B,C}': 70,\n    '{A,B,C}': 150  # Grand coalition\n}\n\n# For each player, average marginal contribution across\n# all orderings they could join:\n\n# Player A joins: \u2205\u2192+50, {B}\u2192+80, {C}\u2192+60, {B,C}\u2192+80\nshapley_A = weighted_average([50, 80, 60, 80])  # = $66.7k\n\n# Player B joins: \u2205\u2192+40, {A}\u2192+70, {C}\u2192+50, {A,C}\u2192+70\nshapley_B = weighted_average([40, 70, 50, 70])  # = $56.7k\n\n# Player C joins: \u2205\u2192+20, {A}\u2192+30, {B}\u2192+30, {A,B}\u2192+30\nshapley_C = weighted_average([20, 30, 30, 30])  # = $26.7k\n</code></pre> <p>Output: <pre><code>Final allocation:\n  A: $66.7k (highest\u2014adds value everywhere)\n  B: $56.7k (good synergy with A)\n  C: $26.7k (consistent but lower contribution)\n  Total: $150.0k (= grand coalition value)\n</code></pre></p> <p>Interpretation: Shapley values are the only allocation that is fair, efficient, and additive. In ML, features are \"players\" and the prediction is the \"payoff\"\u2014SHAP tells us how much each feature contributed to pushing the prediction away from the baseline.</p> <p>Source: <code>slide_computations/module9_examples.py</code> - <code>demo_shapley_game()</code></p>"},{"location":"modules/09-interpretability/#shap-in-practice","title":"SHAP in Practice","text":"<pre><code>import shap\n\n# For tree-based models (fast!)\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_test)\n\n# Summary plot (global view)\nshap.summary_plot(shap_values, X_test)\n\n# Force plot (single prediction)\nshap.force_plot(\n    explainer.expected_value,\n    shap_values[0],\n    X_test.iloc[0]\n)\n\n# Waterfall plot (detailed breakdown)\nshap.waterfall_plot(shap.Explanation(\n    values=shap_values[0],\n    base_values=explainer.expected_value,\n    data=X_test.iloc[0],\n    feature_names=feature_names\n))\n</code></pre> <p>SHAP variants: - TreeSHAP: Exact, fast for tree models - KernelSHAP: Model-agnostic, slower - DeepSHAP: For neural networks</p> <p>Use TreeSHAP when possible\u2014it's exact and fast.</p> <p>Numerical Example: SHAP Values Sum to Prediction</p> <pre><code># For a single test instance:\nbaseline = 0.517  # Average training prediction\nprediction = 0.913  # This instance's prediction\ndifference = prediction - baseline  # = +0.396 to explain\n\n# SHAP breaks down the difference by feature:\nshap_values = {\n    'feature_1': +0.328,  # Pushed prediction UP\n    'feature_2': -0.301,  # Pushed prediction DOWN\n    'feature_3': -0.086,  # Pushed prediction DOWN\n    'feature_4': +0.455,  # Pushed prediction UP\n}\n# Sum: 0.328 + (-0.301) + (-0.086) + 0.455 = +0.396\n</code></pre> <p>Output: <pre><code>Component             Value\n----------------------------------------\nBase value            0.517\nfeature_1         +   0.328\nfeature_2         -   0.301\nfeature_3         -   0.086\nfeature_4         +   0.455\n----------------------------------------\nPrediction            0.913  \u2713\n</code></pre></p> <p>Interpretation: The SHAP additivity property guarantees: base_value + \u03a3(SHAP values) = prediction. Every prediction is fully explained\u2014no residual, no approximation. Feature 4 pushed the prediction up most (+0.455), while features 2 and 3 pushed it down.</p> <p>Source: <code>slide_computations/module9_examples.py</code> - <code>demo_shap_sum_to_prediction()</code></p>"},{"location":"modules/09-interpretability/#shap-visualizations","title":"SHAP Visualizations","text":"<p>Summary plot: Global importance with direction - Each dot is one sample - X-axis: SHAP value - Color: Feature value (red = high, blue = low)</p> <p>Force plot: Single prediction breakdown - Starts from baseline - Shows features pushing up and down - Ends at actual prediction</p> <p>Waterfall plot: Step-by-step breakdown - From baseline to prediction - Each bar is one feature's contribution</p> <p>Dependence plot: Feature effect with interactions - Like PDP but shows actual points - Can color by another feature to see interactions</p> <p>How to read a SHAP summary plot step by step: 1. Look at feature order: Features at the top are most important (widest spread of dots) 2. Find the red dots: Red = high feature value, blue = low feature value 3. See where red clusters: If red dots are on the RIGHT \u2192 high values increase predictions 4. See where blue clusters: If blue dots are on the RIGHT \u2192 low values increase predictions 5. Check the spread: Wide horizontal spread = strong impact; tight cluster at 0 = weak impact</p> <p>Example interpretation: If \"support_tickets\" shows red dots clustered on the right, it means: \"Customers with many support tickets (high value = red) have higher churn predictions (positive SHAP = right).\"</p> <p>Numerical Example: Reading a SHAP Summary Plot</p> <pre><code># Churn prediction model - SHAP summary patterns:\n# (Each feature shows where high/low values cluster)\n\nfeature_patterns = {\n    'support_tickets': 'Red dots RIGHT \u2192 high tickets = higher churn',\n    'months_customer': 'Red dots LEFT \u2192 long tenure = lower churn',\n    'income':          'Red dots LEFT \u2192 higher income = lower churn',\n    'age':             'Dots spread evenly \u2192 weak/noisy effect',\n}\n</code></pre> <p>Output: <pre><code>Feature            High values (red)    Pattern\n----------------------------------------------------------------\nsupport_tickets    \u2192 cluster RIGHT      More tickets = higher churn\nmonths_customer    \u2192 cluster LEFT       Longer tenure = lower churn\nincome             \u2192 cluster LEFT       Higher income = lower churn\nage                \u2192 spread across      Age effect is noisy/weak\n</code></pre></p> <p>Interpretation: The summary plot tells a complete story. Support tickets is the top driver (widest spread), with high values strongly increasing churn risk. Tenure is protective\u2014long-term customers (red) have negative SHAP values (left). Age shows no clear pattern, suggesting it's not a reliable predictor.</p> <p>Source: <code>slide_computations/module9_examples.py</code> - <code>demo_shap_summary_interpretation()</code></p>"},{"location":"modules/09-interpretability/#lime-local-interpretable-model-agnostic-explanations","title":"LIME (Local Interpretable Model-agnostic Explanations)","text":"<p>Core idea: Approximate complex model locally with a simple one.</p> <p>The magnifying glass analogy: A complex model's decision boundary might be wildly curved and twisted at the global level\u2014impossible to describe simply. But if you zoom in with a magnifying glass to a tiny neighborhood around one point, even the most complex curve looks approximately straight. LIME zooms into that local neighborhood, fits a simple linear model that captures the local behavior, and interprets that simple model. The explanation is only valid in that neighborhood\u2014move to a different point and you'd get a different local approximation.</p> <p>How it works: 1. Generate perturbed samples around the instance 2. Get complex model's predictions for those samples 3. Fit simple model (linear) weighted by distance 4. Interpret the simple model</p> <p>\"In the neighborhood of THIS prediction, what does the model behave like?\"</p> <pre><code>import lime.lime_tabular\n\nexplainer = lime.lime_tabular.LimeTabularExplainer(\n    X_train.values,\n    feature_names=feature_names,\n    class_names=['Not Churn', 'Churn'],\n    mode='classification'\n)\n\nexplanation = explainer.explain_instance(\n    X_test.iloc[0].values,\n    model.predict_proba,\n    num_features=10\n)\n\nexplanation.show_in_notebook()\n</code></pre> <p>Numerical Example: LIME Perturbation in Action</p> <pre><code># Original instance: [0.80, 0.90, 0.10] \u2192 P(class=1) = 100%\n# LIME generates nearby perturbed samples and gets predictions:\n\nperturbed_samples = [\n    [0.95, 0.86, 0.29],  # \u2192 94%\n    [1.26, 0.83, 0.03],  # \u2192 100%\n    [0.63, 0.60, 0.19],  # \u2192 98%\n    [0.53, 0.48, 0.54],  # \u2192 100%\n    # ... more samples ...\n]\n\n# Fit weighted linear model (closer samples get more weight)\n# Local linear approximation coefficients:\nlocal_coefs = {\n    'feature_A': +0.041,  # increases prediction locally\n    'feature_B': -0.057,  # decreases prediction locally\n    'feature_C': +0.015,  # slight positive effect\n}\n</code></pre> <p>Output: <pre><code>Local linear approximation (LIME):\n  feature_A: +0.041 (increases prediction)\n  feature_B: -0.057 (decreases prediction)\n  feature_C: +0.015 (increases prediction)\n  Intercept: 0.977\n</code></pre></p> <p>Interpretation: In the neighborhood of this specific instance, the model behaves approximately linearly. Feature A has a positive local effect, while feature B has a negative effect. This explanation is only valid nearby\u2014a different instance might have completely different local behavior.</p> <p>Source: <code>slide_computations/module9_examples.py</code> - <code>demo_lime_perturbation()</code></p>"},{"location":"modules/09-interpretability/#shap-vs-lime","title":"SHAP vs LIME","text":"Aspect SHAP LIME Foundation Game theory Local approximation Consistency Mathematically guaranteed Not guaranteed Speed Fast with TreeSHAP Generally slower Global view Yes (aggregate) Limited <p>Both are valuable. SHAP has stronger theoretical foundations. LIME can be more intuitive.</p> <p>Numerical Example: SHAP vs Permutation Importance</p> <pre><code># Dataset with correlated features:\n# x1: True predictor (target depends on x1)\n# x2: Correlated with x1 (r \u2248 0.96) but not directly causal\n# x3: Independent noise\n\n# Correlation matrix:\n#         x1      x2      x3\n# x1    1.00    0.96   -0.06\n# x2    0.96    1.00   -0.03\n# x3   -0.06   -0.03    1.00\n</code></pre> <p>Output: <pre><code>Feature              Permutation     SHAP (mean |val|)\n-------------------------------------------------------\nx1 (causal)                0.216              0.350\nx2 (correlated)            0.016              0.250\nx3 (noise)                 0.005              0.020\n</code></pre></p> <p>Interpretation: Permutation importance shows x2 as unimportant (0.016) because when x2 is shuffled, x1 still carries the signal. SHAP distributes credit between correlated features, giving x2 a meaningful value (0.250). Neither is \"wrong\"\u2014they answer different questions: - Permutation: \"What if we removed this feature?\" - SHAP: \"How much did each feature contribute to predictions?\"</p> <p>Source: <code>slide_computations/module9_examples.py</code> - <code>demo_shap_vs_permutation()</code></p>"},{"location":"modules/09-interpretability/#important-caveats","title":"Important Caveats","text":"<p>Feature importance \u2260 causation.</p> <p>When SHAP says \"age is the most important feature,\" it means age most influences predictions. It does NOT mean age causes the outcome.</p> <p>A model might use age as a strong predictor of churn, but that doesn't mean getting older causes churn. There might be a confounder.</p> <p>The umbrella sales example: A model predicting outdoor event attendance might show \"umbrella sales\" as the most important feature. But buying umbrellas doesn't cause low attendance\u2014both are caused by rain (a confounder). If you tried to increase attendance by banning umbrella sales, you'd fail miserably. The model correctly learned that umbrella sales predict attendance, but the causal intervention point is weather, not umbrellas. This is why domain expertise matters: a meteorologist would immediately spot the spurious relationship.</p> <p>Don't confuse prediction importance with causal importance.</p>"},{"location":"modules/09-interpretability/#common-misconceptions_1","title":"Common Misconceptions","text":"Misconception Reality \"Feature importance = causation\" Importance shows prediction influence, not causal effect \"SHAP values are always exact\" KernelSHAP is approximate; TreeSHAP is exact only for trees \"High attention = high importance\" Attention weights can be misleading"},{"location":"modules/09-interpretability/#93-communicating-model-insights","title":"9.3 Communicating Model Insights","text":""},{"location":"modules/09-interpretability/#the-communication-challenge","title":"The Communication Challenge","text":"<p>You've learned powerful interpretation techniques. SHAP gives detailed attributions. PDP shows relationships. LIME approximates local behavior.</p> <p>But your stakeholders don't care about SHAP values.</p> <p>The CEO wants: \"Should we invest in this model?\" The marketing VP wants: \"Which customers should we target?\" The compliance officer wants: \"Can we legally use this?\"</p> <p>Your job is to translate technical insights into actionable business recommendations.</p>"},{"location":"modules/09-interpretability/#executive-summary-structure","title":"Executive Summary Structure","text":"<p>Five parts:</p> <ol> <li>Business question: What were we predicting and why?</li> <li>Key finding: What's the main takeaway?</li> <li>Top factors: What drives predictions? (3-5 factors max)</li> <li>Confidence: How reliable? Any limitations?</li> <li>Recommendation: What should we do?</li> </ol> <p>No code. No jargon. Just business value.</p>"},{"location":"modules/09-interpretability/#example-executive-summary","title":"Example Executive Summary","text":"<pre><code>EXECUTIVE SUMMARY: Customer Churn Model\n\nBusiness Question: Which customers are likely to cancel\ntheir subscription in the next 90 days?\n\nKey Finding: We can identify 75% of churning customers\nbefore they leave, with 80% precision\u2014meaning 4 out of 5\ncustomers we flag will actually churn.\n\nTop Factors Driving Churn Risk:\n1. Support tickets in last 30 days (more tickets = higher risk)\n2. Days since last login (longer gap = higher risk)\n3. Contract type (monthly contracts 3x more likely to churn)\n\nConfidence: Model validated on 6 months of holdout data.\nLimitation: Works best for customers with 90+ days of history.\n\nRecommendation: Prioritize retention outreach to customers\nwith churn probability &gt; 70%. Expected ROI: $2.50 saved\nper $1 spent on retention.\n</code></pre> <p>No mention of random forests, SHAP, or cross-validation. Just business-relevant insights.</p> <p>Numerical Example: From SHAP to Business English</p> <pre><code># Raw SHAP output for a high-risk customer:\nshap_output = {\n    'base_value': 0.25,      # Average churn rate (25%)\n    'prediction': 0.78,       # This customer (78%)\n    'contributions': {\n        'support_tickets_30d': +0.22,\n        'days_since_login': +0.18,\n        'contract_type': +0.12,\n        'tenure_months': +0.08,\n        'satisfaction_score': -0.05,\n        'total_spend': -0.02,\n    }\n}\n</code></pre> <p>Translated to business language: <pre><code>Customer Churn Risk Assessment\n--------------------------------\nRisk Level: HIGH (78% likelihood of churning)\nBaseline: Average customer has 25% churn risk\n\nTop factors INCREASING risk:\n1. SUPPORT ISSUES (+22 points)\n   Filed 5 tickets in 30 days\u2014indicates frustration\n\n2. ENGAGEMENT DROP (+18 points)\n   Last login 45 days ago\u2014stopped using product\n\n3. CONTRACT FLEXIBILITY (+12 points)\n   Monthly contract\u2014easy to cancel anytime\n\nMitigating factors:\n- Satisfaction score 6/10 (better than churners)\n- Recent spending $250 (some investment)\n\nRECOMMENDED ACTIONS:\n1. Customer success outreach within 48 hours\n2. Resolve open support tickets immediately\n3. Offer annual contract incentive\n</code></pre></p> <p>Interpretation: The translation removes all technical jargon (no \"SHAP values,\" \"base value,\" or decimals). It groups factors into \"increasing risk\" vs \"mitigating,\" uses percentage points instead of raw values, and ends with actionable recommendations.</p> <p>Source: <code>slide_computations/module9_examples.py</code> - <code>demo_shap_to_business()</code></p>"},{"location":"modules/09-interpretability/#visualizations-for-business-audiences","title":"Visualizations for Business Audiences","text":"<ul> <li>Keep visualizations simple</li> <li>Use familiar formats (bar charts, line plots)</li> <li>Add clear labels and titles</li> <li>Highlight key insights with annotations</li> </ul> <p>Bad: Show a SHAP summary plot with no explanation</p> <p>Good: Show \"Top 5 Factors Driving Churn Risk\" with clear labels</p> <p>You might derive it from SHAP values, but the presentation is business-focused.</p> <p>Ethics of simplification: Simplification is often your professional obligation\u2014communication your audience can't understand serves no one. Distinguish appropriate simplification (\"the model uses engagement patterns\") from misleading omission (\"95% accurate\" without mentioning failure on new customers). Report uncertainty and limitations clearly. The ethical burden is on honesty, not exhaustive technical detail.</p>"},{"location":"modules/09-interpretability/#explaining-individual-predictions","title":"Explaining Individual Predictions","text":"<p>For customer-facing explanations: - Use natural language - Focus on top 2-3 factors - Avoid technical jargon - Provide actionable insights</p>"},{"location":"modules/09-interpretability/#adverse-action-example","title":"Adverse Action Example","text":"<p>When someone is denied credit, they're legally entitled to reasons:</p> <pre><code>Your loan application was declined. The main factors were:\n\n1. Your debt-to-income ratio is above our threshold\n2. Your credit history is shorter than we typically require\n3. Recent credit inquiries suggest high credit-seeking behavior\n\nSteps you can take to improve your chances:\n- Pay down existing debt to lower your debt-to-income ratio\n- Wait 6 months to build more credit history\n- Avoid applying for new credit in the near term\n</code></pre> <p>Specific, actionable, no jargon.</p>"},{"location":"modules/09-interpretability/#model-cards","title":"Model Cards","text":"<p>Model cards are documentation standards for ML models (introduced by Google).</p> <p>Components: 1. Model details: Type, version, date, owner 2. Intended use: What is this model for? What is it NOT for? 3. Factors: Relevant attributes (demographics, etc.) 4. Metrics: Performance overall AND by subgroup 5. Training data: What data was used? 6. Limitations: When does the model fail? 7. Ethical considerations: Potential harms, biases</p> <p>What belongs in a model card that wouldn't be in a technical report?</p> <p>Intended use and ethical considerations. A technical report says \"accuracy is 95%.\" A model card says \"this model is intended for prioritizing retention outreach, not for making final decisions about customer termination. It should not be used for populations under 18.\"</p>"},{"location":"modules/09-interpretability/#documenting-limitations","title":"Documenting Limitations","text":"<p>Being honest about limitations builds trust and prevents misuse.</p> <p>Good: - \"Model performance degrades for customers in the first 30 days\" - \"Validated only on US customers; may not generalize internationally\" - \"Does not account for seasonal effects\"</p> <p>Bad: - \"Model has some limitations\" (too vague) - Nothing at all (dangerous)</p>"},{"location":"modules/09-interpretability/#reflection-questions","title":"Reflection Questions","text":"<ol> <li> <p>A bank's loan approval model has 95% accuracy but can't explain decisions. Why might regulators reject it?</p> </li> <li> <p>You discover your hiring model relies heavily on ZIP code. Why is this concerning?</p> </li> <li> <p>SHAP shows 'age' has highest importance, but PDP shows a flat relationship. How is this possible?</p> </li> <li> <p>You need to explain a loan denial to a customer. Would you use SHAP or LIME? Why?</p> </li> <li> <p>A stakeholder asks \"which feature is most important?\" What clarifying questions should you ask?</p> </li> <li> <p>Your model uses 50 features. How do you explain it to a CEO in 5 minutes?</p> </li> <li> <p>A customer asks why their insurance premium increased. How do you respond without technical jargon?</p> </li> </ol>"},{"location":"modules/09-interpretability/#practice-problems","title":"Practice Problems","text":"<ol> <li> <p>Given SHAP values for a prediction, write the explanation in plain English</p> </li> <li> <p>Identify potential problems from PDP shapes (non-monotonic, discontinuous)</p> </li> <li> <p>Choose appropriate explanation technique for different scenarios</p> </li> <li> <p>Write an adverse action notice from model output</p> </li> <li> <p>Create a model card outline for a fraud detection system</p> </li> </ol>"},{"location":"modules/09-interpretability/#chapter-summary","title":"Chapter Summary","text":"<p>Six key takeaways from Module 9:</p> <ol> <li> <p>Interpretability is required for regulation, trust, and debugging</p> </li> <li> <p>Global shows overall patterns; Local shows individual predictions</p> </li> <li> <p>SHAP provides mathematically principled feature attribution</p> </li> <li> <p>LIME approximates complex models locally with simple ones</p> </li> <li> <p>Executive summaries translate technical findings to business value</p> </li> <li> <p>Model cards standardize documentation including limitations</p> </li> </ol>"},{"location":"modules/09-interpretability/#whats-next","title":"What's Next","text":"<p>In Module 10, we tackle Ethics, Fairness &amp; Deployment: - Bias in ML systems and how it arises - Fairness metrics and definitions - Bias mitigation techniques - Responsible AI practices - Model deployment considerations</p> <p>Interpretability is the foundation for fairness analysis! You can't assess whether a model is fair if you can't understand what it's doing.</p>"},{"location":"modules/10-ethics-deployment/","title":"Module 10: Ethics, Deployment &amp; Real-World ML","text":""},{"location":"modules/10-ethics-deployment/#introduction","title":"Introduction","text":"<p>We've covered an incredible amount of ground in this course. You can build regression models, classification models, ensemble methods, neural networks, CNNs for images, transformers for text. You can interpret models with SHAP and LIME. You know how to evaluate, tune, and avoid common pitfalls.</p> <p>But here's the thing: Building a model is only half the journey.</p> <p>This module tackles the other half\u2014getting models into the real world responsibly and effectively. This means grappling with ethics and fairness, learning time series forecasting, understanding deployment, and calculating business value.</p> <p>These topics bridge technical skills to real-world impact. Every data scientist who wants to make a difference needs to master them.</p> <p>Responsibility for fairness: All three share responsibility. Data scientists are the first line of defense and should raise concerns. Companies set culture, allocate resources for fairness audits, and establish review processes\u2014they're culpable for pressuring fast deployment without ethical review. Regulators provide external accountability that markets fail to create. The healthiest ecosystem has all three layers; relying on any single one is insufficient.</p>"},{"location":"modules/10-ethics-deployment/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, you should be able to:</p> <ol> <li>Identify sources of bias in ML systems and propose mitigation strategies</li> <li>Apply fairness metrics to evaluate model equity across groups</li> <li>Build time series forecasting models using appropriate techniques</li> <li>Explain the basics of model deployment and MLOps</li> <li>Calculate business value and ROI of ML projects</li> <li>Communicate uncertainty and manage stakeholder expectations</li> </ol>"},{"location":"modules/10-ethics-deployment/#101-ethics-responsible-ai","title":"10.1 Ethics &amp; Responsible AI","text":""},{"location":"modules/10-ethics-deployment/#the-stakes-are-high","title":"The Stakes Are High","text":"<p>ML systems are making consequential decisions: who gets a loan, who gets a job, who gets parole, who gets medical treatment. These aren't abstract technical problems\u2014they affect real people's lives.</p> <p>And here's the uncomfortable truth: ML systems can be biased. They can be unfair. They can cause harm.</p> <p>Not because the engineers are malicious, but because bias creeps in through data, design choices, and blind spots. If we're going to deploy these systems, we need to understand how bias arises and how to mitigate it.</p>"},{"location":"modules/10-ethics-deployment/#sources-of-bias-in-ml","title":"Sources of Bias in ML","text":"<p>Historical Bias: Training data reflects past discrimination. - If you train on 10 years of hiring data, and that data reflects historical biases against women or minorities, your model learns those biases. - The model isn't \"biased by itself\"\u2014it's learning patterns from biased data.</p> <p>Selection Bias: Training data doesn't represent the population. - A medical AI trained mostly on data from white patients may perform worse on underrepresented groups. - The model has never learned the patterns for those populations.</p> <p>Measurement Bias: Features are measured differently across groups. - \"Years of experience\" penalizes career gaps, which disproportionately affects women. - \"Arrests\" doesn't mean \"crimes committed\"\u2014it reflects policing patterns.</p> <p>Aggregation Bias: One model for heterogeneous populations. - A single diabetes prediction model may work differently across ethnicities. - Sometimes you need separate models or careful feature engineering.</p> <p>Feedback Loops: Model predictions affect future data. - Predictive policing sends more officers to certain neighborhoods \u2192 more arrests \u2192 more \"crime\" data \u2192 model sends even more officers. - The bias becomes self-reinforcing.</p> <p>The Snowball Effect: Feedback loops compound over time. Consider predictive policing over 4 cycles: 1. Year 1: Model trained on historical data sends more officers to Neighborhood A (higher crime rate in data). 2. Year 2: More officers \u2192 more arrests in Neighborhood A (crimes detected, not necessarily committed). Model retrains, sees Neighborhood A as even higher risk. 3. Year 3: Even more officers to Neighborhood A. Meanwhile, crimes in Neighborhood B go undetected (fewer officers there). Gap widens. 4. Year 4: Model now \"confirms\" its own predictions\u2014Neighborhood A looks dangerous, B looks safe. But this reflects policing patterns, not underlying crime rates.</p> <p>The model creates the very data that justifies its predictions. Breaking this cycle requires external validation (e.g., surveys, victimization studies) not generated by the model itself.</p>"},{"location":"modules/10-ethics-deployment/#case-study-amazon-hiring-tool","title":"Case Study: Amazon Hiring Tool","text":"<p>In 2018, it was reported that Amazon had built a hiring tool trained on 10 years of resume data.</p> <p>What went wrong: - The model learned to penalize words like \"women's\" (as in \"women's chess club captain\") - It downgraded graduates of women's colleges - It effectively discriminated against female applicants</p> <p>The lesson: Historical data encodes historical bias. Amazon's tech workforce was predominantly male. The model learned that being male correlated with getting hired. It wasn't explicitly told \"penalize women,\" but it learned it from the patterns.</p> <p>Amazon scrapped the tool.</p>"},{"location":"modules/10-ethics-deployment/#case-study-compas","title":"Case Study: COMPAS","text":"<p>COMPAS is a recidivism prediction algorithm used in the US criminal justice system to predict whether defendants will reoffend.</p> <p>ProPublica's analysis found: - Black defendants had a higher false positive rate (incorrectly flagged as high risk) - White defendants had a higher false negative rate (incorrectly flagged as low risk) - Same overall accuracy, very different error patterns</p> <p>This shows that identical accuracy can hide profoundly different impacts on different groups.</p> <p>Choosing between fairness criteria: This is an ethical decision, not technical\u2014it shouldn't be made solely by data scientists. The data scientist's role is to make tradeoffs transparent (\"if we optimize for A, here's what happens to X and Y\"), not to unilaterally decide. These decisions should involve domain experts, affected communities, legal experts, and ethicists. Document the decision, reasoning, and who was involved.</p>"},{"location":"modules/10-ethics-deployment/#fairness-metrics","title":"Fairness Metrics","text":"<p>There are multiple mathematical definitions of fairness:</p> <p>Demographic Parity (Statistical Parity):</p> \\[P(\\hat{Y}=1|A=0) = P(\\hat{Y}=1|A=1)\\] <ul> <li>Equal positive prediction rates across groups</li> <li>If 30% of men get approved, 30% of women should get approved</li> <li>Limitation: Ignores actual qualification rates</li> </ul> <p>Equalized Odds:</p> \\[P(\\hat{Y}=1|Y=1, A=0) = P(\\hat{Y}=1|Y=1, A=1)\\] \\[P(\\hat{Y}=1|Y=0, A=0) = P(\\hat{Y}=1|Y=0, A=1)\\] <ul> <li>Equal true positive rates AND equal false positive rates across groups</li> <li>If you're qualified, you should have equal chance of being accepted regardless of group</li> <li>If you're unqualified, you should have equal chance of being rejected</li> </ul> <p>Predictive Parity:</p> \\[P(Y=1|\\hat{Y}=1, A=0) = P(Y=1|\\hat{Y}=1, A=1)\\] <ul> <li>Equal precision across groups</li> <li>If the model says \"yes,\" the probability of actually being qualified should be the same across groups</li> </ul> <p>The Courtroom Analogy: Think of fairness metrics through a courtroom lens. Demographic parity is like requiring equal conviction rates across groups\u2014regardless of actual guilt, the same percentage should be convicted. Equalized odds is about equal mistakes: among truly innocent people, both groups should have the same chance of wrongful conviction (equal FPR); among truly guilty people, both groups should have the same chance of being caught (equal TPR). Predictive parity asks: when the jury says \"guilty,\" are they equally likely to be right for both groups? Each metric captures a different intuition about what \"fair\" means\u2014and they often conflict.</p>"},{"location":"modules/10-ethics-deployment/#the-impossibility-theorem","title":"The Impossibility Theorem","text":"<p>You cannot satisfy all fairness criteria simultaneously (except in special cases).</p> <p>This isn't a technical limitation\u2014it's mathematically proven. If groups have different base rates (different proportions of positive outcomes), you have to choose which fairness criterion matters most.</p> <p>Example:</p> Group Accuracy FPR FNR A 85% 10% 20% B 85% 25% 5% <p>Same accuracy. But Group B has more false positives (more people incorrectly flagged). Group A has more false negatives (more people incorrectly missed).</p> <p>Which is worse depends on context. In criminal justice, high FPR means innocent people in jail. In medical diagnosis, high FNR means sick people going untreated.</p> <p>Why the impossibility exists: Imagine you try to fix Group B's higher FPR by raising the threshold (being more conservative with \"yes\" predictions). This reduces false positives, but it also reduces true positives\u2014now qualified people in Group B are less likely to be approved. You've traded one unfairness for another. The only way both could be equal simultaneously is if both groups have the same base rate (same proportion of qualified people)\u2014which is often not the case in the real world due to historical inequalities. This forces a choice, not a calculation.</p> <p>Numerical Example: Impossibility Theorem in Action</p> <pre><code># Two groups with different base rates\n# Group A: 60% positive, Group B: 30% positive\n\n# Starting point: same threshold for both\n# Group A: TPR=90%, FPR=20%\n# Group B: TPR=80%, FPR=20%\n\n# ATTEMPT 1: Equalize TPR (lower threshold for B)\n# Group B now: TPR=90% (equal!), but FPR jumps to 30%\nprint(\"After equalizing TPR:\")\nprint(\"  Group A: TPR=90%, FPR=20%\")\nprint(\"  Group B: TPR=90%, FPR=30%\")  # FPR now 1.5x higher!\n\n# ATTEMPT 2: Equalize FPR (raise threshold for B)\n# Group B now: FPR=20% (equal!), but TPR drops to 70%\nprint(\"After equalizing FPR:\")\nprint(\"  Group A: TPR=90%, FPR=20%\")\nprint(\"  Group B: TPR=70%, FPR=20%\")  # TPR now 22% lower!\n</code></pre> <p>Output: <pre><code>After equalizing TPR:\n  Group A: TPR=90%, FPR=20%\n  Group B: TPR=90%, FPR=30%\nAfter equalizing FPR:\n  Group A: TPR=90%, FPR=20%\n  Group B: TPR=70%, FPR=20%\n</code></pre></p> <p>Interpretation: You cannot have both equal opportunity (equal TPR) and equal protection (equal FPR) when base rates differ. Equalizing TPR means more false accusations for Group B; equalizing FPR means fewer qualified Group B members are approved. This is the impossibility theorem in action\u2014a mathematical constraint, not a technical failure.</p> <p>Source: <code>slide_computations/module10_examples.py</code> - <code>demo_impossibility_theorem()</code></p>"},{"location":"modules/10-ethics-deployment/#calculating-fairness-metrics","title":"Calculating Fairness Metrics","text":"<pre><code>from fairlearn.metrics import MetricFrame\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\n\n# Calculate metrics by group\nmetric_frame = MetricFrame(\n    metrics={\n        'accuracy': accuracy_score,\n        'precision': precision_score,\n        'recall': recall_score\n    },\n    y_true=y_test,\n    y_pred=y_pred,\n    sensitive_features=demographic_feature\n)\n\n# View metrics by group\nprint(metric_frame.by_group)\n\n# View maximum difference between groups\nprint(metric_frame.difference())\n</code></pre> <p>The <code>fairlearn</code> library makes this straightforward. Calculate your metrics by demographic group and look for disparities.</p> <p>Numerical Example: Calculating Fairness Metrics by Hand</p> <pre><code># Loan approval model: two demographic groups, 1000 applicants each\n# Group A: 60% qualified (base rate), Group B: 40% qualified\n\n# Confusion matrices:\n# Group A: TP=510, FP=60, TN=340, FN=90 (85% accuracy)\n# Group B: TP=320, FP=120, TN=480, FN=80 (80% accuracy)\n\n# Calculate metrics for each group\napproval_a = (510 + 60) / 1000   # 57%\napproval_b = (320 + 120) / 1000  # 44%\ntpr_a = 510 / 600                # 85%\ntpr_b = 320 / 400                # 80%\nfpr_a = 60 / 400                 # 15%\nfpr_b = 120 / 600                # 20%\n\nprint(f\"Demographic Parity Ratio: {approval_b/approval_a:.2f}\")  # 0.77\nprint(f\"TPR Ratio: {tpr_b/tpr_a:.2f}\")                          # 0.94\nprint(f\"FPR Ratio: {fpr_b/fpr_a:.2f}\")                          # 1.33\n</code></pre> <p>Output: <pre><code>Demographic Parity Ratio: 0.77\nTPR Ratio: 0.94\nFPR Ratio: 1.33\n</code></pre></p> <p>Interpretation: Despite similar accuracy (~85%), Group B has a 33% higher false positive rate\u2014more unqualified applicants incorrectly approved. This could indicate the model is compensating for lower approval rates with riskier approvals. The 0.77 demographic parity ratio (below the typical 0.8 threshold) flags a potential disparate impact issue.</p> <p>Source: <code>slide_computations/module10_examples.py</code> - <code>demo_fairness_metrics()</code></p>"},{"location":"modules/10-ethics-deployment/#proxy-variables","title":"Proxy Variables","text":"<p>\"We don't use race in our model, so it can't be biased.\"</p> <p>This is wrong.</p> <p>Proxy variables are features that correlate with protected attributes: - ZIP code correlates with race and income - Name can indicate gender or ethnicity - Arrest history correlates with race (due to policing patterns) - \"Years since last job\" correlates with gender (career gaps)</p> <p>Removing the protected attribute doesn't remove the bias if proxies remain.</p> <p>The Hidden Pathway: Imagine bias as water flowing from a protected attribute (race) to model predictions. Removing race from your model is like blocking the front door\u2014but if proxy variables like ZIP code, school attended, or arrest history remain, the water finds another path through the back door. These proxies carry much of the same information because of historical patterns (residential segregation, educational inequality, discriminatory policing). A truly fair model must audit these pathways, not just check whether \"race\" appears in the feature list.</p> <p>Numerical Example: Detecting Proxy Variables</p> <pre><code>import numpy as np\n\n# Simulate 1000 loan applicants\nnp.random.seed(42)\nn = 1000\n\n# Demographic group (protected attribute)\ngroup = np.random.choice(['A', 'B'], size=n, p=[0.7, 0.3])\n\n# Income differs by group (structural inequality)\nincome = np.where(group == 'A',\n    np.random.normal(75000, 20000, n),\n    np.random.normal(55000, 18000, n))\n\n# ZIP score correlates with income (residential segregation)\nzip_score = income / 1000 + np.random.normal(0, 10, n)\n\n# Correlations reveal the hidden pathways\ngroup_numeric = (group == 'B').astype(int)\nprint(f\"ZIP \u2194 Group correlation: {np.corrcoef(zip_score, group_numeric)[0,1]:.3f}\")\nprint(f\"Income \u2194 Group correlation: {np.corrcoef(income, group_numeric)[0,1]:.3f}\")\n</code></pre> <p>Output: <pre><code>ZIP \u2194 Group correlation: -0.408\nIncome \u2194 Group correlation: -0.438\n</code></pre></p> <p>Interpretation: Even without \"group\" in the model, ZIP score is strongly correlated (r = -0.41) with group membership. A model using ZIP score will produce different outcomes by group\u2014not because it's explicitly discriminating, but because ZIP encodes the same information through residential segregation patterns. \"We don't use race\" is not sufficient.</p> <p>Source: <code>slide_computations/module10_examples.py</code> - <code>demo_proxy_variable_correlation()</code></p>"},{"location":"modules/10-ethics-deployment/#when-not-to-use-ml","title":"When NOT to Use ML","text":"<p>Not every problem needs machine learning.</p> <p>Consider avoiding ML when: - Stakes are very high and errors are catastrophic - Accountability and explanation are paramount - Training data is fundamentally biased - The problem is better solved by policy - Human judgment is essential</p> <p>Questions to ask: - Who is affected by this system? - What happens when it's wrong? - Can we explain decisions to affected parties? - Is the training data representative? - Are we automating an already unfair process?</p> <p>Sometimes the right answer is \"don't build this model.\"</p> <p>Pushing back on unethical projects: Document concerns and frame in terms of business risk (legal liability, reputational damage). Escalate through appropriate channels\u2014ethics hotlines, ombudspersons. If internal advocacy fails: comply under protest (documented), refuse the project (accept consequences), or leave. Building a financial cushion gives leverage. Long-term: seek employers whose values align with yours\u2014ask about ethics review processes during interviews.</p>"},{"location":"modules/10-ethics-deployment/#common-misconceptions","title":"Common Misconceptions","text":"Misconception Reality \"ML is objective because it's math\" ML learns patterns from data, including human biases \"Equal accuracy = fairness\" Same accuracy can hide very different error patterns across groups \"Just remove protected attributes\" Proxy variables can encode same information \"Fairness is a purely technical problem\" Requires ethical choices that should involve diverse stakeholders"},{"location":"modules/10-ethics-deployment/#102-time-series-forecasting","title":"10.2 Time Series Forecasting","text":""},{"location":"modules/10-ethics-deployment/#why-time-series-is-different","title":"Why Time Series Is Different","text":"<p>Time series data has a unique property: temporal ordering matters.</p> <p>In standard ML, we assume observations are independent\u2014shuffling rows shouldn't matter. In time series, shuffling destroys the information. Yesterday's sales tell you something about today's sales. January's patterns repeat every January.</p> <p>This changes everything about how we model and validate.</p> <p>Time Series Has Memory: The key insight is that time series data has memory\u2014today's value carries information about tomorrow's. This memory comes in two forms: - AutoRegressive (AR) memory: Tomorrow's value depends on today's value. High sales today \u2192 probably high sales tomorrow. The series \"remembers\" recent values. - Moving Average (MA) memory: The series remembers recent surprises. If yesterday's error was +$1000 (sales beat forecast), today might also beat forecast as the underlying conditions persist.</p> <p>This memory is why ARIMA models can work: they exploit predictable patterns in how values and errors evolve over time. No memory \u2192 no predictability \u2192 just random noise.</p>"},{"location":"modules/10-ethics-deployment/#time-series-components","title":"Time Series Components","text":"<p>Time series can be decomposed into components:</p> <p>Trend: Long-term direction (sales growing over years)</p> <p>Seasonality: Regular patterns (sales spike every December)</p> <p>Cyclical: Irregular longer-term fluctuations (economic cycles)</p> <p>Noise: Random variation</p> <pre><code>from statsmodels.tsa.seasonal import seasonal_decompose\n\ndecomposition = seasonal_decompose(\n    series,\n    model='additive',\n    period=12\n)\ndecomposition.plot()\n</code></pre> <p>Understanding these components helps you choose the right model and spot problems.</p> <p>Numerical Example: Time Series Decomposition</p> <pre><code>import numpy as np\n\n# 12 months of retail sales\nmonths = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n          'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n\n# True components:\ntrend = np.array([50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]) * 1000\nseasonal = np.array([-5, -8, -3, 0, 2, 3, 2, 1, -1, 0, 5, 10]) * 1000\nnoise = np.array([1, 0, 1, 3, 0, -1, 3, 2, -1, 1, -1, -1]) * 1000\n\nsales = trend + seasonal + noise\n\nprint(\"Month    Sales     Trend   Seasonal   Noise\")\nfor i in range(12):\n    print(f\"{months[i]:&lt;6} ${sales[i]:&gt;7,} ${trend[i]:&gt;7,} ${seasonal[i]:&gt;+7,} ${noise[i]:&gt;+6,}\")\n</code></pre> <p>Output: <pre><code>Month    Sales     Trend   Seasonal   Noise\nJan    $ 46,000 $ 50,000 $  -5,000 $ +1,000\nFeb    $ 43,000 $ 51,000 $  -8,000 $     +0\nMar    $ 50,000 $ 52,000 $  -3,000 $ +1,000\n...\nNov    $ 64,000 $ 60,000 $  +5,000 $ -1,000\nDec    $ 70,000 $ 61,000 $ +10,000 $ -1,000\n</code></pre></p> <p>Interpretation: December sales (\\(70K) aren't just \"high\"\u2014they're the sum of underlying trend (\\)61K), seasonal boost (+\\(10K for holiday shopping), and random noise (-\\)1K). Decomposition reveals that the $18K swing from February to December is almost entirely seasonal, not growth. For forecasting, extrapolate trend forward, add the expected seasonal effect, and report uncertainty from the noise variance.</p> <p>Source: <code>slide_computations/module10_examples.py</code> - <code>demo_time_series_decomposition()</code></p>"},{"location":"modules/10-ethics-deployment/#arima-models","title":"ARIMA Models","text":"<p>ARIMA is the classic statistical approach to time series.</p> <p>ARIMA(p, d, q): - AR (AutoRegressive): Predict from past values (how many lags to use = p) - I (Integrated): Differencing for stationarity (how many times to difference = d) - MA (Moving Average): Predict from past errors (how many lag errors to use = q)</p> <pre><code>from statsmodels.tsa.arima.model import ARIMA\n\nmodel = ARIMA(train_series, order=(1, 1, 1))\nresults = model.fit()\nforecast = results.forecast(steps=30)\n</code></pre> <p>Choosing p, d, q: - ACF/PACF plots give guidance - AIC/BIC criteria for model selection - Or use auto_arima:</p> <pre><code>from pmdarima import auto_arima\n\nmodel = auto_arima(\n    train_series,\n    seasonal=True,\n    m=12,  # Monthly seasonality\n    trace=True\n)\n</code></pre> <p>Auto_arima searches through parameter combinations and picks the best one.</p> <p>Numerical Example: ARIMA Parameter Intuition</p> <pre><code>import numpy as np\nnp.random.seed(42)\n\n# Generate AR(1) series with different persistence (phi)\nn, shocks = 50, np.random.normal(0, 1, 50)\n\ndef ar1_series(phi, shocks):\n    y = np.zeros(len(shocks))\n    y[0] = shocks[0]\n    for t in range(1, len(shocks)):\n        y[t] = phi * y[t-1] + shocks[t]\n    return y\n\nfor phi in [0.0, 0.5, 0.9, 0.99]:\n    series = ar1_series(phi, shocks)\n    autocorr = np.corrcoef(series[:-1], series[1:])[0, 1]\n    print(f\"phi={phi:.2f}: variance={np.var(series):.1f}, autocorr={autocorr:.2f}\")\n</code></pre> <p>Output: <pre><code>phi=0.00: variance=0.8, autocorr=0.03\nphi=0.50: variance=1.2, autocorr=0.53\nphi=0.90: variance=5.1, autocorr=0.91\nphi=0.99: variance=18.0, autocorr=0.98\n</code></pre></p> <p>Interpretation: The AR coefficient (phi) controls memory. At phi=0, each value is independent noise\u2014no predictability. At phi=0.9, today strongly predicts tomorrow (autocorr=0.91); shocks persist for many periods. At phi=0.99, the series approaches a random walk\u2014shocks accumulate and variance explodes. When you see high autocorrelation in your data, an AR model can exploit that persistence for forecasting.</p> <p>Source: <code>slide_computations/module10_examples.py</code> - <code>demo_arima_parameter_intuition()</code></p>"},{"location":"modules/10-ethics-deployment/#prophet","title":"Prophet","text":"<p>Facebook's Prophet is a popular alternative to ARIMA.</p> <p>Advantages: - Handles seasonality automatically (multiple seasonalities!) - Robust to missing data - Interpretable components - Easy to add holidays and special events</p> <pre><code>from prophet import Prophet\n\n# Data must have columns 'ds' (date) and 'y' (value)\nmodel = Prophet(\n    yearly_seasonality=True,\n    weekly_seasonality=True\n)\nmodel.fit(train_df)\n\nfuture = model.make_future_dataframe(periods=30)\nforecast = model.predict(future)\n\nmodel.plot(forecast)\nmodel.plot_components(forecast)  # Shows trend, seasonality, etc.\n</code></pre> <p>Prophet is particularly good for business applications with strong seasonal patterns.</p>"},{"location":"modules/10-ethics-deployment/#when-to-choose-what","title":"When to Choose What","text":"<ul> <li>Prophet: Multiple seasonalities, missing data, holidays, interpretable components</li> <li>ARIMA: More control, complex series that don't fit Prophet's assumptions, very short series</li> <li>LSTM: Complex non-linear patterns, multiple input features, long sequences</li> </ul>"},{"location":"modules/10-ethics-deployment/#time-series-validation","title":"Time Series Validation","text":"<p>You cannot use standard k-fold cross-validation for time series.</p> <p>Why? Because it would leak future information into training. If your test set contains January 2024 and your training set contains February 2024, you're cheating\u2014you're using the future to predict the past.</p> <p>Walk-forward validation: <pre><code>Train: [----]          Test: [-]\nTrain: [------]        Test: [-]\nTrain: [--------]      Test: [-]\n</code></pre></p> <p>Always train on the past, test on the future. Never the reverse.</p> <p>The Time Machine Rule: Using k-fold cross-validation on time series is like checking tomorrow's newspaper to predict today's stock price. In fold 1, your model trains on data from weeks 3-10 to predict week 1\u2014it literally uses the future to predict the past. Of course it looks accurate! But in deployment, you'll never have next week's data to help predict this week. Walk-forward validation enforces the constraint you'll face in production: you can only look backward, never forward.</p> <pre><code>from sklearn.model_selection import TimeSeriesSplit\n\ntscv = TimeSeriesSplit(n_splits=5)\nfor train_idx, test_idx in tscv.split(data):\n    train = data[train_idx]\n    test = data[test_idx]\n    # Train and evaluate\n</code></pre> <p>Numerical Example: Walk-Forward vs K-Fold Validation</p> <pre><code># AR(1) time series: y_t = 0.9 * y_{t-1} + noise\n# 100 time points with strong autocorrelation\n\n# K-Fold (WRONG): trains on future, tests on past\n# Fold 1: Train on points 20-99, test on 0-19\n# The model \"knows\" where the series goes!\nkfold_mae = 3.76\n\n# Walk-Forward (CORRECT): trains on past, tests on future\n# Window 1: Train on 0-49, test on 50-59\n# Window 2: Train on 0-59, test on 60-69\nwalk_forward_mae = 3.83\n\nprint(f\"K-Fold MAE (biased):      {kfold_mae:.2f}\")\nprint(f\"Walk-Forward MAE:         {walk_forward_mae:.2f}\")\nprint(f\"K-Fold underestimates by: {walk_forward_mae - kfold_mae:.2f}\")\n</code></pre> <p>Output: <pre><code>K-Fold MAE (biased):      3.76\nWalk-Forward MAE:         3.83\nK-Fold underestimates by: 0.07\n</code></pre></p> <p>Interpretation: K-fold appears more accurate because it cheats\u2014using future data to predict past values. In this example, the difference is small (0.07), but in series with trends or structural breaks, k-fold can underestimate error by 20-50%. Always use walk-forward for time series; it reflects the constraint you'll face in production.</p> <p>Source: <code>slide_computations/module10_examples.py</code> - <code>demo_walk_forward_vs_kfold()</code></p>"},{"location":"modules/10-ethics-deployment/#business-applications","title":"Business Applications","text":"<p>Time series forecasting is everywhere in business: - Sales forecasting: Budget planning, resource allocation - Demand planning: Inventory management, supply chain - Capacity planning: Staffing, infrastructure - Financial forecasting: Revenue projections, cash flow</p>"},{"location":"modules/10-ethics-deployment/#103-model-deployment","title":"10.3 Model Deployment","text":""},{"location":"modules/10-ethics-deployment/#from-notebook-to-production","title":"From Notebook to Production","text":"<p>You've built a model in a Jupyter notebook. It works great. Now what?</p> <p>The gap between \"model works\" and \"model is deployed\" is significant: - How do other systems call your model? - How do you handle errors? - How do you scale? - How do you update the model? - How do you monitor performance?</p> <p>This is where software engineering meets data science.</p>"},{"location":"modules/10-ethics-deployment/#model-serialization","title":"Model Serialization","text":"<p>First, you need to save your model so it can be loaded elsewhere.</p> <p>Pickle/Joblib (for scikit-learn models): <pre><code>import joblib\n\n# Save model\njoblib.dump(model, 'model.pkl')\n\n# Load model\nmodel = joblib.load('model.pkl')\n</code></pre></p> <p>ONNX (cross-platform format): - Works across different frameworks (PyTorch, TensorFlow, scikit-learn) - Optimized for inference - Useful when production environment differs from development</p> <pre><code>import torch.onnx\n\ntorch.onnx.export(\n    model,\n    dummy_input,\n    \"model.onnx\",\n    input_names=['input'],\n    output_names=['output']\n)\n</code></pre>"},{"location":"modules/10-ethics-deployment/#creating-apis","title":"Creating APIs","text":"<p>To let other systems use your model, wrap it in an API.</p> <p>Flask (simple, widely used): <pre><code>from flask import Flask, request, jsonify\nimport joblib\n\napp = Flask(__name__)\nmodel = joblib.load('model.pkl')\n\n@app.route('/predict', methods=['POST'])\ndef predict():\n    data = request.json\n    features = data['features']\n    prediction = model.predict([features])\n    return jsonify({'prediction': prediction[0]})\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5000)\n</code></pre></p> <p>FastAPI (modern, automatic documentation): <pre><code>from fastapi import FastAPI\nfrom pydantic import BaseModel\nimport joblib\n\napp = FastAPI()\nmodel = joblib.load('model.pkl')\n\nclass PredictionRequest(BaseModel):\n    features: list\n\n@app.post(\"/predict\")\ndef predict(request: PredictionRequest):\n    prediction = model.predict([request.features])\n    return {\"prediction\": prediction[0]}\n</code></pre></p> <p>Now other applications can send HTTP requests to get predictions.</p>"},{"location":"modules/10-ethics-deployment/#containerization-with-docker","title":"Containerization with Docker","text":"<p>Docker packages your application with all its dependencies.</p> <p>Dockerfile: <pre><code>FROM python:3.9-slim\n\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nCOPY model.pkl .\nCOPY app.py .\n\nEXPOSE 5000\nCMD [\"python\", \"app.py\"]\n</code></pre></p> <p>Benefits: - Reproducibility: Same environment everywhere - Portability: Runs on any system with Docker - Scalability: Easy to run multiple containers - Isolation: Dependencies don't conflict</p> <p>The Shipping Container Analogy: Before standardized shipping containers, loading cargo was chaos\u2014every ship, every port, every truck had different requirements. Containers changed everything: pack once in a standard-sized box, and it moves seamlessly across ships, trains, and trucks worldwide. Docker does the same for software. Your model, its Python version, its exact library versions, its configuration\u2014all packed into one container that runs identically on your laptop, your colleague's machine, and the production server. No more \"but it worked on my machine!\"</p>"},{"location":"modules/10-ethics-deployment/#cloud-deployment-options","title":"Cloud Deployment Options","text":"Platform Use Case Complexity AWS SageMaker Full ML platform Medium Google Vertex AI Full ML platform Medium Azure ML Full ML platform Medium Heroku Simple web apps Low AWS Lambda Serverless, simple models Low <p>For production ML at scale, the major cloud platforms provide integrated solutions. For simple models or prototypes, Heroku or Lambda can be quick to set up.</p>"},{"location":"modules/10-ethics-deployment/#104-production-business-value","title":"10.4 Production &amp; Business Value","text":""},{"location":"modules/10-ethics-deployment/#monitoring-in-production","title":"Monitoring in Production","text":"<p>Deployment isn't the end\u2014it's the beginning of a new phase.</p> <p>Metrics to track continuously: - Model accuracy over time: Is performance degrading? - Feature distributions: Are inputs changing? - Latency and throughput: Is the system fast enough? - Error rates: Are there failures?</p> <p>Set up alerts. If accuracy drops below a threshold, you want to know immediately.</p>"},{"location":"modules/10-ethics-deployment/#model-drift","title":"Model Drift","text":"<p>Data drift: Input distribution changes. - Customer demographics shift - Seasonality wasn't captured - Data collection process changed</p> <p>Concept drift: The relationship between inputs and outputs changes. - Customer behavior changed (e.g., during COVID) - What used to predict churn no longer does</p> <p>Telling Them Apart: Data drift and concept drift require different responses:</p> Type Example What Changed Solution Data drift Your customer base shifted younger (average age dropped from 45 to 35) Input distribution Model may still be correct for each age; retrain on new demographics Concept drift COVID hits; customers who previously never churned are now churning at high rates Relationship between inputs and outputs The rules changed; old patterns don't apply; must retrain on post-COVID behavior Both Pandemic: customers younger (data drift) AND their behavior fundamentally changed (concept drift) Everything Full reassessment needed <p>The distinction matters because data drift can sometimes be addressed by resampling or weighting, while concept drift means your model's core assumptions are invalidated\u2014yesterday's rules don't work today.</p> <pre><code>from evidently import Report\nfrom evidently.metrics import DataDriftTable\n\nreport = Report(metrics=[DataDriftTable()])\nreport.run(\n    reference_data=training_data,\n    current_data=production_data\n)\nreport.show()\n</code></pre> <p>Evidently AI and similar tools help detect when your data has shifted.</p>"},{"location":"modules/10-ethics-deployment/#retraining-strategies","title":"Retraining Strategies","text":"<p>When drift is detected (or just periodically), you need to retrain.</p> <p>Scheduled retraining: Weekly, monthly\u2014on a fixed schedule</p> <p>Triggered retraining: When drift exceeds a threshold</p> <p>Continuous training: Update with each new batch of data</p> <p>Automate the pipeline: 1. Data validation 2. Feature engineering 3. Model training 4. Evaluation (reject if metrics don't meet threshold) 5. Deployment</p> <p>This is where MLOps comes in\u2014applying DevOps principles to ML.</p>"},{"location":"modules/10-ethics-deployment/#ab-testing","title":"A/B Testing","text":"<p>Purpose: Compare new model against current model</p> <p>Implementation: 1. Route percentage of traffic to new model 2. Track metrics for both models 3. Statistical test for significance 4. Roll out winner</p> <p>Key considerations: - Sample size requirements - Duration of test - Guardrail metrics (don't harm user experience)</p> <p>Numerical Example: A/B Test Sample Size Calculation</p> <pre><code>from scipy import stats\nimport numpy as np\n\n# Scenario: Testing if new model improves conversion rate\np_control = 0.05     # Current: 5% conversion\np_treatment = 0.055  # Target: 5.5% (10% relative improvement)\nalpha = 0.05         # 5% significance level\npower = 0.80         # 80% power\n\n# Sample size formula for two-proportion z-test\nz_alpha = stats.norm.ppf(1 - alpha/2)  # 1.96\nz_beta = stats.norm.ppf(power)          # 0.84\np_pooled = (p_control + p_treatment) / 2\neffect = p_treatment - p_control\n\nn_per_group = 2 * (z_alpha + z_beta)**2 * p_pooled * (1 - p_pooled) / effect**2\nprint(f\"Required per group: {int(np.ceil(n_per_group)):,}\")\nprint(f\"Total required:     {2 * int(np.ceil(n_per_group)):,}\")\n</code></pre> <p>Output: <pre><code>Required per group: 31,235\nTotal required:     62,470\n</code></pre></p> <p>Interpretation: To detect a 0.5% absolute improvement (5.0% \u2192 5.5%) with 80% power, you need ~62,000 total users. With 10,000 daily visitors, that's about 1 week. Smaller effects need exponentially more data: detecting 0.25% improvement would require ~244,000 users. Plan your test duration before starting, and resist the temptation to peek at results early\u2014that inflates false positive rates.</p> <p>Source: <code>slide_computations/module10_examples.py</code> - <code>demo_ab_test_sample_size()</code></p>"},{"location":"modules/10-ethics-deployment/#roi-calculation","title":"ROI Calculation","text":"<p>How do you justify an ML project?</p> <p>Example: Churn prevention model</p> <pre><code>Annual churning customers: 10,000\nCustomer lifetime value: $500\nChurn cost without model: $5,000,000\n\nModel performance:\n- Identifies 75% of churners\n- Intervention success rate: 30%\n- Customers saved: 10,000 \u00d7 0.75 \u00d7 0.30 = 2,250\n- Value saved: 2,250 \u00d7 $500 = $1,125,000\n\nCosts:\n- Development: $100,000\n- Annual maintenance: $20,000\n- Intervention cost: $50 per flagged customer\n- Total intervention cost: 7,500 \u00d7 $50 = $375,000\n- Total costs: $495,000\n\nFirst year ROI: ($1,125,000 - $495,000) / $495,000 = 127%\n</code></pre> <p>The key: Quantify business impact, not just accuracy. \"95% accuracy\" means nothing to a CFO. \"$630,000 net value in year one\" does.</p> <p>Numerical Example: ROI Sensitivity Analysis</p> <pre><code># Base case: 127% ROI\n# But how sensitive is this to our assumptions?\n\n# Vary model recall: 60%, 75%, 90%\nrecall_scenarios = {'60%': 114, '75%': 127, '90%': 137}\n\n# Vary intervention success: 20%, 30%, 40%\nintervention_scenarios = {'20%': 52, '30%': 127, '40%': 203}\n\n# Scenario analysis\npessimistic = {'recall': 0.60, 'intervention': 0.20, 'ltv': 400, 'dev_cost': 150000}\noptimistic = {'recall': 0.90, 'intervention': 0.40, 'ltv': 600, 'dev_cost': 75000}\n\nprint(\"Scenario        Net Value       ROI\")\nprint(\"Pessimistic     $10,000          2%\")\nprint(\"Base Case      $630,000        127%\")\nprint(\"Optimistic   $1,615,000        296%\")\n</code></pre> <p>Output: <pre><code>Scenario        Net Value       ROI\nPessimistic     $10,000          2%\nBase Case      $630,000        127%\nOptimistic   $1,615,000        296%\n</code></pre></p> <p>Interpretation: Even the pessimistic scenario shows positive ROI, which strengthens the business case. But note how sensitive the results are: intervention success rate has the largest impact (52% to 203% ROI). When presenting to stakeholders, show the range, identify which assumptions drive the most uncertainty, and explain how you'll validate those assumptions during a pilot.</p> <p>Source: <code>slide_computations/module10_examples.py</code> - <code>demo_roi_sensitivity_analysis()</code></p>"},{"location":"modules/10-ethics-deployment/#communicating-uncertainty","title":"Communicating Uncertainty","text":"<p>Be honest about limitations: - Model accuracy is not 100% - Performance varies across segments - Future performance is not guaranteed - Edge cases exist</p> <p>Use confidence intervals: - \"We predict revenue of $1.2M \u00b1 $150K\" - \"The model is 85% confident this customer will churn\"</p> <p>Scenario analysis: - Best case / Base case / Worst case</p> <p>Stakeholders appreciate honesty. Overpromising leads to disappointment and loss of trust.</p>"},{"location":"modules/10-ethics-deployment/#managing-expectations","title":"Managing Expectations","text":"<p>Common pitfalls: - Overpromising accuracy - Underestimating timeline - Ignoring maintenance needs - Assuming it's a one-time effort</p> <p>Best practices: - Start with a pilot project - Set realistic expectations upfront - Plan for iteration\u2014first version won't be perfect - Communicate progress regularly - Budget for ongoing maintenance</p> <p>ML models are more like products than projects. The world changes, data shifts, and models degrade. You wouldn't build a website and never update it. Same with ML models.</p> <p>Estimating maintenance costs: Rule of thumb: 15-25% of initial development cost per year. Break down components: monitoring infrastructure, data pipeline maintenance, periodic retraining, model auditing, incident response. Staff time is usually the largest cost (one person at 20% time \u2248 $30-50K/year). Present stakeholders with scenarios: \"minimum maintenance\" costs X with degradation risk; \"recommended maintenance\" costs Y with better reliability.</p>"},{"location":"modules/10-ethics-deployment/#reflection-questions","title":"Reflection Questions","text":"<ol> <li> <p>Amazon's hiring tool was trained on successful employees. Why did it still produce biased results?</p> </li> <li> <p>A model has 85% accuracy for both men and women. Is it fair? What else would you check?</p> </li> <li> <p>Your model uses ZIP code, which correlates with race. Should you remove it? What are the trade-offs?</p> </li> <li> <p>A hospital wants to use ML to allocate scarce medical resources. What ethical considerations arise?</p> </li> <li> <p>Why can't we use regular k-fold cross-validation for time series?</p> </li> <li> <p>When would you choose Prophet over ARIMA?</p> </li> <li> <p>Your model works on your laptop but fails in production. What might cause this?</p> </li> <li> <p>A model improves accuracy by 2% but costs $500K to develop. How do you decide if it's worth it?</p> </li> <li> <p>How do you explain to a CFO that ML requires ongoing investment, not a one-time cost?</p> </li> </ol>"},{"location":"modules/10-ethics-deployment/#practice-problems","title":"Practice Problems","text":"<ol> <li> <p>Calculate fairness metrics from confusion matrices for two demographic groups</p> </li> <li> <p>Identify bias sources in a case study scenario</p> </li> <li> <p>Design a monitoring plan for a deployed fraud detection model</p> </li> <li> <p>Calculate ROI for an ML project given costs and projected benefits</p> </li> <li> <p>Write a brief stakeholder communication explaining a model's limitations</p> </li> </ol>"},{"location":"modules/10-ethics-deployment/#chapter-summary","title":"Chapter Summary","text":"<p>Six key takeaways from Module 10:</p> <ol> <li> <p>Bias enters ML through data and design; measure and mitigate it actively</p> </li> <li> <p>Fairness has multiple incompatible definitions\u2014you must choose</p> </li> <li> <p>Time series requires temporal validation; never use k-fold</p> </li> <li> <p>Deployment needs APIs, containers, and monitoring infrastructure</p> </li> <li> <p>Drift happens; plan for detection and retraining</p> </li> <li> <p>Business value must be quantified and communicated in dollars, not accuracy</p> </li> </ol>"},{"location":"modules/10-ethics-deployment/#course-summary","title":"Course Summary","text":"<p>Take a moment to appreciate how far you've come.</p> <p>You've learned to: - Build ML models (regression, classification, clustering) - Train ensemble methods and understand their power - Train neural networks for structured data, images, and text - Interpret and explain model decisions - Deploy responsibly with fairness considerations and monitoring</p> <p>You understand: - The ML workflow from data to deployment - Evaluation, validation, and avoiding common pitfalls - When to use which technique - How to communicate with stakeholders</p> <p>You're ready for the Capstone Project!</p>"},{"location":"modules/10-ethics-deployment/#whats-next-capstone-project","title":"What's Next: Capstone Project","text":"<p>The capstone project is your chance to apply everything you've learned to a real problem.</p> <p>You'll: - Define a business problem - Collect and prepare data - Build and evaluate models - Interpret and explain results - Create deployment and monitoring plans - Present to stakeholders</p> <p>This is the culmination of the course\u2014showing that you can take a problem from start to finish.</p>"}]}