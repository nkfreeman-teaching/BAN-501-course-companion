
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Predictive Modeling - Course Companion">
      
      
        <meta name="author" content="Nick Freeman">
      
      
        <link rel="canonical" href="https://nkfreeman-teaching.github.io/BAN-501-course-companion/modules/05-unsupervised/">
      
      
        <link rel="prev" href="../04-ensemble-methods/">
      
      
        <link rel="next" href="../06-neural-networks/">
      
      
        
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.1">
    
    
      
        <title>5. Unsupervised Learning - BAN 501 Course Companion</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.484c7ddc.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#module-5-unsupervised-learning" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="BAN 501 Course Companion" class="md-header__button md-logo" aria-label="BAN 501 Course Companion" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            BAN 501 Course Companion
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              5. Unsupervised Learning
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/nkfreeman-teaching/BAN-501-course-companion" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    nkfreeman-teaching/BAN-501-course-companion
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="BAN 501 Course Companion" class="md-nav__button md-logo" aria-label="BAN 501 Course Companion" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    BAN 501 Course Companion
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/nkfreeman-teaching/BAN-501-course-companion" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    nkfreeman-teaching/BAN-501-course-companion
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Home
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    
  
    Modules
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Modules
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../01-foundations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    1. Foundations of ML
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../02-regression/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    2. Regression
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../03-classification/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    3. Classification
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../04-ensemble-methods/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    4. Ensemble Methods
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    
  
    5. Unsupervised Learning
  

    
  </span>
  
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    
  
    5. Unsupervised Learning
  

    
  </span>
  
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    <span class="md-ellipsis">
      
        Introduction
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#learning-objectives" class="md-nav__link">
    <span class="md-ellipsis">
      
        Learning Objectives
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#51-clustering" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.1 Clustering
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5.1 Clustering">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#supervised-vs-unsupervised" class="md-nav__link">
    <span class="md-ellipsis">
      
        Supervised vs Unsupervised
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#three-components-k-means" class="md-nav__link">
    <span class="md-ellipsis">
      
        Three Components: K-Means
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clustering-applications" class="md-nav__link">
    <span class="md-ellipsis">
      
        Clustering Applications
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#k-means-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      
        K-Means Algorithm
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#choosing-k-elbow-method" class="md-nav__link">
    <span class="md-ellipsis">
      
        Choosing K: Elbow Method
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#choosing-k-silhouette-score" class="md-nav__link">
    <span class="md-ellipsis">
      
        Choosing K: Silhouette Score
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dbscan-density-based-clustering" class="md-nav__link">
    <span class="md-ellipsis">
      
        DBSCAN: Density-Based Clustering
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#k-means-vs-dbscan" class="md-nav__link">
    <span class="md-ellipsis">
      
        K-Means vs DBSCAN
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hdbscan-hierarchical-dbscan" class="md-nav__link">
    <span class="md-ellipsis">
      
        HDBSCAN: Hierarchical DBSCAN
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hierarchical-clustering" class="md-nav__link">
    <span class="md-ellipsis">
      
        Hierarchical Clustering
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#common-misconceptions" class="md-nav__link">
    <span class="md-ellipsis">
      
        Common Misconceptions
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#52-dimensionality-reduction" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.2 Dimensionality Reduction
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5.2 Dimensionality Reduction">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#why-reduce-dimensions" class="md-nav__link">
    <span class="md-ellipsis">
      
        Why Reduce Dimensions?
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#principal-component-analysis-pca" class="md-nav__link">
    <span class="md-ellipsis">
      
        Principal Component Analysis (PCA)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#choosing-number-of-components" class="md-nav__link">
    <span class="md-ellipsis">
      
        Choosing Number of Components
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#interpreting-pca-loadings" class="md-nav__link">
    <span class="md-ellipsis">
      
        Interpreting PCA Loadings
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#t-sne-for-visualization" class="md-nav__link">
    <span class="md-ellipsis">
      
        t-SNE for Visualization
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#umap" class="md-nav__link">
    <span class="md-ellipsis">
      
        UMAP
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#method-comparison" class="md-nav__link">
    <span class="md-ellipsis">
      
        Method Comparison
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mnist-example" class="md-nav__link">
    <span class="md-ellipsis">
      
        MNIST Example
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#common-misconceptions_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Common Misconceptions
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reflection-questions" class="md-nav__link">
    <span class="md-ellipsis">
      
        Reflection Questions
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#practice-problems" class="md-nav__link">
    <span class="md-ellipsis">
      
        Practice Problems
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#chapter-summary" class="md-nav__link">
    <span class="md-ellipsis">
      
        Chapter Summary
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#whats-next" class="md-nav__link">
    <span class="md-ellipsis">
      
        What's Next
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../06-neural-networks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    6. Neural Networks
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../07-computer-vision/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    7. Computer Vision
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../08-nlp/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    8. NLP
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../09-interpretability/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    9. Interpretability
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../10-ethics-deployment/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    10. Ethics & Deployment
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    
  
    Appendices
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Appendices
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../appendices/universal-approximators/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Neural Networks as Universal Approximators
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../appendices/cnn-architecture/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    CNN Architecture
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../appendices/transformer-architecture/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Transformer Architecture
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    <span class="md-ellipsis">
      
        Introduction
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#learning-objectives" class="md-nav__link">
    <span class="md-ellipsis">
      
        Learning Objectives
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#51-clustering" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.1 Clustering
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5.1 Clustering">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#supervised-vs-unsupervised" class="md-nav__link">
    <span class="md-ellipsis">
      
        Supervised vs Unsupervised
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#three-components-k-means" class="md-nav__link">
    <span class="md-ellipsis">
      
        Three Components: K-Means
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clustering-applications" class="md-nav__link">
    <span class="md-ellipsis">
      
        Clustering Applications
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#k-means-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      
        K-Means Algorithm
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#choosing-k-elbow-method" class="md-nav__link">
    <span class="md-ellipsis">
      
        Choosing K: Elbow Method
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#choosing-k-silhouette-score" class="md-nav__link">
    <span class="md-ellipsis">
      
        Choosing K: Silhouette Score
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dbscan-density-based-clustering" class="md-nav__link">
    <span class="md-ellipsis">
      
        DBSCAN: Density-Based Clustering
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#k-means-vs-dbscan" class="md-nav__link">
    <span class="md-ellipsis">
      
        K-Means vs DBSCAN
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hdbscan-hierarchical-dbscan" class="md-nav__link">
    <span class="md-ellipsis">
      
        HDBSCAN: Hierarchical DBSCAN
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hierarchical-clustering" class="md-nav__link">
    <span class="md-ellipsis">
      
        Hierarchical Clustering
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#common-misconceptions" class="md-nav__link">
    <span class="md-ellipsis">
      
        Common Misconceptions
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#52-dimensionality-reduction" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.2 Dimensionality Reduction
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5.2 Dimensionality Reduction">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#why-reduce-dimensions" class="md-nav__link">
    <span class="md-ellipsis">
      
        Why Reduce Dimensions?
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#principal-component-analysis-pca" class="md-nav__link">
    <span class="md-ellipsis">
      
        Principal Component Analysis (PCA)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#choosing-number-of-components" class="md-nav__link">
    <span class="md-ellipsis">
      
        Choosing Number of Components
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#interpreting-pca-loadings" class="md-nav__link">
    <span class="md-ellipsis">
      
        Interpreting PCA Loadings
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#t-sne-for-visualization" class="md-nav__link">
    <span class="md-ellipsis">
      
        t-SNE for Visualization
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#umap" class="md-nav__link">
    <span class="md-ellipsis">
      
        UMAP
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#method-comparison" class="md-nav__link">
    <span class="md-ellipsis">
      
        Method Comparison
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mnist-example" class="md-nav__link">
    <span class="md-ellipsis">
      
        MNIST Example
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#common-misconceptions_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Common Misconceptions
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reflection-questions" class="md-nav__link">
    <span class="md-ellipsis">
      
        Reflection Questions
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#practice-problems" class="md-nav__link">
    <span class="md-ellipsis">
      
        Practice Problems
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#chapter-summary" class="md-nav__link">
    <span class="md-ellipsis">
      
        Chapter Summary
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#whats-next" class="md-nav__link">
    <span class="md-ellipsis">
      
        What's Next
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="module-5-unsupervised-learning">Module 5: Unsupervised Learning<a class="headerlink" href="#module-5-unsupervised-learning" title="Permanent link">&para;</a></h1>
<h2 id="introduction">Introduction<a class="headerlink" href="#introduction" title="Permanent link">&para;</a></h2>
<p>Today marks a significant shift in how we think about machine learning.</p>
<p>In Modules 2 through 4, we always had a target variable—sales, churn, fraud. We had labels, and we trained models to predict those labels.</p>
<p><strong>Now we throw that away. No labels. No target variable.</strong></p>
<p>Unsupervised learning is about discovering structure in data when you don't know what you're looking for. You're exploring, not predicting.</p>
<p>This might sound less useful, but unsupervised learning solves critical business problems: customer segmentation, anomaly detection, data visualization, feature extraction. These are problems where labels don't exist or are too expensive to obtain.</p>
<p><strong>Validating unsupervised learning</strong>: "Right" is about usefulness, not correctness. Use internal metrics (silhouette, inertia), check stability across runs, and—most importantly—validate with domain experts. Do clusters suggest actionable strategies? A "statistically optimal" 7-cluster solution that marketing can't operationalize is less useful than a 3-cluster solution they can act on.</p>
<hr />
<h2 id="learning-objectives">Learning Objectives<a class="headerlink" href="#learning-objectives" title="Permanent link">&para;</a></h2>
<p>By the end of this module, you should be able to:</p>
<ol>
<li><strong>Explain</strong> the difference between supervised and unsupervised learning</li>
<li><strong>Apply</strong> K-means and DBSCAN clustering algorithms and interpret results</li>
<li><strong>Determine</strong> optimal number of clusters using elbow method and silhouette scores</li>
<li><strong>Apply</strong> PCA for dimensionality reduction and interpret principal components</li>
<li><strong>Use</strong> manifold learning techniques (t-SNE, UMAP) for visualization</li>
<li><strong>Identify</strong> business applications for clustering and dimensionality reduction</li>
</ol>
<hr />
<h2 id="51-clustering">5.1 Clustering<a class="headerlink" href="#51-clustering" title="Permanent link">&para;</a></h2>
<h3 id="supervised-vs-unsupervised">Supervised vs Unsupervised<a class="headerlink" href="#supervised-vs-unsupervised" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Supervised</th>
<th>Unsupervised</th>
</tr>
</thead>
<tbody>
<tr>
<td>Have labels</td>
<td>No labels</td>
</tr>
<tr>
<td>Learn to predict</td>
<td>Discover structure</td>
</tr>
<tr>
<td>Regression, Classification</td>
<td>Clustering, Dim. Reduction</td>
</tr>
</tbody>
</table>
<p><strong>Supervised</strong>: "Here are the right answers; learn to predict them."</p>
<p><strong>Unsupervised</strong>: "Here's the data; find interesting patterns."</p>
<h3 id="three-components-k-means">Three Components: K-Means<a class="headerlink" href="#three-components-k-means" title="Permanent link">&para;</a></h3>
<p>Even unsupervised algorithms fit our three-component framework:</p>
<table>
<thead>
<tr>
<th>Component</th>
<th>K-Means</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Decision Model</strong></td>
<td>Cluster assignments — each point belongs to nearest centroid</td>
</tr>
<tr>
<td><strong>Quality Measure</strong></td>
<td>Within-cluster sum of squares (inertia)</td>
</tr>
<tr>
<td><strong>Update Method</strong></td>
<td>Iterative assignment-update — alternate between assigning and moving centroids</td>
</tr>
</tbody>
</table>
<p><strong>Key difference from supervised learning</strong>: Without labels, we define "quality" differently. Instead of prediction error, we measure how compact and well-separated clusters are.</p>
<p><strong>Distinguishing real structure from noise</strong>: Clustering algorithms will always find clusters—even in random data. Use the gap statistic (compares quality to random data), stability analysis (cluster on subsets—real structure is stable), and multiple algorithms (if K-means, DBSCAN, and hierarchical all find similar groups, structure is more credible). Always verify clusters predict something meaningful.</p>
<h3 id="clustering-applications">Clustering Applications<a class="headerlink" href="#clustering-applications" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Customer segmentation</strong> — Group by purchasing behavior, target marketing per segment</li>
<li><strong>Document grouping</strong> — Organize by topic without predefined categories</li>
<li><strong>Anomaly detection</strong> — Find observations that don't fit any group</li>
<li><strong>Image compression</strong> — Reduce color palettes by clustering similar colors</li>
<li><strong>Gene expression</strong> — Group genes with similar activation patterns</li>
</ul>
<h3 id="k-means-algorithm">K-Means Algorithm<a class="headerlink" href="#k-means-algorithm" title="Permanent link">&para;</a></h3>
<p><strong>The algorithm:</strong>
1. <strong>Choose K</strong> (number of clusters)
2. <strong>Randomly initialize</strong> K centroids
3. <strong>Assign</strong>: Each point to nearest centroid
4. <strong>Update</strong>: Move centroids to mean of assigned points
5. <strong>Repeat</strong> until centroids stop moving</p>
<p><strong>The objective:</strong></p>
<div class="arithmatex">\[\text{minimize } \sum_{i=1}^{K}\sum_{x \in C_i} ||x - \mu_i||^2\]</div>
<p>Where <span class="arithmatex">\(\mu_i\)</span> is the centroid of cluster <span class="arithmatex">\(C_i\)</span>. Minimize total distance from points to their centroids.</p>
<p><strong>Strengths:</strong>
- Fast and scalable—works on millions of points
- Easy to implement and interpret
- Works well with spherical clusters</p>
<p><strong>Weaknesses:</strong>
- Must specify K in advance
- Sensitive to initialization
- Assumes spherical, similar-sized clusters</p>
<p><strong>"Spherical" clusters</strong>: K-means assigns points to the nearest centroid using Euclidean distance, implicitly assuming clusters are ball-shaped with equal spread in all directions. K-means essentially draws Voronoi cells (straight-line boundaries)—any cluster that can't fit in a convex cell will be problematic. For non-spherical shapes, use DBSCAN (any shape), GMMs (elliptical), or spectral clustering (complex manifolds).</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.cluster</span><span class="w"> </span><span class="kn">import</span> <span class="n">KMeans</span>
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a><span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span>
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>    <span class="n">n_clusters</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
</span><span id="__span-0-5"><a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>    <span class="n">init</span><span class="o">=</span><span class="s1">&#39;k-means++&#39;</span><span class="p">,</span>    <span class="c1"># Smart initialization</span>
</span><span id="__span-0-6"><a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>    <span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>           <span class="c1"># Run 10 times, keep best</span>
</span><span id="__span-0-7"><a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
</span><span id="__span-0-8"><a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a><span class="p">)</span>
</span><span id="__span-0-9"><a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a>
</span><span id="__span-0-10"><a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a><span class="n">labels</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
</span><span id="__span-0-11"><a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a><span class="n">centroids</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">cluster_centers_</span>
</span><span id="__span-0-12"><a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Inertia: </span><span class="si">{</span><span class="n">kmeans</span><span class="o">.</span><span class="n">inertia_</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span></code></pre></div>
<h3 id="choosing-k-elbow-method">Choosing K: Elbow Method<a class="headerlink" href="#choosing-k-elbow-method" title="Permanent link">&para;</a></h3>
<p><strong>Process:</strong>
1. Run K-means for K = 1, 2, 3, ..., n
2. Plot inertia vs K
3. Look for the "elbow" where adding clusters gives diminishing returns</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-1-1"><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a><span class="n">inertias</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="__span-1-2"><a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a><span class="n">K_range</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span>
</span><span id="__span-1-3"><a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a>
</span><span id="__span-1-4"><a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a><span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">K_range</span><span class="p">:</span>
</span><span id="__span-1-5"><a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a>    <span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</span><span id="__span-1-6"><a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a>    <span class="n">kmeans</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
</span><span id="__span-1-7"><a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a>    <span class="n">inertias</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">kmeans</span><span class="o">.</span><span class="n">inertia_</span><span class="p">)</span>
</span><span id="__span-1-8"><a id="__codelineno-1-8" name="__codelineno-1-8" href="#__codelineno-1-8"></a>
</span><span id="__span-1-9"><a id="__codelineno-1-9" name="__codelineno-1-9" href="#__codelineno-1-9"></a><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">K_range</span><span class="p">,</span> <span class="n">inertias</span><span class="p">,</span> <span class="s1">&#39;bo-&#39;</span><span class="p">)</span>
</span><span id="__span-1-10"><a id="__codelineno-1-10" name="__codelineno-1-10" href="#__codelineno-1-10"></a><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Number of Clusters (K)&#39;</span><span class="p">)</span>
</span><span id="__span-1-11"><a id="__codelineno-1-11" name="__codelineno-1-11" href="#__codelineno-1-11"></a><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Inertia&#39;</span><span class="p">)</span>
</span><span id="__span-1-12"><a id="__codelineno-1-12" name="__codelineno-1-12" href="#__codelineno-1-12"></a><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Elbow Method&#39;</span><span class="p">)</span>
</span></code></pre></div>
<h3 id="choosing-k-silhouette-score">Choosing K: Silhouette Score<a class="headerlink" href="#choosing-k-silhouette-score" title="Permanent link">&para;</a></h3>
<p>For each point, measure how similar it is to its own cluster vs. other clusters:</p>
<div class="arithmatex">\[s(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))}\]</div>
<p>Where:
- <span class="arithmatex">\(a(i)\)</span> = average distance to points in same cluster
- <span class="arithmatex">\(b(i)\)</span> = average distance to points in nearest other cluster</p>
<p><strong>Interpretation:</strong>
- s = 1: Well-clustered (far from other clusters)
- s = 0: On boundary between clusters
- s = -1: Probably in wrong cluster</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-2-1"><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">silhouette_score</span><span class="p">,</span> <span class="n">silhouette_samples</span>
</span><span id="__span-2-2"><a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a>
</span><span id="__span-2-3"><a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a><span class="c1"># Overall score</span>
</span><span id="__span-2-4"><a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a><span class="n">score</span> <span class="o">=</span> <span class="n">silhouette_score</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</span><span id="__span-2-5"><a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a>
</span><span id="__span-2-6"><a id="__codelineno-2-6" name="__codelineno-2-6" href="#__codelineno-2-6"></a><span class="c1"># Per-sample (for diagnostics)</span>
</span><span id="__span-2-7"><a id="__codelineno-2-7" name="__codelineno-2-7" href="#__codelineno-2-7"></a><span class="n">sample_scores</span> <span class="o">=</span> <span class="n">silhouette_samples</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</span></code></pre></div>
<p><strong>Individual scores are useful too:</strong>
- Find misclassified points (negative scores)
- Identify boundary cases (scores near 0)
- Detect outliers (very low scores)</p>
<p><strong>Business consideration</strong>: Sometimes the "right" K comes from domain knowledge, not just metrics!</p>
<p><strong>When elbow and silhouette disagree</strong>: Elbow (inertia) measures compactness; silhouette measures both compactness AND separation. Adding clusters always reduces inertia but may not improve silhouette if new clusters aren't well-separated. If elbow says 5 and silhouette says 3, clusters 4-5 might be subdividing natural groups. Look at both metrics, examine cluster profiles, consider business constraints, and check stability. There's rarely a single "correct" K.</p>
<h3 id="dbscan-density-based-clustering">DBSCAN: Density-Based Clustering<a class="headerlink" href="#dbscan-density-based-clustering" title="Permanent link">&para;</a></h3>
<p>K-means assumes spherical clusters. DBSCAN handles:
- Irregular shapes
- Different densities
- Noise/outliers</p>
<p><strong>Core concepts:</strong>
- <strong>Core point</strong>: Has at least <code>min_samples</code> points within <code>eps</code> distance
- <strong>Border point</strong>: Within <code>eps</code> of a core point, but not core itself
- <strong>Noise point</strong>: Neither (labeled -1)</p>
<p><strong>Algorithm:</strong>
1. Find all core points
2. Connect core points within <code>eps</code> of each other (transitively)
3. Assign border points to nearest core point's cluster
4. Everything else is noise</p>
<p><strong>Connecting core points (step 2) in detail:</strong></p>
<p>Two core points belong to the same cluster if they're "density-reachable":
- Direct: Within <code>eps</code> of each other
- Transitive: A connects to B, B connects to C → A and C same cluster</p>
<p>This is graph traversal where core points are nodes and edges exist between points within <code>eps</code>. Each connected component becomes a cluster.</p>
<p><strong>Choosing eps and min_samples</strong>: For eps, use the k-distance plot—compute each point's distance to its k-th nearest neighbor, sort and plot, look for the elbow. For min_samples, start with dimensions + 1 or 2×dimensions. Larger min_samples = more conservative. If DBSCAN parameter tuning is frustrating, try HDBSCAN—it removes the eps parameter entirely.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-3-1"><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.cluster</span><span class="w"> </span><span class="kn">import</span> <span class="n">DBSCAN</span>
</span><span id="__span-3-2"><a id="__codelineno-3-2" name="__codelineno-3-2" href="#__codelineno-3-2"></a>
</span><span id="__span-3-3"><a id="__codelineno-3-3" name="__codelineno-3-3" href="#__codelineno-3-3"></a><span class="n">dbscan</span> <span class="o">=</span> <span class="n">DBSCAN</span><span class="p">(</span><span class="n">eps</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">min_samples</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</span><span id="__span-3-4"><a id="__codelineno-3-4" name="__codelineno-3-4" href="#__codelineno-3-4"></a><span class="n">labels</span> <span class="o">=</span> <span class="n">dbscan</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
</span><span id="__span-3-5"><a id="__codelineno-3-5" name="__codelineno-3-5" href="#__codelineno-3-5"></a>
</span><span id="__span-3-6"><a id="__codelineno-3-6" name="__codelineno-3-6" href="#__codelineno-3-6"></a><span class="n">n_clusters</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">labels</span><span class="p">))</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="k">if</span> <span class="o">-</span><span class="mi">1</span> <span class="ow">in</span> <span class="n">labels</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>
</span><span id="__span-3-7"><a id="__codelineno-3-7" name="__codelineno-3-7" href="#__codelineno-3-7"></a><span class="n">n_noise</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="__span-3-8"><a id="__codelineno-3-8" name="__codelineno-3-8" href="#__codelineno-3-8"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Clusters: </span><span class="si">{</span><span class="n">n_clusters</span><span class="si">}</span><span class="s2">, Noise: </span><span class="si">{</span><span class="n">n_noise</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span></code></pre></div>
<h3 id="k-means-vs-dbscan">K-Means vs DBSCAN<a class="headerlink" href="#k-means-vs-dbscan" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>K-Means</th>
<th>DBSCAN</th>
</tr>
</thead>
<tbody>
<tr>
<td># Clusters</td>
<td>Must specify</td>
<td>Auto-detected</td>
</tr>
<tr>
<td>Shapes</td>
<td>Spherical</td>
<td>Arbitrary</td>
</tr>
<tr>
<td>Handles noise</td>
<td>No</td>
<td>Yes (labels -1)</td>
</tr>
<tr>
<td>Speed</td>
<td>Very fast</td>
<td>Slower</td>
</tr>
</tbody>
</table>
<h3 id="hdbscan-hierarchical-dbscan">HDBSCAN: Hierarchical DBSCAN<a class="headerlink" href="#hdbscan-hierarchical-dbscan" title="Permanent link">&para;</a></h3>
<p>HDBSCAN addresses DBSCAN's sensitivity to the <code>eps</code> parameter:</p>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>DBSCAN</th>
<th>HDBSCAN</th>
</tr>
</thead>
<tbody>
<tr>
<td>Parameters</td>
<td><code>eps</code> and <code>min_samples</code></td>
<td>Just <code>min_samples</code></td>
</tr>
<tr>
<td>Cluster densities</td>
<td>Assumes uniform</td>
<td>Handles varying</td>
</tr>
</tbody>
</table>
<p><strong>How it works:</strong>
1. Build a hierarchy considering all possible <code>eps</code> values
2. Find stable clusters that persist across <code>eps</code> range
3. Extract flat clustering from the hierarchy</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-4-1"><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">hdbscan</span>
</span><span id="__span-4-2"><a id="__codelineno-4-2" name="__codelineno-4-2" href="#__codelineno-4-2"></a><span class="n">clusterer</span> <span class="o">=</span> <span class="n">hdbscan</span><span class="o">.</span><span class="n">HDBSCAN</span><span class="p">(</span><span class="n">min_cluster_size</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
</span><span id="__span-4-3"><a id="__codelineno-4-3" name="__codelineno-4-3" href="#__codelineno-4-3"></a><span class="n">labels</span> <span class="o">=</span> <span class="n">clusterer</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span></code></pre></div>
<h3 id="hierarchical-clustering">Hierarchical Clustering<a class="headerlink" href="#hierarchical-clustering" title="Permanent link">&para;</a></h3>
<p>Build a tree of nested clusters.</p>
<p><strong>Agglomerative (bottom-up):</strong>
1. Start: Each point is its own cluster
2. Find two closest clusters
3. Merge them
4. Repeat until one cluster remains</p>
<p><strong>Linkage methods</strong> (distance between clusters):
- <strong>Single</strong>: Minimum distance between any points
- <strong>Complete</strong>: Maximum distance between any points
- <strong>Average</strong>: Average distance between all pairs
- <strong>Ward</strong>: Minimize variance increase when merging</p>
<p><strong>Dendrogram</strong>: Tree showing merge history
- Y-axis = distance at which clusters merged
- Cut at any height to get that many clusters</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-5-1"><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">scipy.cluster.hierarchy</span><span class="w"> </span><span class="kn">import</span> <span class="n">dendrogram</span><span class="p">,</span> <span class="n">linkage</span><span class="p">,</span> <span class="n">fcluster</span>
</span><span id="__span-5-2"><a id="__codelineno-5-2" name="__codelineno-5-2" href="#__codelineno-5-2"></a>
</span><span id="__span-5-3"><a id="__codelineno-5-3" name="__codelineno-5-3" href="#__codelineno-5-3"></a><span class="n">Z</span> <span class="o">=</span> <span class="n">linkage</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;ward&#39;</span><span class="p">)</span>
</span><span id="__span-5-4"><a id="__codelineno-5-4" name="__codelineno-5-4" href="#__codelineno-5-4"></a>
</span><span id="__span-5-5"><a id="__codelineno-5-5" name="__codelineno-5-5" href="#__codelineno-5-5"></a><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
</span><span id="__span-5-6"><a id="__codelineno-5-6" name="__codelineno-5-6" href="#__codelineno-5-6"></a><span class="n">dendrogram</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>
</span><span id="__span-5-7"><a id="__codelineno-5-7" name="__codelineno-5-7" href="#__codelineno-5-7"></a><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Sample Index&#39;</span><span class="p">)</span>
</span><span id="__span-5-8"><a id="__codelineno-5-8" name="__codelineno-5-8" href="#__codelineno-5-8"></a><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Distance&#39;</span><span class="p">)</span>
</span><span id="__span-5-9"><a id="__codelineno-5-9" name="__codelineno-5-9" href="#__codelineno-5-9"></a>
</span><span id="__span-5-10"><a id="__codelineno-5-10" name="__codelineno-5-10" href="#__codelineno-5-10"></a><span class="c1"># Cut to get 3 clusters</span>
</span><span id="__span-5-11"><a id="__codelineno-5-11" name="__codelineno-5-11" href="#__codelineno-5-11"></a><span class="n">labels</span> <span class="o">=</span> <span class="n">fcluster</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;maxclust&#39;</span><span class="p">)</span>
</span></code></pre></div>
<h3 id="common-misconceptions">Common Misconceptions<a class="headerlink" href="#common-misconceptions" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Misconception</th>
<th>Reality</th>
</tr>
</thead>
<tbody>
<tr>
<td>"There's one correct number of clusters"</td>
<td>Clustering is exploratory. Multiple valid solutions exist. Business context matters.</td>
</tr>
<tr>
<td>"K-means always works"</td>
<td>Fails on complex shapes, varying densities, outliers.</td>
</tr>
<tr>
<td>"Silhouette = 0.9 means perfect clustering"</td>
<td>Silhouette measures separation, not business meaning.</td>
</tr>
<tr>
<td>"More clusters is always better"</td>
<td>Reduces variance but may not be useful. Aim for interpretable segments.</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="52-dimensionality-reduction">5.2 Dimensionality Reduction<a class="headerlink" href="#52-dimensionality-reduction" title="Permanent link">&para;</a></h2>
<h3 id="why-reduce-dimensions">Why Reduce Dimensions?<a class="headerlink" href="#why-reduce-dimensions" title="Permanent link">&para;</a></h3>
<p><strong>The curse of dimensionality:</strong>
- High-dimensional spaces are sparse
- Distance metrics become less meaningful
- Models overfit more easily</p>
<p><strong>Benefits:</strong>
- <strong>Visualization</strong>: Can't plot 50 dimensions. Can plot 2.
- <strong>Noise reduction</strong>: Remove uninformative dimensions
- <strong>Faster training</strong>: Fewer features
- <strong>Feature extraction</strong>: Create meaningful composites</p>
<p><strong>Are we losing important information?</strong> You ARE losing information—the question is signal vs. noise. If 10 components capture 95% of variance, the last 40 combined contribute 5% (mostly noise). Verify by comparing model performance with/without reduction. Caveats: rare but important patterns may have low variance; PCA doesn't know your target, so captured variance might not be predictive.</p>
<h3 id="principal-component-analysis-pca">Principal Component Analysis (PCA)<a class="headerlink" href="#principal-component-analysis-pca" title="Permanent link">&para;</a></h3>
<p>Find new axes (principal components) that:
1. Are linear combinations of original features
2. Capture maximum variance
3. Are orthogonal (uncorrelated)</p>
<p><strong>Algorithm:</strong>
1. Center the data (subtract mean)
2. Find direction of maximum variance → PC1
3. Find direction of max remaining variance, perpendicular to PC1 → PC2
4. Continue...</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-6-1"><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.decomposition</span><span class="w"> </span><span class="kn">import</span> <span class="n">PCA</span>
</span><span id="__span-6-2"><a id="__codelineno-6-2" name="__codelineno-6-2" href="#__codelineno-6-2"></a>
</span><span id="__span-6-3"><a id="__codelineno-6-3" name="__codelineno-6-3" href="#__codelineno-6-3"></a><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span><span id="__span-6-4"><a id="__codelineno-6-4" name="__codelineno-6-4" href="#__codelineno-6-4"></a><span class="n">X_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
</span><span id="__span-6-5"><a id="__codelineno-6-5" name="__codelineno-6-5" href="#__codelineno-6-5"></a>
</span><span id="__span-6-6"><a id="__codelineno-6-6" name="__codelineno-6-6" href="#__codelineno-6-6"></a><span class="c1"># Variance explained</span>
</span><span id="__span-6-7"><a id="__codelineno-6-7" name="__codelineno-6-7" href="#__codelineno-6-7"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Variance explained: </span><span class="si">{</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-6-8"><a id="__codelineno-6-8" name="__codelineno-6-8" href="#__codelineno-6-8"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total: </span><span class="si">{</span><span class="nb">sum</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">)</span><span class="si">:</span><span class="s2">.2%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span></code></pre></div>
<h3 id="choosing-number-of-components">Choosing Number of Components<a class="headerlink" href="#choosing-number-of-components" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Scree plot</strong>: Variance explained vs component number</li>
<li><strong>Cumulative variance</strong>: Keep enough for 80-95%</li>
<li><strong>Kaiser criterion</strong>: Keep components with eigenvalue &gt; 1</li>
</ul>
<div class="language-python highlight"><pre><span></span><code><span id="__span-7-1"><a id="__codelineno-7-1" name="__codelineno-7-1" href="#__codelineno-7-1"></a><span class="n">pca_full</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">()</span>
</span><span id="__span-7-2"><a id="__codelineno-7-2" name="__codelineno-7-2" href="#__codelineno-7-2"></a><span class="n">pca_full</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
</span><span id="__span-7-3"><a id="__codelineno-7-3" name="__codelineno-7-3" href="#__codelineno-7-3"></a>
</span><span id="__span-7-4"><a id="__codelineno-7-4" name="__codelineno-7-4" href="#__codelineno-7-4"></a><span class="c1"># Cumulative variance plot</span>
</span><span id="__span-7-5"><a id="__codelineno-7-5" name="__codelineno-7-5" href="#__codelineno-7-5"></a><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">pca_full</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span>
</span><span id="__span-7-6"><a id="__codelineno-7-6" name="__codelineno-7-6" href="#__codelineno-7-6"></a>         <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">pca_full</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">),</span> <span class="s1">&#39;bo-&#39;</span><span class="p">)</span>
</span><span id="__span-7-7"><a id="__codelineno-7-7" name="__codelineno-7-7" href="#__codelineno-7-7"></a><span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
</span><span id="__span-7-8"><a id="__codelineno-7-8" name="__codelineno-7-8" href="#__codelineno-7-8"></a><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Number of Components&#39;</span><span class="p">)</span>
</span><span id="__span-7-9"><a id="__codelineno-7-9" name="__codelineno-7-9" href="#__codelineno-7-9"></a><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Cumulative Variance&#39;</span><span class="p">)</span>
</span></code></pre></div>
<h3 id="interpreting-pca-loadings">Interpreting PCA Loadings<a class="headerlink" href="#interpreting-pca-loadings" title="Permanent link">&para;</a></h3>
<p><strong>Loadings</strong> show how original features contribute to each component:</p>
<table>
<thead>
<tr>
<th>Feature</th>
<th>PC1</th>
<th>PC2</th>
</tr>
</thead>
<tbody>
<tr>
<td>Income</td>
<td>0.8</td>
<td>0.1</td>
</tr>
<tr>
<td>Age</td>
<td>0.7</td>
<td>-0.2</td>
</tr>
<tr>
<td>Spending</td>
<td>0.6</td>
<td>0.8</td>
</tr>
</tbody>
</table>
<p><strong>Interpretation:</strong>
- PC1 loads on Income, Age, Spending → "Overall affluence"
- PC2 loads mainly on Spending → "Spending tendency"</p>
<p><strong>Naming is subjective</strong>: Component naming is interpretation, not discovery. Two analysts might name the same loadings differently ("Wealth" vs "Financial Stability" vs "Affluence Score"). Report actual loadings alongside interpretation, acknowledge subjectivity, and validate with domain experts. If you can't tell a coherent story, the component may not be meaningfully interpretable.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-8-1"><a id="__codelineno-8-1" name="__codelineno-8-1" href="#__codelineno-8-1"></a><span class="c1"># Loadings are in components_ (rows = components, cols = features)</span>
</span><span id="__span-8-2"><a id="__codelineno-8-2" name="__codelineno-8-2" href="#__codelineno-8-2"></a><span class="n">loadings</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">components_</span>
</span><span id="__span-8-3"><a id="__codelineno-8-3" name="__codelineno-8-3" href="#__codelineno-8-3"></a>
</span><span id="__span-8-4"><a id="__codelineno-8-4" name="__codelineno-8-4" href="#__codelineno-8-4"></a><span class="kn">import</span><span class="w"> </span><span class="nn">polars</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pl</span>
</span><span id="__span-8-5"><a id="__codelineno-8-5" name="__codelineno-8-5" href="#__codelineno-8-5"></a><span class="n">loadings_df</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
</span><span id="__span-8-6"><a id="__codelineno-8-6" name="__codelineno-8-6" href="#__codelineno-8-6"></a>    <span class="n">loadings</span><span class="p">,</span>
</span><span id="__span-8-7"><a id="__codelineno-8-7" name="__codelineno-8-7" href="#__codelineno-8-7"></a>    <span class="n">schema</span><span class="o">=</span><span class="n">feature_names</span>
</span><span id="__span-8-8"><a id="__codelineno-8-8" name="__codelineno-8-8" href="#__codelineno-8-8"></a><span class="p">)</span><span class="o">.</span><span class="n">with_row_index</span><span class="p">(</span><span class="s2">&quot;PC&quot;</span><span class="p">)</span>
</span></code></pre></div>
<h3 id="t-sne-for-visualization">t-SNE for Visualization<a class="headerlink" href="#t-sne-for-visualization" title="Permanent link">&para;</a></h3>
<p>PCA assumes linear relationships. t-SNE handles non-linear manifolds.</p>
<p><strong>Goal</strong>: Preserve local neighborhoods in 2D
- Points close in high-D stay close
- Points far apart can move freely</p>
<p><strong>Key parameter</strong>: <code>perplexity</code> (~5-50)
- Roughly expected number of neighbors
- Try multiple values</p>
<p><strong>Critical caveats:</strong>
- Stochastic—different runs give different results
- <strong>Cluster sizes are meaningless</strong> (distances distorted)
- Can create false patterns in random data
- Slow for large datasets
- <strong>ONLY for visualization, NOT preprocessing!</strong></p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-9-1"><a id="__codelineno-9-1" name="__codelineno-9-1" href="#__codelineno-9-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.manifold</span><span class="w"> </span><span class="kn">import</span> <span class="n">TSNE</span>
</span><span id="__span-9-2"><a id="__codelineno-9-2" name="__codelineno-9-2" href="#__codelineno-9-2"></a>
</span><span id="__span-9-3"><a id="__codelineno-9-3" name="__codelineno-9-3" href="#__codelineno-9-3"></a><span class="n">tsne</span> <span class="o">=</span> <span class="n">TSNE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">perplexity</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</span><span id="__span-9-4"><a id="__codelineno-9-4" name="__codelineno-9-4" href="#__codelineno-9-4"></a><span class="n">X_tsne</span> <span class="o">=</span> <span class="n">tsne</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
</span></code></pre></div>
<p><strong>Never use t-SNE coordinates as features for a classifier.</strong> Distances are distorted. Use PCA for preprocessing.</p>
<p><strong>Trusting t-SNE clusters</strong>: t-SNE preserves local neighborhoods but distorts global distances, cluster sizes, and densities. To avoid being fooled: run multiple times with different seeds/perplexity, validate with clustering on the original high-D data (if K-means finds no structure there, t-SNE may be misleading), and check perplexity sensitivity. t-SNE is for visualization and hypothesis generation—always verify clusters with methods on the original data.</p>
<h3 id="umap">UMAP<a class="headerlink" href="#umap" title="Permanent link">&para;</a></h3>
<p>UMAP (Uniform Manifold Approximation and Projection) is often better than t-SNE:</p>
<ul>
<li><strong>Faster</strong>, especially for large data</li>
<li><strong>Preserves global structure</strong> better</li>
<li><strong>Can be used for preprocessing</strong> (not just visualization)</li>
<li><strong>More reproducible</strong></li>
</ul>
<div class="language-python highlight"><pre><span></span><code><span id="__span-10-1"><a id="__codelineno-10-1" name="__codelineno-10-1" href="#__codelineno-10-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">umap</span>
</span><span id="__span-10-2"><a id="__codelineno-10-2" name="__codelineno-10-2" href="#__codelineno-10-2"></a>
</span><span id="__span-10-3"><a id="__codelineno-10-3" name="__codelineno-10-3" href="#__codelineno-10-3"></a><span class="n">reducer</span> <span class="o">=</span> <span class="n">umap</span><span class="o">.</span><span class="n">UMAP</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_neighbors</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">min_dist</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</span><span id="__span-10-4"><a id="__codelineno-10-4" name="__codelineno-10-4" href="#__codelineno-10-4"></a><span class="n">X_umap</span> <span class="o">=</span> <span class="n">reducer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
</span></code></pre></div>
<h3 id="method-comparison">Method Comparison<a class="headerlink" href="#method-comparison" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Method</th>
<th>Speed</th>
<th>Global Structure</th>
<th>Use For</th>
</tr>
</thead>
<tbody>
<tr>
<td>PCA</td>
<td>Fast</td>
<td>Preserved</td>
<td>Preprocessing, visualization</td>
</tr>
<tr>
<td>t-SNE</td>
<td>Slow</td>
<td>Lost</td>
<td>Visualization only</td>
</tr>
<tr>
<td>UMAP</td>
<td>Medium</td>
<td>Partially preserved</td>
<td>Both</td>
</tr>
</tbody>
</table>
<p><strong>Rule of thumb:</strong>
- PCA for preprocessing and quick visualization
- t-SNE or UMAP for beautiful visualizations
- UMAP if you want the best of both worlds</p>
<h3 id="mnist-example">MNIST Example<a class="headerlink" href="#mnist-example" title="Permanent link">&para;</a></h3>
<ul>
<li>Original: 784 dimensions (28×28 pixels)</li>
<li>PCA to 2D: Blurry separation</li>
<li>t-SNE/UMAP to 2D: Clear digit clusters!</li>
</ul>
<p><strong>Why?</strong> The digit manifold is non-linear. PCA's linear assumption can't capture it. t-SNE and UMAP can.</p>
<h3 id="common-misconceptions_1">Common Misconceptions<a class="headerlink" href="#common-misconceptions_1" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Misconception</th>
<th>Reality</th>
</tr>
</thead>
<tbody>
<tr>
<td>"PCA finds the most important features"</td>
<td>PCA finds linear combinations. Components may not correspond to individual features.</td>
</tr>
<tr>
<td>"t-SNE cluster sizes are meaningful"</td>
<td>t-SNE distorts distances. A big cluster in t-SNE might be same size as small one in reality.</td>
</tr>
<tr>
<td>"More components = better"</td>
<td>More preserves more info but may include noise. Choose based on task.</td>
</tr>
<tr>
<td>"Dimensionality reduction always helps ML models"</td>
<td>Sometimes original features are better. Compare performance.</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="reflection-questions">Reflection Questions<a class="headerlink" href="#reflection-questions" title="Permanent link">&para;</a></h2>
<ol>
<li>
<p>You're segmenting customers for marketing. K-means suggests 5 clusters, but your team can only create 3 campaigns. What do you do?</p>
</li>
<li>
<p>Your clustering puts 95% of data in one cluster and creates 4 tiny ones. Is this a problem? What might cause this?</p>
</li>
<li>
<p>When would you choose DBSCAN over K-means? Give a business example.</p>
</li>
<li>
<p>A colleague says they found "the optimal number of clusters." Why should you be skeptical?</p>
</li>
<li>
<p>PCA on customer data shows PC1 explains 80% of variance. Should you only use PC1?</p>
</li>
<li>
<p>You run t-SNE twice and get different-looking plots. Is one wrong?</p>
</li>
<li>
<p>A colleague says "UMAP proves our data has 5 clusters." What's wrong with this statement?</p>
</li>
</ol>
<hr />
<h2 id="practice-problems">Practice Problems<a class="headerlink" href="#practice-problems" title="Permanent link">&para;</a></h2>
<ol>
<li>
<p>Given cluster assignments, calculate silhouette score by hand for a small example</p>
</li>
<li>
<p>Interpret PCA loadings for a business dataset (name the components)</p>
</li>
<li>
<p>Choose between K-means and DBSCAN for different data scenarios</p>
</li>
<li>
<p>Explain why t-SNE shouldn't be used for preprocessing</p>
</li>
<li>
<p>For customer data with features {Income, Age, Transactions, Days_Since_Purchase}, describe what PC1 and PC2 might represent</p>
</li>
</ol>
<hr />
<h2 id="chapter-summary">Chapter Summary<a class="headerlink" href="#chapter-summary" title="Permanent link">&para;</a></h2>
<p><strong>Six key takeaways from Module 5:</strong></p>
<ol>
<li>
<p><strong>Unsupervised learning</strong> discovers structure without labels</p>
</li>
<li>
<p><strong>K-means</strong> is fast but needs spherical clusters and specified K</p>
</li>
<li>
<p><strong>DBSCAN</strong> handles arbitrary shapes and identifies outliers</p>
</li>
<li>
<p><strong>Silhouette scores</strong> measure cluster quality (but not business meaning)</p>
</li>
<li>
<p><strong>PCA</strong> finds linear combinations that maximize variance</p>
</li>
<li>
<p><strong>t-SNE/UMAP</strong> reveal non-linear structure—use t-SNE for visualization only!</p>
</li>
</ol>
<hr />
<h2 id="whats-next">What's Next<a class="headerlink" href="#whats-next" title="Permanent link">&para;</a></h2>
<p>In Module 6, we tackle <strong>Neural Networks Fundamentals</strong>:
- Perceptrons and multi-layer networks
- Activation functions
- Backpropagation
- Deep learning basics</p>
<p>Here's an interesting connection: dimensionality reduction is related to neural network feature learning. Neural networks automatically learn compressed representations of inputs—that's partly why deep learning works so well.</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "../..", "features": ["navigation.instant", "navigation.tracking", "navigation.sections", "navigation.expand", "navigation.top", "search.suggest", "search.highlight", "content.code.copy"], "search": "../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.79ae519e.min.js"></script>
      
        <script src="../../javascripts/mathjax.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>