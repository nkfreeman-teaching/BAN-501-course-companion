# BAN 501: Predictive Modeling

## Course Companion

Welcome to the BAN 501 Course Companion. This resource provides comprehensive coverage of predictive modeling concepts, from foundational machine learning principles to advanced deep learning applications.

## How to Use This Companion

- **Sequential reading**: Work through modules 1-10 in order for a complete learning path
- **Reference**: Jump to specific topics using the navigation sidebar
- **Search**: Use the search bar to find specific concepts or terms
- **Deep dives**: Explore appendices for extended technical discussions

## Module Overview

| Module | Topic | Key Concepts |
|--------|-------|--------------|
| 1 | Foundations | Train/test splits, bias-variance tradeoff, evaluation metrics |
| 2 | Regression | Linear regression, gradient descent, regularization |
| 3 | Classification | Logistic regression, decision boundaries, ROC/AUC |
| 4 | Ensemble Methods | Bagging, boosting, random forests, XGBoost |
| 5 | Unsupervised Learning | Clustering, dimensionality reduction, PCA |
| 6 | Neural Networks | Perceptrons, backpropagation, deep learning |
| 7 | Computer Vision | CNNs, image classification, transfer learning |
| 8 | NLP | Text processing, embeddings, transformers |
| 9 | Interpretability | SHAP, LIME, feature importance |
| 10 | Ethics & Deployment | Fairness, model deployment, monitoring |

## Prerequisites

This companion assumes familiarity with:

- Basic Python programming
- Introductory statistics (mean, variance, distributions)
- Linear algebra fundamentals (matrices, vectors)

## Appendices

The appendices provide deeper technical explorations:

- **Universal Approximators**: Why neural networks can learn any function
- **CNN Architecture**: Detailed breakdown of convolutional networks
- **Transformer Architecture**: The architecture behind modern NLP
- **Surprising Phenomena**: Double descent, grokking, and emergent abilities
