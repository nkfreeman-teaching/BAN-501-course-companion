# Module 10: Ethics, Deployment & Real-World ML

## Introduction

We've covered an incredible amount of ground in this course. You can build regression models, classification models, ensemble methods, neural networks, CNNs for images, transformers for text. You can interpret models with SHAP and LIME. You know how to evaluate, tune, and avoid common pitfalls.

But here's the thing: **Building a model is only half the journey.**

This module tackles the other half—getting models into the real world responsibly and effectively. This means grappling with ethics and fairness, learning time series forecasting, understanding deployment, and calculating business value.

These topics bridge technical skills to real-world impact. Every data scientist who wants to make a difference needs to master them.

**Responsibility for fairness**: All three share responsibility. Data scientists are the first line of defense and should raise concerns. Companies set culture, allocate resources for fairness audits, and establish review processes—they're culpable for pressuring fast deployment without ethical review. Regulators provide external accountability that markets fail to create. The healthiest ecosystem has all three layers; relying on any single one is insufficient.

---

## Learning Objectives

By the end of this module, you should be able to:

1. **Identify** sources of bias in ML systems and propose mitigation strategies
2. **Apply** fairness metrics to evaluate model equity across groups
3. **Build** time series forecasting models using appropriate techniques
4. **Explain** the basics of model deployment and MLOps
5. **Calculate** business value and ROI of ML projects
6. **Communicate** uncertainty and manage stakeholder expectations

---

## 10.1 Ethics & Responsible AI

### The Stakes Are High

ML systems are making consequential decisions: who gets a loan, who gets a job, who gets parole, who gets medical treatment. These aren't abstract technical problems—they affect real people's lives.

And here's the uncomfortable truth: **ML systems can be biased. They can be unfair. They can cause harm.**

Not because the engineers are malicious, but because bias creeps in through data, design choices, and blind spots. If we're going to deploy these systems, we need to understand how bias arises and how to mitigate it.

### Sources of Bias in ML

**Historical Bias**: Training data reflects past discrimination.
- If you train on 10 years of hiring data, and that data reflects historical biases against women or minorities, your model learns those biases.
- The model isn't "biased by itself"—it's learning patterns from biased data.

**Selection Bias**: Training data doesn't represent the population.
- A medical AI trained mostly on data from white patients may perform worse on underrepresented groups.
- The model has never learned the patterns for those populations.

**Measurement Bias**: Features are measured differently across groups.
- "Years of experience" penalizes career gaps, which disproportionately affects women.
- "Arrests" doesn't mean "crimes committed"—it reflects policing patterns.

**Aggregation Bias**: One model for heterogeneous populations.
- A single diabetes prediction model may work differently across ethnicities.
- Sometimes you need separate models or careful feature engineering.

**Feedback Loops**: Model predictions affect future data.
- Predictive policing sends more officers to certain neighborhoods → more arrests → more "crime" data → model sends even more officers.
- The bias becomes self-reinforcing.

**The Snowball Effect**: Feedback loops compound over time. Consider predictive policing over 4 cycles:
1. **Year 1**: Model trained on historical data sends more officers to Neighborhood A (higher crime rate in data).
2. **Year 2**: More officers → more arrests in Neighborhood A (crimes detected, not necessarily committed). Model retrains, sees Neighborhood A as even higher risk.
3. **Year 3**: Even more officers to Neighborhood A. Meanwhile, crimes in Neighborhood B go undetected (fewer officers there). Gap widens.
4. **Year 4**: Model now "confirms" its own predictions—Neighborhood A looks dangerous, B looks safe. But this reflects *policing patterns*, not underlying crime rates.

The model creates the very data that justifies its predictions. Breaking this cycle requires external validation (e.g., surveys, victimization studies) not generated by the model itself.

### Case Study: Amazon Hiring Tool

In 2018, it was reported that Amazon had built a hiring tool trained on 10 years of resume data.

**What went wrong:**
- The model learned to penalize words like "women's" (as in "women's chess club captain")
- It downgraded graduates of women's colleges
- It effectively discriminated against female applicants

**The lesson**: Historical data encodes historical bias. Amazon's tech workforce was predominantly male. The model learned that being male correlated with getting hired. It wasn't explicitly told "penalize women," but it learned it from the patterns.

Amazon scrapped the tool.

### Case Study: COMPAS

COMPAS is a recidivism prediction algorithm used in the US criminal justice system to predict whether defendants will reoffend.

**ProPublica's analysis found:**
- Black defendants had a higher **false positive rate** (incorrectly flagged as high risk)
- White defendants had a higher **false negative rate** (incorrectly flagged as low risk)
- Same overall accuracy, very different error patterns

This shows that identical accuracy can hide profoundly different impacts on different groups.

**Choosing between fairness criteria**: This is an ethical decision, not technical—it shouldn't be made solely by data scientists. The data scientist's role is to make tradeoffs transparent ("if we optimize for A, here's what happens to X and Y"), not to unilaterally decide. These decisions should involve domain experts, affected communities, legal experts, and ethicists. Document the decision, reasoning, and who was involved.

### Fairness Metrics

There are multiple mathematical definitions of fairness:

**Demographic Parity** (Statistical Parity):

$$P(\hat{Y}=1|A=0) = P(\hat{Y}=1|A=1)$$

- Equal positive prediction rates across groups
- If 30% of men get approved, 30% of women should get approved
- **Limitation**: Ignores actual qualification rates

**Equalized Odds**:

$$P(\hat{Y}=1|Y=1, A=0) = P(\hat{Y}=1|Y=1, A=1)$$

$$P(\hat{Y}=1|Y=0, A=0) = P(\hat{Y}=1|Y=0, A=1)$$

- Equal true positive rates AND equal false positive rates across groups
- If you're qualified, you should have equal chance of being accepted regardless of group
- If you're unqualified, you should have equal chance of being rejected

**Predictive Parity**:

$$P(Y=1|\hat{Y}=1, A=0) = P(Y=1|\hat{Y}=1, A=1)$$

- Equal precision across groups
- If the model says "yes," the probability of actually being qualified should be the same across groups

**The Courtroom Analogy**: Think of fairness metrics through a courtroom lens. *Demographic parity* is like requiring equal conviction rates across groups—regardless of actual guilt, the same percentage should be convicted. *Equalized odds* is about equal *mistakes*: among truly innocent people, both groups should have the same chance of wrongful conviction (equal FPR); among truly guilty people, both groups should have the same chance of being caught (equal TPR). *Predictive parity* asks: when the jury says "guilty," are they equally likely to be right for both groups? Each metric captures a different intuition about what "fair" means—and they often conflict.

### The Impossibility Theorem

**You cannot satisfy all fairness criteria simultaneously** (except in special cases).

This isn't a technical limitation—it's mathematically proven. If groups have different base rates (different proportions of positive outcomes), you have to choose which fairness criterion matters most.

**Example:**

| Group | Accuracy | FPR | FNR |
|-------|----------|-----|-----|
| A | 85% | 10% | 20% |
| B | 85% | 25% | 5% |

Same accuracy. But Group B has more false positives (more people incorrectly flagged). Group A has more false negatives (more people incorrectly missed).

Which is worse depends on context. In criminal justice, high FPR means innocent people in jail. In medical diagnosis, high FNR means sick people going untreated.

**Why the impossibility exists**: Imagine you try to fix Group B's higher FPR by raising the threshold (being more conservative with "yes" predictions). This reduces false positives, but it also reduces true positives—now qualified people in Group B are less likely to be approved. You've traded one unfairness for another. The only way both could be equal simultaneously is if both groups have the same base rate (same proportion of qualified people)—which is often not the case in the real world due to historical inequalities. This forces a *choice*, not a calculation.

> **Numerical Example: Impossibility Theorem in Action**
>
> ```python
> # Two groups with different base rates
> # Group A: 60% positive, Group B: 30% positive
>
> # Starting point: same threshold for both
> # Group A: TPR=90%, FPR=20%
> # Group B: TPR=80%, FPR=20%
>
> # ATTEMPT 1: Equalize TPR (lower threshold for B)
> # Group B now: TPR=90% (equal!), but FPR jumps to 30%
> print("After equalizing TPR:")
> print("  Group A: TPR=90%, FPR=20%")
> print("  Group B: TPR=90%, FPR=30%")  # FPR now 1.5x higher!
>
> # ATTEMPT 2: Equalize FPR (raise threshold for B)
> # Group B now: FPR=20% (equal!), but TPR drops to 70%
> print("After equalizing FPR:")
> print("  Group A: TPR=90%, FPR=20%")
> print("  Group B: TPR=70%, FPR=20%")  # TPR now 22% lower!
> ```
>
> **Output:**
> ```
> After equalizing TPR:
>   Group A: TPR=90%, FPR=20%
>   Group B: TPR=90%, FPR=30%
> After equalizing FPR:
>   Group A: TPR=90%, FPR=20%
>   Group B: TPR=70%, FPR=20%
> ```
>
> **Interpretation:** You cannot have both equal opportunity (equal TPR) and equal protection (equal FPR) when base rates differ. Equalizing TPR means more false accusations for Group B; equalizing FPR means fewer qualified Group B members are approved. This is the impossibility theorem in action—a mathematical constraint, not a technical failure.
>
> *Source: `slide_computations/module10_examples.py` - `demo_impossibility_theorem()`*

### Calculating Fairness Metrics

```python
from fairlearn.metrics import MetricFrame
from sklearn.metrics import accuracy_score, precision_score, recall_score

# Calculate metrics by group
metric_frame = MetricFrame(
    metrics={
        'accuracy': accuracy_score,
        'precision': precision_score,
        'recall': recall_score
    },
    y_true=y_test,
    y_pred=y_pred,
    sensitive_features=demographic_feature
)

# View metrics by group
print(metric_frame.by_group)

# View maximum difference between groups
print(metric_frame.difference())
```

The `fairlearn` library makes this straightforward. Calculate your metrics by demographic group and look for disparities.

> **Numerical Example: Calculating Fairness Metrics by Hand**
>
> ```python
> # Loan approval model: two demographic groups, 1000 applicants each
> # Group A: 60% qualified (base rate), Group B: 40% qualified
>
> # Confusion matrices:
> # Group A: TP=510, FP=60, TN=340, FN=90 (85% accuracy)
> # Group B: TP=320, FP=120, TN=480, FN=80 (80% accuracy)
>
> # Calculate metrics for each group
> approval_a = (510 + 60) / 1000   # 57%
> approval_b = (320 + 120) / 1000  # 44%
> tpr_a = 510 / 600                # 85%
> tpr_b = 320 / 400                # 80%
> fpr_a = 60 / 400                 # 15%
> fpr_b = 120 / 600                # 20%
>
> print(f"Demographic Parity Ratio: {approval_b/approval_a:.2f}")  # 0.77
> print(f"TPR Ratio: {tpr_b/tpr_a:.2f}")                          # 0.94
> print(f"FPR Ratio: {fpr_b/fpr_a:.2f}")                          # 1.33
> ```
>
> **Output:**
> ```
> Demographic Parity Ratio: 0.77
> TPR Ratio: 0.94
> FPR Ratio: 1.33
> ```
>
> **Interpretation:** Despite similar accuracy (~85%), Group B has a 33% higher false positive rate—more unqualified applicants incorrectly approved. This could indicate the model is compensating for lower approval rates with riskier approvals. The 0.77 demographic parity ratio (below the typical 0.8 threshold) flags a potential disparate impact issue.
>
> *Source: `slide_computations/module10_examples.py` - `demo_fairness_metrics()`*

### Proxy Variables

"We don't use race in our model, so it can't be biased."

This is wrong.

**Proxy variables** are features that correlate with protected attributes:
- ZIP code correlates with race and income
- Name can indicate gender or ethnicity
- Arrest history correlates with race (due to policing patterns)
- "Years since last job" correlates with gender (career gaps)

Removing the protected attribute doesn't remove the bias if proxies remain.

**The Hidden Pathway**: Imagine bias as water flowing from a protected attribute (race) to model predictions. Removing race from your model is like blocking the front door—but if proxy variables like ZIP code, school attended, or arrest history remain, the water finds another path through the back door. These proxies carry much of the same information because of historical patterns (residential segregation, educational inequality, discriminatory policing). A truly fair model must audit these pathways, not just check whether "race" appears in the feature list.

> **Numerical Example: Detecting Proxy Variables**
>
> ```python
> import numpy as np
>
> # Simulate 1000 loan applicants
> np.random.seed(42)
> n = 1000
>
> # Demographic group (protected attribute)
> group = np.random.choice(['A', 'B'], size=n, p=[0.7, 0.3])
>
> # Income differs by group (structural inequality)
> income = np.where(group == 'A',
>     np.random.normal(75000, 20000, n),
>     np.random.normal(55000, 18000, n))
>
> # ZIP score correlates with income (residential segregation)
> zip_score = income / 1000 + np.random.normal(0, 10, n)
>
> # Correlations reveal the hidden pathways
> group_numeric = (group == 'B').astype(int)
> print(f"ZIP ↔ Group correlation: {np.corrcoef(zip_score, group_numeric)[0,1]:.3f}")
> print(f"Income ↔ Group correlation: {np.corrcoef(income, group_numeric)[0,1]:.3f}")
> ```
>
> **Output:**
> ```
> ZIP ↔ Group correlation: -0.408
> Income ↔ Group correlation: -0.438
> ```
>
> **Interpretation:** Even without "group" in the model, ZIP score is strongly correlated (r = -0.41) with group membership. A model using ZIP score will produce different outcomes by group—not because it's explicitly discriminating, but because ZIP encodes the same information through residential segregation patterns. "We don't use race" is not sufficient.
>
> *Source: `slide_computations/module10_examples.py` - `demo_proxy_variable_correlation()`*

### When NOT to Use ML

Not every problem needs machine learning.

**Consider avoiding ML when:**
- Stakes are very high and errors are catastrophic
- Accountability and explanation are paramount
- Training data is fundamentally biased
- The problem is better solved by policy
- Human judgment is essential

**Questions to ask:**
- Who is affected by this system?
- What happens when it's wrong?
- Can we explain decisions to affected parties?
- Is the training data representative?
- Are we automating an already unfair process?

Sometimes the right answer is "don't build this model."

**Pushing back on unethical projects**: Document concerns and frame in terms of business risk (legal liability, reputational damage). Escalate through appropriate channels—ethics hotlines, ombudspersons. If internal advocacy fails: comply under protest (documented), refuse the project (accept consequences), or leave. Building a financial cushion gives leverage. Long-term: seek employers whose values align with yours—ask about ethics review processes during interviews.

### Common Misconceptions

| Misconception | Reality |
|--------------|---------|
| "ML is objective because it's math" | ML learns patterns from data, including human biases |
| "Equal accuracy = fairness" | Same accuracy can hide very different error patterns across groups |
| "Just remove protected attributes" | Proxy variables can encode same information |
| "Fairness is a purely technical problem" | Requires ethical choices that should involve diverse stakeholders |

---

## 10.2 Time Series Forecasting

### Why Time Series Is Different

Time series data has a unique property: **temporal ordering matters**.

In standard ML, we assume observations are independent—shuffling rows shouldn't matter. In time series, shuffling destroys the information. Yesterday's sales tell you something about today's sales. January's patterns repeat every January.

This changes everything about how we model and validate.

**Time Series Has Memory**: The key insight is that time series data has *memory*—today's value carries information about tomorrow's. This memory comes in two forms:
- **AutoRegressive (AR) memory**: Tomorrow's value depends on today's value. High sales today → probably high sales tomorrow. The series "remembers" recent values.
- **Moving Average (MA) memory**: The series remembers recent *surprises*. If yesterday's error was +$1000 (sales beat forecast), today might also beat forecast as the underlying conditions persist.

This memory is why ARIMA models can work: they exploit predictable patterns in how values and errors evolve over time. No memory → no predictability → just random noise.

### Time Series Components

Time series can be decomposed into components:

**Trend**: Long-term direction (sales growing over years)

**Seasonality**: Regular patterns (sales spike every December)

**Cyclical**: Irregular longer-term fluctuations (economic cycles)

**Noise**: Random variation

```python
from statsmodels.tsa.seasonal import seasonal_decompose

decomposition = seasonal_decompose(
    series,
    model='additive',
    period=12
)
decomposition.plot()
```

Understanding these components helps you choose the right model and spot problems.

> **Numerical Example: Time Series Decomposition**
>
> ```python
> import numpy as np
>
> # 12 months of retail sales
> months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',
>           'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']
>
> # True components:
> trend = np.array([50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]) * 1000
> seasonal = np.array([-5, -8, -3, 0, 2, 3, 2, 1, -1, 0, 5, 10]) * 1000
> noise = np.array([1, 0, 1, 3, 0, -1, 3, 2, -1, 1, -1, -1]) * 1000
>
> sales = trend + seasonal + noise
>
> print("Month    Sales     Trend   Seasonal   Noise")
> for i in range(12):
>     print(f"{months[i]:<6} ${sales[i]:>7,} ${trend[i]:>7,} ${seasonal[i]:>+7,} ${noise[i]:>+6,}")
> ```
>
> **Output:**
> ```
> Month    Sales     Trend   Seasonal   Noise
> Jan    $ 46,000 $ 50,000 $  -5,000 $ +1,000
> Feb    $ 43,000 $ 51,000 $  -8,000 $     +0
> Mar    $ 50,000 $ 52,000 $  -3,000 $ +1,000
> ...
> Nov    $ 64,000 $ 60,000 $  +5,000 $ -1,000
> Dec    $ 70,000 $ 61,000 $ +10,000 $ -1,000
> ```
>
> **Interpretation:** December sales ($70K) aren't just "high"—they're the sum of underlying trend ($61K), seasonal boost (+$10K for holiday shopping), and random noise (-$1K). Decomposition reveals that the $18K swing from February to December is almost entirely seasonal, not growth. For forecasting, extrapolate trend forward, add the expected seasonal effect, and report uncertainty from the noise variance.
>
> *Source: `slide_computations/module10_examples.py` - `demo_time_series_decomposition()`*

### ARIMA Models

ARIMA is the classic statistical approach to time series.

**ARIMA(p, d, q):**
- **AR (AutoRegressive)**: Predict from past values (how many lags to use = p)
- **I (Integrated)**: Differencing for stationarity (how many times to difference = d)
- **MA (Moving Average)**: Predict from past errors (how many lag errors to use = q)

```python
from statsmodels.tsa.arima.model import ARIMA

model = ARIMA(train_series, order=(1, 1, 1))
results = model.fit()
forecast = results.forecast(steps=30)
```

**Choosing p, d, q:**
- ACF/PACF plots give guidance
- AIC/BIC criteria for model selection
- Or use auto_arima:

```python
from pmdarima import auto_arima

model = auto_arima(
    train_series,
    seasonal=True,
    m=12,  # Monthly seasonality
    trace=True
)
```

Auto_arima searches through parameter combinations and picks the best one.

> **Numerical Example: ARIMA Parameter Intuition**
>
> ```python
> import numpy as np
> np.random.seed(42)
>
> # Generate AR(1) series with different persistence (phi)
> n, shocks = 50, np.random.normal(0, 1, 50)
>
> def ar1_series(phi, shocks):
>     y = np.zeros(len(shocks))
>     y[0] = shocks[0]
>     for t in range(1, len(shocks)):
>         y[t] = phi * y[t-1] + shocks[t]
>     return y
>
> for phi in [0.0, 0.5, 0.9, 0.99]:
>     series = ar1_series(phi, shocks)
>     autocorr = np.corrcoef(series[:-1], series[1:])[0, 1]
>     print(f"phi={phi:.2f}: variance={np.var(series):.1f}, autocorr={autocorr:.2f}")
> ```
>
> **Output:**
> ```
> phi=0.00: variance=0.8, autocorr=0.03
> phi=0.50: variance=1.2, autocorr=0.53
> phi=0.90: variance=5.1, autocorr=0.91
> phi=0.99: variance=18.0, autocorr=0.98
> ```
>
> **Interpretation:** The AR coefficient (phi) controls memory. At phi=0, each value is independent noise—no predictability. At phi=0.9, today strongly predicts tomorrow (autocorr=0.91); shocks persist for many periods. At phi=0.99, the series approaches a random walk—shocks accumulate and variance explodes. When you see high autocorrelation in your data, an AR model can exploit that persistence for forecasting.
>
> *Source: `slide_computations/module10_examples.py` - `demo_arima_parameter_intuition()`*

### Prophet

Facebook's Prophet is a popular alternative to ARIMA.

**Advantages:**
- Handles seasonality automatically (multiple seasonalities!)
- Robust to missing data
- Interpretable components
- Easy to add holidays and special events

```python
from prophet import Prophet

# Data must have columns 'ds' (date) and 'y' (value)
model = Prophet(
    yearly_seasonality=True,
    weekly_seasonality=True
)
model.fit(train_df)

future = model.make_future_dataframe(periods=30)
forecast = model.predict(future)

model.plot(forecast)
model.plot_components(forecast)  # Shows trend, seasonality, etc.
```

Prophet is particularly good for business applications with strong seasonal patterns.

### When to Choose What

- **Prophet**: Multiple seasonalities, missing data, holidays, interpretable components
- **ARIMA**: More control, complex series that don't fit Prophet's assumptions, very short series
- **LSTM**: Complex non-linear patterns, multiple input features, long sequences

### Time Series Validation

**You cannot use standard k-fold cross-validation for time series.**

Why? Because it would leak future information into training. If your test set contains January 2024 and your training set contains February 2024, you're cheating—you're using the future to predict the past.

**Walk-forward validation:**
```
Train: [----]          Test: [-]
Train: [------]        Test: [-]
Train: [--------]      Test: [-]
```

Always train on the past, test on the future. Never the reverse.

**The Time Machine Rule**: Using k-fold cross-validation on time series is like checking tomorrow's newspaper to predict today's stock price. In fold 1, your model trains on data from weeks 3-10 to predict week 1—it literally uses the future to predict the past. Of course it looks accurate! But in deployment, you'll never have next week's data to help predict this week. Walk-forward validation enforces the constraint you'll face in production: you can only look backward, never forward.

```python
from sklearn.model_selection import TimeSeriesSplit

tscv = TimeSeriesSplit(n_splits=5)
for train_idx, test_idx in tscv.split(data):
    train = data[train_idx]
    test = data[test_idx]
    # Train and evaluate
```

> **Numerical Example: Walk-Forward vs K-Fold Validation**
>
> ```python
> # AR(1) time series: y_t = 0.9 * y_{t-1} + noise
> # 100 time points with strong autocorrelation
>
> # K-Fold (WRONG): trains on future, tests on past
> # Fold 1: Train on points 20-99, test on 0-19
> # The model "knows" where the series goes!
> kfold_mae = 3.76
>
> # Walk-Forward (CORRECT): trains on past, tests on future
> # Window 1: Train on 0-49, test on 50-59
> # Window 2: Train on 0-59, test on 60-69
> walk_forward_mae = 3.83
>
> print(f"K-Fold MAE (biased):      {kfold_mae:.2f}")
> print(f"Walk-Forward MAE:         {walk_forward_mae:.2f}")
> print(f"K-Fold underestimates by: {walk_forward_mae - kfold_mae:.2f}")
> ```
>
> **Output:**
> ```
> K-Fold MAE (biased):      3.76
> Walk-Forward MAE:         3.83
> K-Fold underestimates by: 0.07
> ```
>
> **Interpretation:** K-fold appears more accurate because it cheats—using future data to predict past values. In this example, the difference is small (0.07), but in series with trends or structural breaks, k-fold can underestimate error by 20-50%. Always use walk-forward for time series; it reflects the constraint you'll face in production.
>
> *Source: `slide_computations/module10_examples.py` - `demo_walk_forward_vs_kfold()`*

### Business Applications

Time series forecasting is everywhere in business:
- **Sales forecasting**: Budget planning, resource allocation
- **Demand planning**: Inventory management, supply chain
- **Capacity planning**: Staffing, infrastructure
- **Financial forecasting**: Revenue projections, cash flow

---

## 10.3 Model Deployment

### From Notebook to Production

You've built a model in a Jupyter notebook. It works great. Now what?

**The gap between "model works" and "model is deployed" is significant:**
- How do other systems call your model?
- How do you handle errors?
- How do you scale?
- How do you update the model?
- How do you monitor performance?

This is where software engineering meets data science.

### Model Serialization

First, you need to save your model so it can be loaded elsewhere.

**Pickle/Joblib** (for scikit-learn models):
```python
import joblib

# Save model
joblib.dump(model, 'model.pkl')

# Load model
model = joblib.load('model.pkl')
```

**ONNX** (cross-platform format):
- Works across different frameworks (PyTorch, TensorFlow, scikit-learn)
- Optimized for inference
- Useful when production environment differs from development

```python
import torch.onnx

torch.onnx.export(
    model,
    dummy_input,
    "model.onnx",
    input_names=['input'],
    output_names=['output']
)
```

### Creating APIs

To let other systems use your model, wrap it in an API.

**Flask** (simple, widely used):
```python
from flask import Flask, request, jsonify
import joblib

app = Flask(__name__)
model = joblib.load('model.pkl')

@app.route('/predict', methods=['POST'])
def predict():
    data = request.json
    features = data['features']
    prediction = model.predict([features])
    return jsonify({'prediction': prediction[0]})

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

**FastAPI** (modern, automatic documentation):
```python
from fastapi import FastAPI
from pydantic import BaseModel
import joblib

app = FastAPI()
model = joblib.load('model.pkl')

class PredictionRequest(BaseModel):
    features: list

@app.post("/predict")
def predict(request: PredictionRequest):
    prediction = model.predict([request.features])
    return {"prediction": prediction[0]}
```

Now other applications can send HTTP requests to get predictions.

### Containerization with Docker

Docker packages your application with all its dependencies.

**Dockerfile:**
```dockerfile
FROM python:3.9-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY model.pkl .
COPY app.py .

EXPOSE 5000
CMD ["python", "app.py"]
```

**Benefits:**
- **Reproducibility**: Same environment everywhere
- **Portability**: Runs on any system with Docker
- **Scalability**: Easy to run multiple containers
- **Isolation**: Dependencies don't conflict

**The Shipping Container Analogy**: Before standardized shipping containers, loading cargo was chaos—every ship, every port, every truck had different requirements. Containers changed everything: pack once in a standard-sized box, and it moves seamlessly across ships, trains, and trucks worldwide. Docker does the same for software. Your model, its Python version, its exact library versions, its configuration—all packed into one container that runs identically on your laptop, your colleague's machine, and the production server. No more "but it worked on my machine!"

### Cloud Deployment Options

| Platform | Use Case | Complexity |
|----------|----------|------------|
| AWS SageMaker | Full ML platform | Medium |
| Google Vertex AI | Full ML platform | Medium |
| Azure ML | Full ML platform | Medium |
| Heroku | Simple web apps | Low |
| AWS Lambda | Serverless, simple models | Low |

For production ML at scale, the major cloud platforms provide integrated solutions. For simple models or prototypes, Heroku or Lambda can be quick to set up.

---

## 10.4 Production & Business Value

### Monitoring in Production

Deployment isn't the end—it's the beginning of a new phase.

**Metrics to track continuously:**
- **Model accuracy over time**: Is performance degrading?
- **Feature distributions**: Are inputs changing?
- **Latency and throughput**: Is the system fast enough?
- **Error rates**: Are there failures?

Set up alerts. If accuracy drops below a threshold, you want to know immediately.

### Model Drift

**Data drift**: Input distribution changes.
- Customer demographics shift
- Seasonality wasn't captured
- Data collection process changed

**Concept drift**: The relationship between inputs and outputs changes.
- Customer behavior changed (e.g., during COVID)
- What used to predict churn no longer does

**Telling Them Apart**: Data drift and concept drift require different responses:

| Type | Example | What Changed | Solution |
|------|---------|--------------|----------|
| **Data drift** | Your customer base shifted younger (average age dropped from 45 to 35) | Input distribution | Model may still be correct for each age; retrain on new demographics |
| **Concept drift** | COVID hits; customers who previously never churned are now churning at high rates | Relationship between inputs and outputs | The rules changed; old patterns don't apply; must retrain on post-COVID behavior |
| **Both** | Pandemic: customers younger (data drift) AND their behavior fundamentally changed (concept drift) | Everything | Full reassessment needed |

The distinction matters because data drift can sometimes be addressed by resampling or weighting, while concept drift means your model's core assumptions are invalidated—yesterday's rules don't work today.

```python
from evidently import Report
from evidently.metrics import DataDriftTable

report = Report(metrics=[DataDriftTable()])
report.run(
    reference_data=training_data,
    current_data=production_data
)
report.show()
```

Evidently AI and similar tools help detect when your data has shifted.

### Retraining Strategies

When drift is detected (or just periodically), you need to retrain.

**Scheduled retraining**: Weekly, monthly—on a fixed schedule

**Triggered retraining**: When drift exceeds a threshold

**Continuous training**: Update with each new batch of data

**Automate the pipeline:**
1. Data validation
2. Feature engineering
3. Model training
4. Evaluation (reject if metrics don't meet threshold)
5. Deployment

This is where MLOps comes in—applying DevOps principles to ML.

### A/B Testing

**Purpose**: Compare new model against current model

**Implementation:**
1. Route percentage of traffic to new model
2. Track metrics for both models
3. Statistical test for significance
4. Roll out winner

**Key considerations:**
- Sample size requirements
- Duration of test
- Guardrail metrics (don't harm user experience)

> **Numerical Example: A/B Test Sample Size Calculation**
>
> ```python
> from scipy import stats
> import numpy as np
>
> # Scenario: Testing if new model improves conversion rate
> p_control = 0.05     # Current: 5% conversion
> p_treatment = 0.055  # Target: 5.5% (10% relative improvement)
> alpha = 0.05         # 5% significance level
> power = 0.80         # 80% power
>
> # Sample size formula for two-proportion z-test
> z_alpha = stats.norm.ppf(1 - alpha/2)  # 1.96
> z_beta = stats.norm.ppf(power)          # 0.84
> p_pooled = (p_control + p_treatment) / 2
> effect = p_treatment - p_control
>
> n_per_group = 2 * (z_alpha + z_beta)**2 * p_pooled * (1 - p_pooled) / effect**2
> print(f"Required per group: {int(np.ceil(n_per_group)):,}")
> print(f"Total required:     {2 * int(np.ceil(n_per_group)):,}")
> ```
>
> **Output:**
> ```
> Required per group: 31,235
> Total required:     62,470
> ```
>
> **Interpretation:** To detect a 0.5% absolute improvement (5.0% → 5.5%) with 80% power, you need ~62,000 total users. With 10,000 daily visitors, that's about 1 week. Smaller effects need exponentially more data: detecting 0.25% improvement would require ~244,000 users. Plan your test duration *before* starting, and resist the temptation to peek at results early—that inflates false positive rates.
>
> *Source: `slide_computations/module10_examples.py` - `demo_ab_test_sample_size()`*

### ROI Calculation

How do you justify an ML project?

**Example: Churn prevention model**

```
Annual churning customers: 10,000
Customer lifetime value: $500
Churn cost without model: $5,000,000

Model performance:
- Identifies 75% of churners
- Intervention success rate: 30%
- Customers saved: 10,000 × 0.75 × 0.30 = 2,250
- Value saved: 2,250 × $500 = $1,125,000

Costs:
- Development: $100,000
- Annual maintenance: $20,000
- Intervention cost: $50 per flagged customer
- Total intervention cost: 7,500 × $50 = $375,000
- Total costs: $495,000

First year ROI: ($1,125,000 - $495,000) / $495,000 = 127%
```

**The key**: Quantify business impact, not just accuracy. "95% accuracy" means nothing to a CFO. "$630,000 net value in year one" does.

> **Numerical Example: ROI Sensitivity Analysis**
>
> ```python
> # Base case: 127% ROI
> # But how sensitive is this to our assumptions?
>
> # Vary model recall: 60%, 75%, 90%
> recall_scenarios = {'60%': 114, '75%': 127, '90%': 137}
>
> # Vary intervention success: 20%, 30%, 40%
> intervention_scenarios = {'20%': 52, '30%': 127, '40%': 203}
>
> # Scenario analysis
> pessimistic = {'recall': 0.60, 'intervention': 0.20, 'ltv': 400, 'dev_cost': 150000}
> optimistic = {'recall': 0.90, 'intervention': 0.40, 'ltv': 600, 'dev_cost': 75000}
>
> print("Scenario        Net Value       ROI")
> print("Pessimistic     $10,000          2%")
> print("Base Case      $630,000        127%")
> print("Optimistic   $1,615,000        296%")
> ```
>
> **Output:**
> ```
> Scenario        Net Value       ROI
> Pessimistic     $10,000          2%
> Base Case      $630,000        127%
> Optimistic   $1,615,000        296%
> ```
>
> **Interpretation:** Even the pessimistic scenario shows positive ROI, which strengthens the business case. But note how sensitive the results are: intervention success rate has the largest impact (52% to 203% ROI). When presenting to stakeholders, show the range, identify which assumptions drive the most uncertainty, and explain how you'll validate those assumptions during a pilot.
>
> *Source: `slide_computations/module10_examples.py` - `demo_roi_sensitivity_analysis()`*

### Communicating Uncertainty

**Be honest about limitations:**
- Model accuracy is not 100%
- Performance varies across segments
- Future performance is not guaranteed
- Edge cases exist

**Use confidence intervals:**
- "We predict revenue of $1.2M ± $150K"
- "The model is 85% confident this customer will churn"

**Scenario analysis:**
- Best case / Base case / Worst case

Stakeholders appreciate honesty. Overpromising leads to disappointment and loss of trust.

### Managing Expectations

**Common pitfalls:**
- Overpromising accuracy
- Underestimating timeline
- Ignoring maintenance needs
- Assuming it's a one-time effort

**Best practices:**
- Start with a pilot project
- Set realistic expectations upfront
- Plan for iteration—first version won't be perfect
- Communicate progress regularly
- Budget for ongoing maintenance

ML models are more like products than projects. The world changes, data shifts, and models degrade. You wouldn't build a website and never update it. Same with ML models.

**Estimating maintenance costs**: Rule of thumb: 15-25% of initial development cost per year. Break down components: monitoring infrastructure, data pipeline maintenance, periodic retraining, model auditing, incident response. Staff time is usually the largest cost (one person at 20% time ≈ $30-50K/year). Present stakeholders with scenarios: "minimum maintenance" costs X with degradation risk; "recommended maintenance" costs Y with better reliability.

---

## Reflection Questions

1. Amazon's hiring tool was trained on successful employees. Why did it still produce biased results?

2. A model has 85% accuracy for both men and women. Is it fair? What else would you check?

3. Your model uses ZIP code, which correlates with race. Should you remove it? What are the trade-offs?

4. A hospital wants to use ML to allocate scarce medical resources. What ethical considerations arise?

5. Why can't we use regular k-fold cross-validation for time series?

6. When would you choose Prophet over ARIMA?

7. Your model works on your laptop but fails in production. What might cause this?

8. A model improves accuracy by 2% but costs $500K to develop. How do you decide if it's worth it?

9. How do you explain to a CFO that ML requires ongoing investment, not a one-time cost?

---

## Practice Problems

1. Calculate fairness metrics from confusion matrices for two demographic groups

2. Identify bias sources in a case study scenario

3. Design a monitoring plan for a deployed fraud detection model

4. Calculate ROI for an ML project given costs and projected benefits

5. Write a brief stakeholder communication explaining a model's limitations

---

## Chapter Summary

**Six key takeaways from Module 10:**

1. **Bias** enters ML through data and design; measure and mitigate it actively

2. **Fairness** has multiple incompatible definitions—you must choose

3. **Time series** requires temporal validation; never use k-fold

4. **Deployment** needs APIs, containers, and monitoring infrastructure

5. **Drift** happens; plan for detection and retraining

6. **Business value** must be quantified and communicated in dollars, not accuracy

---

## Course Summary

Take a moment to appreciate how far you've come.

**You've learned to:**
- Build ML models (regression, classification, clustering)
- Train ensemble methods and understand their power
- Train neural networks for structured data, images, and text
- Interpret and explain model decisions
- Deploy responsibly with fairness considerations and monitoring

**You understand:**
- The ML workflow from data to deployment
- Evaluation, validation, and avoiding common pitfalls
- When to use which technique
- How to communicate with stakeholders

**You're ready for the Capstone Project!**

---

## What's Next: Capstone Project

The capstone project is your chance to apply everything you've learned to a real problem.

You'll:
- Define a business problem
- Collect and prepare data
- Build and evaluate models
- Interpret and explain results
- Create deployment and monitoring plans
- Present to stakeholders

This is the culmination of the course—showing that you can take a problem from start to finish.
